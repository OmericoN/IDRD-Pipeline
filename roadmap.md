# IDRD Pipeline â€” Roadmap

## Project Overview
A research paper processing pipeline that fetches academic papers from Semantic
Scholar, downloads PDFs, converts them to structured XML, extracts sections, and
(eventually) uses an LLM to extract features for a RAG system.

---

## Current Status

| Step | Module | Status | Notes |
|------|--------|--------|-------|
| 1. Fetch papers | `src/pubfetcher/client.py` | âœ… Done | Renamed from `fetching.py` |
| 2. Parse & store | `src/db/db.py` | âœ… Done | Migrated SQLite â†’ PostgreSQL |
| 3. Download PDFs | `src/extractor/downloader.py` | âœ… Done | Still has SQLite-style raw SQL |
| 4. Convert PDFs â†’ XML | `src/extractor/converter.py` | âœ… Done | Still has SQLite-style raw SQL |
| 5. Extract sections | `src/extractor/extractor.py` | âŒ Empty | Not started |
| 6. LLM feature extraction | TBD | âŒ Not started | RAG prep |
| 7. RAG / Vector search | TBD | âŒ Not started | pgvector planned |
| Tests | `src/extractor/tests.py` | âš ï¸ Partial | Tests still import old-style DB |
| Config | `src/config.py` | âš ï¸ Incomplete | Missing DB + LLM config |

---

## Known Issues Right Now

1. **`downloader.py` and `converter.py`** still use raw SQLite-style queries
   (`?` placeholders, `= 0`, `= 1` for booleans) â€” they bypass `db.py` methods
   and talk directly to `self.db.cursor` with old syntax.
2. **`main.py`** calls `self.db.cursor` and `self.db.db_path` directly â€” both
   are no longer valid after the Postgres migration.
3. **`config.py`** is incomplete â€” no DB config, no LLM config.
4. **`extractor.py`** is empty.
5. **`src/db/__init__.py`** is empty â€” `PublicationDatabase` is not exported.
6. **Tests** mock `db_path` (SQLite) and will fail with the new DB class.
7. **Redundancy**: DB queries are duplicated inline in `downloader.py`,
   `converter.py`, and `main.py` instead of using `db.py` methods.

---

## Roadmap

### Phase 1 â€” Stabilise & Refactor (NOW) ğŸ”§

#### 1.1 Fix `src/db/__init__.py`
Export `PublicationDatabase` so imports are clean across the project.

#### 1.2 Complete `src/config.py`
Add Postgres and future LLM config in one place.

#### 1.3 Fix `downloader.py`
- Replace all raw SQL + `?` placeholders with calls to `db.py` methods.
- Remove `db_path` argument (no longer relevant for Postgres).

#### 1.4 Fix `converter.py`
- Same as downloader â€” remove raw SQL, use `db.py` methods.
- Remove `self.db.db_path` reference.

#### 1.5 Fix `main.py`
- Remove all direct `self.db.cursor` calls.
- Use `db.get_pipeline_status()` and `db.get_papers_needing_download()` etc.

#### 1.6 Add `src/utils/db_utils.py`
Centralise repeated query patterns (sync PDFs, print status, etc.)

#### 1.7 Update tests
- Mock `psycopg2` instead of SQLite.
- Or use a test Postgres DB / `pytest-postgresql`.

---

### Phase 2 â€” Section Extraction ğŸ“„

#### 2.1 Implement `src/extractor/extractor.py`
Parse TEI XML output from GROBID and extract:
- Title, Abstract
- Introduction, Related Work, Methods, Results, Conclusion
- References

#### 2.2 Add `sections` table to `db.py`
Store extracted sections per paper.

#### 2.3 Update pipeline in `main.py`
Wire up `step_4_extract_sections()`.

---

### Phase 3 â€” LLM Feature Extraction ğŸ¤–

#### 3.1 Create `src/llm/` module
- `client.py` â€” LLM API wrapper (OpenAI / local)
- `prompts.py` â€” prompt templates
- `extractor.py` â€” run prompts over sections

#### 3.2 Define features to extract
Examples: methodology, dataset used, metrics, findings, limitations.

#### 3.3 Add `features` table to `db.py`
Store structured LLM output per paper/section.

#### 3.4 Update `config.py`
Add `LLM_MODEL`, `LLM_API_KEY`, `MAX_TOKENS` etc.

---

### Phase 4 â€” RAG & Vector Search ğŸ”

#### 4.1 Enable pgvector
```sql
CREATE EXTENSION IF NOT EXISTS vector;
ALTER TABLE publications ADD COLUMN embedding vector(1536);
CREATE INDEX ON publications USING hnsw (embedding vector_cosine_ops);
```

#### 4.2 Create `src/rag/` module
- `embedder.py` â€” generate embeddings from sections/features
- `retriever.py` â€” vector similarity search via pgvector
- `pipeline.py` â€” end-to-end RAG query handler

#### 4.3 Populate embeddings
Run embedder over all extracted sections and store in DB.

---

### Phase 5 â€” Quality & Production ğŸš€

- [ ] Add connection pooling (`psycopg2.pool` or `asyncpg`)
- [ ] Add proper logging (`logging` module, replace `print`)
- [ ] Add `alembic` for DB migrations
- [ ] Docker Compose for Postgres + GROBID + app
- [ ] CI/CD with GitHub Actions
- [ ] Full test coverage with `pytest` + `pytest-postgresql`

---

## Suggested Project Structure (Target)

```
IDRD-Pipeline/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py               âœ… exists â€” needs completion
â”‚   â”œâ”€â”€ main.py                 âœ… exists â€” needs fixes
â”‚   â”œâ”€â”€ db/
â”‚   â”‚   â”œâ”€â”€ __init__.py         âš ï¸  empty  â€” needs export
â”‚   â”‚   â””â”€â”€ db.py               âœ… migrated to Postgres
â”‚   â”œâ”€â”€ pubfetcher/
â”‚   â”‚   â””â”€â”€ client.py           âœ… renamed
â”‚   â”œâ”€â”€ extractor/
â”‚   â”‚   â”œâ”€â”€ downloader.py       âš ï¸  needs raw SQL removed
â”‚   â”‚   â”œâ”€â”€ converter.py        âš ï¸  needs raw SQL removed
â”‚   â”‚   â”œâ”€â”€ extractor.py        âŒ  empty
â”‚   â”‚   â””â”€â”€ tests.py            âš ï¸  needs Postgres mocks
â”‚   â”œâ”€â”€ llm/                    âŒ  not created
â”‚   â”‚   â”œâ”€â”€ client.py
â”‚   â”‚   â”œâ”€â”€ prompts.py
â”‚   â”‚   â””â”€â”€ extractor.py
â”‚   â”œâ”€â”€ rag/                    âŒ  not created
â”‚   â”‚   â”œâ”€â”€ embedder.py
â”‚   â”‚   â””â”€â”€ retriever.py
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ dict_parser.py      âœ… exists
â”‚       â””â”€â”€ db_utils.py         âŒ  not created â€” needed now
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ pdf/
â”‚   â”œâ”€â”€ xml/
â”‚   â””â”€â”€ metadata/
â”œâ”€â”€ .env                        âœ… exists
â”œâ”€â”€ ROADMAP.md                  âœ… this file
â””â”€â”€ requirements.txt            â“  check exists / up to date
```

---

## Immediate Next Steps (Priority Order)

1. `src/db/__init__.py` â€” export `PublicationDatabase`
2. `src/config.py` â€” add DB + LLM config
3. `src/utils/db_utils.py` â€” extract shared helpers
4. Fix `downloader.py` â€” remove raw SQL
5. Fix `converter.py` â€” remove raw SQL
6. Fix `main.py` â€” remove direct cursor access
7. Implement `extractor.py` â€” section parsing
8. Update tests â€” Postgres mocks