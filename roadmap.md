# IDRD Pipeline â€” Roadmap

## Project Overview
A research paper processing pipeline that fetches academic papers from Semantic
Scholar, downloads PDFs, converts them to structured TEI XML, extracts Markdown
sections, and (eventually) uses an LLM to extract features for a RAG system.

---

## Current Status

| Step | Module | Status | Notes |
|------|--------|--------|-------|
| 1. Fetch papers | `src/pubfetcher/client.py` | âœ… Done | |
| 2. Parse & store | `src/db/db.py` | âœ… Done | PostgreSQL |
| 3. Download PDFs | `src/ingestion/downloader.py` | âœ… Done | |
| 4. Convert PDFs â†’ XML | `src/ingestion/converter.py` | âœ… Done | GROBID via Docker |
| 5. Extract Markdown | `src/ingestion/extractor.py` | âœ… Done | TEI XML â†’ `.md` |
| 6. Ground truth experiment | `experiments/ground_truth/` | âœ… Done | Fully isolated |
| 7. LLM feature extraction | `src/llm/` | ğŸ”² Phase 3 | Not started |
| 8. RAG / Vector search | `src/rag/` | ğŸ”² Phase 4 | pgvector planned |

---

## Phase 1 â€” Core Pipeline âœ… COMPLETE

- [x] Fetch papers from Semantic Scholar API
- [x] Parse and store in PostgreSQL
- [x] Download open-access PDFs
- [x] Convert PDFs to TEI XML via GROBID (Docker)
- [x] Extract structured Markdown from TEI XML
- [x] CLI with individual step flags and full pipeline mode
- [x] Ground truth experiment runner (fully isolated)
- [x] SQL injection hardening (`psycopg2.sql.Identifier`, parameterised LIMIT)
- [x] Per-run logs saved to `logs/runs/`

---

## Phase 2 â€” Quality & Robustness ğŸ”§

- [ ] Add proper logging (`logging` module, replace `print` statements)
- [ ] Add `alembic` for DB schema migrations
- [ ] Add connection pooling (`psycopg2.pool`)
- [ ] Full test suite with `pytest` + `pytest-postgresql`
- [ ] Docker Compose for Postgres + GROBID + app together
- [ ] `requirements.txt` audit and pin versions

---

## Phase 3 â€” LLM Feature Extraction ğŸ¤–

#### 3.1 Create `src/llm/` module
- `client.py` â€” LLM API wrapper (OpenAI / local Ollama)
- `prompts.py` â€” prompt templates per section type
- `extractor.py` â€” run prompts over extracted Markdown sections

#### 3.2 Features to extract (per paper)
- Methodology used
- Datasets referenced (implicit + explicit)
- Metrics and results
- Limitations
- Key findings

#### 3.3 Add `features` table to `db.py`
Store structured LLM output per paper.

#### 3.4 Wire up `step_5_extract_features()` in `main.py`
Currently a placeholder â€” implement once `src/llm/` is ready.

#### 3.5 Update `config.py`
Add `LLM_MODEL`, `LLM_API_KEY`, `LLM_BASE_URL`, `MAX_TOKENS`.

---

## Phase 4 â€” RAG & Vector Search ğŸ”

#### 4.1 Enable pgvector
```sql
CREATE EXTENSION IF NOT EXISTS vector;
ALTER TABLE publications ADD COLUMN embedding vector(1536);
CREATE INDEX ON publications USING hnsw (embedding vector_cosine_ops);
```

#### 4.2 Create `src/rag/` module
- `embedder.py` â€” generate embeddings from Markdown sections
- `retriever.py` â€” vector similarity search via pgvector
- `pipeline.py` â€” end-to-end RAG query handler

#### 4.3 Populate embeddings
Run embedder over all extracted sections and store in DB.

---

## Phase 5 â€” Scale (Long Term) ğŸš€

- [ ] Replace Semantic Scholar API with local S2ORC snapshot (~300 GB metadata)
  - Drop-in replacement for `client.py` â€” same `search_papers()` interface
  - Load S2ORC JSONL shards into PostgreSQL
  - Elasticsearch or pgvector for full-text title/abstract search
- [ ] `ThreadPoolExecutor` in downloader (3â€“5 workers) for I/O throughput
- [ ] CI/CD with GitHub Actions
- [ ] Monitoring dashboard for pipeline runs

---

## Project Structure (Current)

```
IDRD-Pipeline/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py                      â† pipeline entry point
â”‚   â”œâ”€â”€ config.py                    â† all settings
â”‚   â”œâ”€â”€ db/
â”‚   â”‚   â”œâ”€â”€ db.py                    â† PostgreSQL manager
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ pubfetcher/
â”‚   â”‚   â””â”€â”€ client.py                â† Semantic Scholar client
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â”œâ”€â”€ downloader.py            â† PDF downloader
â”‚   â”‚   â”œâ”€â”€ converter.py             â† GROBID converter
â”‚   â”‚   â””â”€â”€ extractor.py             â† TEI XML â†’ Markdown
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ db_utils.py              â† shared helpers
â”‚       â””â”€â”€ dict_parser.py           â† paper dict parser
â”œâ”€â”€ experiments/
â”‚   â””â”€â”€ ground_truth/
â”‚       â”œâ”€â”€ gt_runner.py             â† GT experiment entry point
â”‚       â”œâ”€â”€ gt_fetcher.py            â† fetch GT papers
â”‚       â”œâ”€â”€ gt_downloader.py         â† download GT PDFs
â”‚       â””â”€â”€ gt_report.py             â† coverage report
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ pdf/
â”‚   â”œâ”€â”€ xml/
â”‚   â”œâ”€â”€ markdown/
â”‚   â”œâ”€â”€ ground_truth/
â”‚   â”‚   â””â”€â”€ ground_truth.csv
â”‚   â””â”€â”€ gt_experiment/
â”‚       â”œâ”€â”€ pdf/
â”‚       â”œâ”€â”€ xml/
â”‚       â”œâ”€â”€ markdown/
â”‚       â””â”€â”€ report_*.json
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ runs/
â”œâ”€â”€ .env
â”œâ”€â”€ ROADMAP.md
â””â”€â”€ README.md
```