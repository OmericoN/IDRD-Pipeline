# IDRD Pipeline

A multi-stage pipeline for fetching academic publications, downloading their PDFs,
converting them to structured XML, extracting Markdown sections, and (in future phases)
extracting features for a RAG system.

---

## Requirements

```bash
pip install psycopg2-binary python-dotenv requests tqdm docker pandas openpyxl lxml
```

Create a `.env` file in the project root:

```
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=idrd_pipeline
POSTGRES_USER=postgres
POSTGRES_PASSWORD=your_password
SEMANTIC_SCHOLAR_API_KEY=your_key
```

Make sure **PostgreSQL** is running and the database exists:

```bash
psql -U postgres -c "CREATE DATABASE idrd_pipeline;"
```

The schema is created automatically on first run.

---

## Running the Pipeline

All commands are run from the **project root**.

### Full Pipeline (recommended starting point)
Fetch â†’ Download â†’ Convert â†’ Extract Markdown in one command.

```bash
python src/main.py --query "implicit dataset references" --limit 50
```

---

### Individual Steps

#### Step 1 â€” Fetch papers only
Store papers in the database without downloading anything.

```bash
python src/main.py --query "Transformers NLP" --limit 100 --fetch-only
```

Filter by field of study:
```bash
python src/main.py --query "dataset" --limit 200 --fetch-only --fields-of-study "Computer Science"
```

Include non-open-access papers (no PDF URL required):
```bash
python src/main.py --query "dataset" --limit 200 --fetch-only --all-access
```

---

#### Step 2 â€” Download PDFs only
Download PDFs for papers already in the database.

```bash
python src/main.py --download-only
```

Limit how many to download, set delay, or force re-download:
```bash
python src/main.py --download-only --dl-limit 20 --dl-delay 1.0 --dl-overwrite
```

---

#### Step 3 â€” Convert PDFs to XML only
Requires **Docker** running. Starts a GROBID container automatically.

```bash
python src/main.py --convert-only
```

Delete PDFs after successful conversion (saves disk space):
```bash
python src/main.py --convert-only --delete-pdfs
```

Re-convert already converted files:
```bash
python src/main.py --convert-only --cv-overwrite
```

---

#### Step 4 â€” Extract Markdown only
Extract structured Markdown from TEI XML files already in `data/xml/`.

```bash
python src/main.py --extract-only
```

Re-extract, overwriting existing `.md` files:
```bash
python src/main.py --extract-only --ex-overwrite
```

Limit how many to extract:
```bash
python src/main.py --extract-only --ex-limit 20
```

---

### Check Status
See how many papers are at each pipeline stage.

```bash
python src/main.py --status
```

Example output:
```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
PIPELINE STATUS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  Total papers          : 150
  PDFs downloaded       : 120
  Converted to XML      : 98
  Sections extracted    : 87
  Features extracted    : 0
  Download errors       : 5
  Conversion errors     : 2
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

### Reset

Reset pipeline tracking flags (keeps all papers, allows re-running steps):
```bash
python src/main.py --reset status
```

Full database wipe â€” **deletes everything** (requires double confirmation):
```bash
python src/main.py --reset full
```

---

## Ground Truth Experiment

Run the pipeline in a fully isolated mode against the ground truth CSV.
Does **not** touch the main database or `data/pdf/`, `data/xml/`, `data/markdown/`.

```bash
# Full run â€” fetch + download + convert + extract markdown
python experiments/ground_truth/gt_runner.py

# Fetch metadata only â€” see what Semantic Scholar has before committing
python experiments/ground_truth/gt_runner.py --fetch-only

# Fetch + download, skip GROBID
python experiments/ground_truth/gt_runner.py --no-xml

# Skip markdown extraction
python experiments/ground_truth/gt_runner.py --no-extract

# Re-run extractor on existing XMLs only (no network calls)
python experiments/ground_truth/gt_runner.py --extract-only

# Use a custom CSV
python experiments/ground_truth/gt_runner.py --csv data/ground_truth/temp.csv
```

Output goes to `data/gt_experiment/`:
```
data/gt_experiment/
â”œâ”€â”€ pdf/          â† downloaded PDFs
â”œâ”€â”€ xml/          â† GROBID TEI XML
â”œâ”€â”€ markdown/     â† extracted .md files
â””â”€â”€ report_*.json â† coverage report per run
```

---

## All CLI Options

| Flag | Description | Default |
|---|---|---|
| `--query TEXT` | Semantic Scholar search query | required for fetch |
| `--limit N` | Max papers to fetch | 100 |
| `--fetch-only` | Only fetch papers (Step 1) | off |
| `--download-only` | Only download PDFs (Step 2) | off |
| `--convert-only` | Only convert PDFs to XML (Step 3) | off |
| `--extract-only` | Only extract Markdown from XMLs (Step 4) | off |
| `--no-xml` | Skip Step 3 in full pipeline | off |
| `--no-extract` | Skip Step 4 in full pipeline | off |
| `--all-access` | Include non-open-access papers | off |
| `--fields-of-study TEXT` | Filter by field e.g. `"Computer Science"` | none |
| `--dl-limit N` | Max PDFs to download | all |
| `--dl-overwrite` | Re-download existing PDFs | off |
| `--dl-delay N` | Seconds between downloads | 0.5 |
| `--cv-limit N` | Max PDFs to convert | all |
| `--cv-overwrite` | Re-convert existing XMLs | off |
| `--cv-delay N` | Seconds between conversions | 0.1 |
| `--delete-pdfs` | Delete PDFs after conversion | off |
| `--ex-limit N` | Max XMLs to extract | all |
| `--ex-overwrite` | Re-extract existing `.md` files | off |
| `--status` | Show pipeline status and exit | â€” |
| `--reset {status\|full}` | Reset pipeline tracking or full DB | â€” |

---

## Pipeline Stages

| # | Stage | Module | Status |
|---|---|---|---|
| 1 | Fetch papers | `src/pubfetcher/client.py` | âœ… Done |
| 2 | Download PDFs | `src/ingestion/downloader.py` | âœ… Done |
| 3 | Convert PDF â†’ XML | `src/ingestion/converter.py` | âœ… Done |
| 4 | Extract Markdown | `src/ingestion/extractor.py` | âœ… Done |
| 5 | LLM feature extraction | `src/llm/` | ğŸ”² Phase 3 |
| 6 | RAG / Vector search | `src/rag/` | ğŸ”² Phase 4 |

---

## Project Structure

```
IDRD-Pipeline/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py                      â† pipeline entry point
â”‚   â”œâ”€â”€ config.py                    â† all settings in one place
â”‚   â”œâ”€â”€ db/
â”‚   â”‚   â”œâ”€â”€ db.py                    â† PostgreSQL database manager
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ pubfetcher/
â”‚   â”‚   â””â”€â”€ client.py                â† Semantic Scholar API client
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â”œâ”€â”€ downloader.py            â† PDF downloader
â”‚   â”‚   â”œâ”€â”€ converter.py             â† GROBID PDF â†’ TEI XML converter
â”‚   â”‚   â””â”€â”€ extractor.py             â† TEI XML â†’ Markdown extractor
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ db_utils.py              â† shared DB helpers
â”‚       â””â”€â”€ dict_parser.py           â† paper dict parser / exports
â”œâ”€â”€ experiments/
â”‚   â””â”€â”€ ground_truth/
â”‚       â”œâ”€â”€ gt_runner.py             â† isolated GT experiment entry point
â”‚       â”œâ”€â”€ gt_fetcher.py            â† fetch GT papers from Semantic Scholar
â”‚       â”œâ”€â”€ gt_downloader.py         â† download GT PDFs
â”‚       â””â”€â”€ gt_report.py             â† coverage report builder
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ pdf/                         â† downloaded PDFs
â”‚   â”œâ”€â”€ xml/                         â† GROBID TEI XML files
â”‚   â”œâ”€â”€ markdown/                    â† extracted Markdown files
â”‚   â””â”€â”€ ground_truth/
â”‚       â””â”€â”€ ground_truth.csv         â† ground truth paper list
â”œâ”€â”€ data/gt_experiment/              â† isolated GT experiment output
â”‚   â”œâ”€â”€ pdf/
â”‚   â”œâ”€â”€ xml/
â”‚   â”œâ”€â”€ markdown/
â”‚   â””â”€â”€ report_*.json
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ runs/                        â† per-run logs and metadata
â”œâ”€â”€ .env                             â† credentials (do not commit)
â”œâ”€â”€ ROADMAP.md
â””â”€â”€ README.md
```

---

## Installation

```bash
git clone https://github.com/OmericoN/IDRD-Pipeline.git
cd IDRD-Pipeline
pip install -r requirements.txt
```

---

## License

See [LICENSE](LICENSE) file for details.