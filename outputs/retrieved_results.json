[
  {
    "paperId": "ad4a0938c48e61b7827869e4ac3baffd0aefab35",
    "title": "Emerging Properties in Self-Supervised Vision Transformers",
    "year": 2021,
    "abstract": "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) [16] that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder [26], multi-crop training [9], and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base.",
    "url": "https://www.semanticscholar.org/paper/ad4a0938c48e61b7827869e4ac3baffd0aefab35",
    "venue": "IEEE International Conference on Computer Vision",
    "publicationDate": "2021-04-29",
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2104.14294",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2104.14294, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "2062862676",
        "name": "Mathilde Caron",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2113243762",
        "name": "Hugo Touvron",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1806773",
        "name": "Ishan Misra",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2065248680",
        "name": "Herv'e J'egou",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2599292",
        "name": "J. Mairal",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2329288",
        "name": "Piotr Bojanowski",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2319608",
        "name": "Armand Joulin",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 8066,
    "referenceCount": 91,
    "influentialCitationCount": 1496,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "externalIds": {
      "ArXiv": "2104.14294",
      "DBLP": "journals/corr/abs-2104-14294",
      "DOI": "10.1109/ICCV48922.2021.00951",
      "CorpusId": 233444273
    },
    "journal": {
      "name": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)",
      "volume": null,
      "pages": "9630-9640"
    },
    "tldr": "This paper questions if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets) and implements DINO, a form of self-distillation with no labels, which implements the synergy between DINO and ViTs.",
    "citations": [
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "6b8cd4a2ac3e49b98862bc27aa6b91a9e64d7502",
          "title": "FusionKD: Fusion knowledge distillation of vision-language foundation model for strip steel surface defect detection",
          "year": 2026,
          "authors": [
            {
              "authorId": "2014611762",
              "name": "Jiaojiao Su"
            },
            {
              "authorId": "2074295045",
              "name": "Qiwu Luo"
            },
            {
              "authorId": "2108824780",
              "name": "Yibo Wang"
            },
            {
              "authorId": "2249764485",
              "name": "Chunhua Yang"
            },
            {
              "authorId": "2289227147",
              "name": "Weihua Gui"
            },
            {
              "authorId": "2065301833",
              "name": "Janne Heikkilä"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "fbf243de61cd054c95abae59fe95b0e02c95750b",
          "title": "Whole slide image redundancy reduction for efficient pathological diagnosis",
          "year": 2026,
          "authors": [
            {
              "authorId": "2396299123",
              "name": "Shijie Li"
            },
            {
              "authorId": "2265575625",
              "name": "Zhineng Chen"
            },
            {
              "authorId": "49102844",
              "name": "Feng-Jung Chen"
            },
            {
              "authorId": "2269123414",
              "name": "Xieping Gao"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "09bd2a16aa65c6966a5c6a41699db9dc0d42fb6c",
          "title": "Unleash and integrate the power of pre-trained ViTs via feature fusion for open-vocabulary object detection",
          "year": 2026,
          "authors": [
            {
              "authorId": "2342652397",
              "name": "Xiangyu Gao"
            },
            {
              "authorId": "2294555228",
              "name": "Yu Dai"
            },
            {
              "authorId": "1993661016",
              "name": "Taijin Zhao"
            },
            {
              "authorId": "74787012",
              "name": "Benliu Qiu"
            },
            {
              "authorId": "1993431084",
              "name": "Lanxiao Wang"
            },
            {
              "authorId": "66125335",
              "name": "Heqian Qiu"
            },
            {
              "authorId": "144816629",
              "name": "Qingbo Wu"
            },
            {
              "authorId": "2211347525",
              "name": "Hongliang Li"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "b9a085e3d54afddedb903bf62211084c6c691c5a",
          "title": "Towards smarter warehouse perception: integrating adaptive tile segmentation in warehousing object detection pipelines",
          "year": 2026,
          "authors": [
            {
              "authorId": "2296207328",
              "name": "Carlos Clavero"
            },
            {
              "authorId": "2337590813",
              "name": "Miguel A. Patricio"
            },
            {
              "authorId": "2286949490",
              "name": "Jesús García"
            },
            {
              "authorId": "2237787814",
              "name": "J. M. Molina"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "4b023b3fd01a220969e8e463f973a0d8f7ef2d7b",
          "title": "Contrastive learning strategies for better image classification with imbalanced datasets",
          "year": 2026,
          "authors": [
            {
              "authorId": "2276138417",
              "name": "Sudipta Roy"
            },
            {
              "authorId": "2402809210",
              "name": "Rohit Jadhav"
            },
            {
              "authorId": "2041295037",
              "name": "Tanushree Meena"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "8ef9570784e3044ab211d2810d15200aa52ced91",
          "title": "Decoding vision transformer variations for image classification: A guide to performance and usability",
          "year": 2026,
          "authors": [
            {
              "authorId": "2395949759",
              "name": "João Montrezol"
            },
            {
              "authorId": "2220834940",
              "name": "Hugo S. Oliveira"
            },
            {
              "authorId": "2280342404",
              "name": "Hélder P. Oliveira"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "778734e5d3621c5850983a519791b234470e5782",
          "title": "SuperMapNet for long-range and high-accuracy vectorized HD map construction",
          "year": 2026,
          "authors": [
            {
              "authorId": "29972224",
              "name": "Ruqin Zhou"
            },
            {
              "authorId": "2274351923",
              "name": "Chenguang Dai"
            },
            {
              "authorId": "2332447486",
              "name": "Wanshou Jiang"
            },
            {
              "authorId": "2405403116",
              "name": "Yongsheng Zhang"
            },
            {
              "authorId": "2291637446",
              "name": "Zhenchao Zhang"
            },
            {
              "authorId": "49047830",
              "name": "San Jiang"
            }
          ]
        }
      },
      {
        "contexts": [
          "Gradients satisfy leading to ∂ L DINO ∂x = ( W ⊤ ( p − q )) ⊤ J ( x ) .",
          "DINOv2 and MAE assign extremely high cosine similarity to both targets and distractors (often exceeding 0.97), yet exhibit negligible or negative margins between them.",
          "In contrast, CLIP, DINOv2, and MAE frequently select distractors that preserve object identity but violate positional binding, resulting in near-chance performance.",
          "DINO-style objectives minimize cross-entropy between student and teacher outputs: where q is the teacher distribution and W is a projection matrix.",
          "In contrast, CLIP and DINOv2 exhibit a pronounced mid-network collapse (blocks 5–8), with DINOv2 briefly dropping to rank 4 before partial recovery.",
          "In contrast, CLIP, DINOv2, and MAE display rapid spectral decay, concentrating functional sensitivity in a small number of dominant modes.",
          "DINO (Caron et al., 2021), and redundancy-reduction formulations such as VICReg (Bardes et al., 2022a) and Barlow Twins (Zbontar et al., 2021).",
          "For CLIP, DINOv2, and MAE, all three methods (i.e., cosine similarity, kNN, and local PCA) perform near or below chance level (25%).",
          "Clustering and Self-Distillation Objectives DINO / DINOv2.",
          "We evaluate 21 pretrained vision encoders spanning eight distinct objective families: contrastive (Sim-CLR), variance-decorrelation (Barlow Twins, VICReg), non-contrastive clustering (SwAV), self-distillation (DINO, DINOv2), masked prediction (MAE, BEiT, I-JEPA), vision-language (CLIP, SigLIP, EVA-CLIP), and supervised base-lines (ConvNeXt)."
        ],
        "intents": [],
        "isInfluential": true,
        "citingPaper": {
          "paperId": "75ba50a64f226eb206292af20992590e46a802c7",
          "title": "Global Geometry Is Not Enough for Vision Representations",
          "year": 2026,
          "authors": [
            {
              "authorId": null,
              "name": "Jiwan Chung"
            },
            {
              "authorId": null,
              "name": "Seon Joo Kim"
            }
          ]
        }
      },
      {
        "contexts": [
          "In recent years a body of work [12, 26, 27, 43, 54, 55] has focused on using large foundation models including DINO [3, 9, 31] and Stable Diffusion [38] due to their powerful zero shot performance on downstream tasks.",
          "Recent methods [3, 12, 26, 43, 54, 55] using pretrained foundation models [3, 38] have identified that the spatial smoothness property can be provided by Stable Diffusion features due to its strong spatial awareness in contrast to DINO features."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "498831ec43209f50ecdd93f49f57f42d1d7e0dec",
          "title": "Gromov Wasserstein Optimal Transport for Semantic Correspondences",
          "year": 2026,
          "authors": [
            {
              "authorId": "2375058871",
              "name": "Francis Snelgar"
            },
            {
              "authorId": "2261278539",
              "name": "Stephen Gould"
            },
            {
              "authorId": "2261377534",
              "name": "Ming Xu"
            },
            {
              "authorId": "2282207149",
              "name": "Liang Zheng"
            },
            {
              "authorId": "2265751765",
              "name": "Akshay Asthana"
            }
          ]
        }
      },
      {
        "contexts": [
          "We propose a self-supervised quantization learning approach based on the DINO framework, which is a state-of-the-art image-based SSL method (Caron et al., 2021; Oquab et al., 2024; Siméoni et al., 2025)."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "5dbd59a02f68162cba8e40ecc210313c8b89fd72",
          "title": "Multimodal Generative Recommendation for Fusing Semantic and Collaborative Signals",
          "year": 2026,
          "authors": [
            {
              "authorId": "2150687285",
              "name": "Moritz Vandenhirtz"
            },
            {
              "authorId": "2443055",
              "name": "Kaveh Hassani"
            },
            {
              "authorId": "2709180",
              "name": "S. Ghasemlou"
            },
            {
              "authorId": "2333231351",
              "name": "Shuai Shao"
            },
            {
              "authorId": "2329105165",
              "name": "Hamid Eghbalzadeh"
            },
            {
              "authorId": null,
              "name": "Fuchun Peng"
            },
            {
              "authorId": null,
              "name": "Jun Liu"
            },
            {
              "authorId": null,
              "name": "Michael Louis Iuzzolino"
            }
          ]
        }
      },
      {
        "contexts": [
          "For example, DINO Caron et al. (2021) can co-segment objects directly from extracted features Amir et al. (2021); Wang et al. (2023)."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "244ffbdad6d20ff091cee2394c1cc82e86c613a5",
          "title": "FSOD-VFM: Few-Shot Object Detection with Vision Foundation Models and Graph Diffusion",
          "year": 2026,
          "authors": [
            {
              "authorId": null,
              "name": "Chen-Bin Feng"
            },
            {
              "authorId": "2408532531",
              "name": "Youyang Sha"
            },
            {
              "authorId": null,
              "name": "Longfei Liu"
            },
            {
              "authorId": null,
              "name": "Yongjun Yu"
            },
            {
              "authorId": "2408431234",
              "name": "Chi Man Vong"
            },
            {
              "authorId": null,
              "name": "Xuanlong Yu"
            },
            {
              "authorId": null,
              "name": "Xi Shen"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "5845ab51bd94182b398a3e669acd2680c4d43843",
          "title": "LaVPR: Benchmarking Language and Vision for Place Recognition",
          "year": 2026,
          "authors": [
            {
              "authorId": "5463800",
              "name": "Ofer Idan"
            },
            {
              "authorId": "2408531422",
              "name": "Dan Badur"
            },
            {
              "authorId": "2261478376",
              "name": "Yosi Keller"
            },
            {
              "authorId": "36070812",
              "name": "Yoli Shavit"
            }
          ]
        }
      },
      {
        "contexts": [
          "While hybrids like iBOT (Zhou et al., 2021) and DINOv2 (Oquab et al., 2023) attempt to combine these objectives, they still rely heavily on global invariance and forgo pixel reconstruction.",
          "For DINO and MAE, we used mean-pooled dense features, and for sparse models STELLAR and TiTok, we used mean-pooled sparse tokens.",
          "Following (Caron et al., 2020; Darcet et al., 2025), we compute balanced assignments ˜ q ij from q ij using the Sinkhorn-Knopp algorithm (see appendix) without gradient, and minimize Unlike DINOv2 and SwAV which only use Sinkhorn for balancing teacher targets, we explicitly minimize L cluster along with all other objectives.",
          "Joint Embedding (JE) methods, such as the MoCo (He et al., 2020) and DINO (Oquab et al., 2023) families, prioritize global invariance via multi-view alignment, yielding strong semantics but often losing spatial grounding.",
          "The STELLAR framework can be used on a pretrained ViT such as MAE or DINO to leverage the foundation prior and shape it into a sparse holistic representation.",
          "The linear probing and k-NN accuracy of STELLAR surpass reconstruction-feasible representations from all baselines, despite trailing behind the CLS token from DINO, which is infeasible as a reconstruction latent.",
          "Traditional SSL methods such as DINO (Caron et al., 2021) attempt to force invariance onto global representations obtained from these dense grids.",
          "2(b), the sparse tokens of STELLAR enjoys high transformation robustness at DINO level.",
          "In order to learn high-level semantics, Joint Embedding (JE) methods (e.g. the DINO family) impose invariance to spatial transformations, even when the image is cropped to as small as only 5%."
        ],
        "intents": [],
        "isInfluential": true,
        "citingPaper": {
          "paperId": "adb186d899a0c8b250ff5bad5daf52a91ba3c9cd",
          "title": "Learning Sparse Visual Representations via Spatial-Semantic Factorization",
          "year": 2026,
          "authors": [
            {
              "authorId": null,
              "name": "Theodore Zhengde Zhao"
            },
            {
              "authorId": "39620434",
              "name": "Sid Kiblawi"
            },
            {
              "authorId": "2408440243",
              "name": "Jianwei Yang"
            },
            {
              "authorId": "2637252",
              "name": "N. Usuyama"
            },
            {
              "authorId": "73441526",
              "name": "Reuben Tan"
            },
            {
              "authorId": "2268318452",
              "name": "Noel C. F. Codella"
            },
            {
              "authorId": "2264107059",
              "name": "Tristan Naumann"
            },
            {
              "authorId": "2277607019",
              "name": "H. Poon"
            },
            {
              "authorId": null,
              "name": "Mu Wei"
            }
          ]
        }
      },
      {
        "contexts": [
          "These A notable recent development is DINO-WM [30], which leverages pre-trained visual features from self-supervised models (e.g., DINO [2,21]) to construct world models capable of zero-shot planning in unseen environments."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "d95f5219d66b098b23eae5a137cef7266c47b63f",
          "title": "An Empirical Study of World Model Quantization",
          "year": 2026,
          "authors": [
            {
              "authorId": "2347691890",
              "name": "Zhongqian Fu"
            },
            {
              "authorId": null,
              "name": "Tianyi Zhao"
            },
            {
              "authorId": null,
              "name": "Kai Han"
            },
            {
              "authorId": null,
              "name": "Hang Zhou"
            },
            {
              "authorId": null,
              "name": "Xinghao Chen"
            },
            {
              "authorId": null,
              "name": "Yunhe Wang"
            }
          ]
        }
      },
      {
        "contexts": [
          "We report the average value of DINO v1 (Caron et al., 2021) and v2 (Oquab et al., 2024) vision encoders’ embeddings."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "93409d95b6038a56f28157b8d1a5187dca9048ed",
          "title": "Generative Visual Code Mobile World Models",
          "year": 2026,
          "authors": [
            {
              "authorId": "2326993178",
              "name": "Woosung Koh"
            },
            {
              "authorId": "2356793298",
              "name": "Sungjun Han"
            },
            {
              "authorId": "2408451839",
              "name": "Segyu Lee"
            },
            {
              "authorId": "2327832017",
              "name": "Se-young Yun"
            },
            {
              "authorId": null,
              "name": "Jamin Shin"
            }
          ]
        }
      },
      {
        "contexts": [
          "…2018; Wu et al., 2018; Gutmann & Hyv¨arinen, 2010; Chen et al., 2020b; Caron et al., 2020; Wu et al., 2018; He et al., 2020; Radford et al., 2021; Caron et al., 2021; Zbontar et al., 2021; Bardes et al., 2022; Ermolov et al., 2021; Chen & He, 2021; Grill et al., 2020; Assran et al., 2023;…"
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "fda346818f179e878efbbca5036aa5c1d64ea32d",
          "title": "Self-Supervised Learning from Structural Invariance",
          "year": 2026,
          "authors": [
            {
              "authorId": "2299103273",
              "name": "Yipeng Zhang"
            },
            {
              "authorId": "2321265308",
              "name": "Hafez Ghaemi"
            },
            {
              "authorId": "2408484562",
              "name": "Jungyoon Lee"
            },
            {
              "authorId": "3429318",
              "name": "Shahab Bakhtiari"
            },
            {
              "authorId": "2350294427",
              "name": "Eilif B. Muller"
            },
            {
              "authorId": "2378870408",
              "name": "Laurent Charlin"
            }
          ]
        }
      },
      {
        "contexts": [
          "Recently, transformer-based architectures such as Mask2Former (Cheng et al., 2022) and Mask DINO (Li et al., 2023) have been adopted for their global attention mechanisms, enabling stronger contextual reasoning (Voulgaris; Yang et al., 2025).",
          "We evaluate three detection-prompter variants: DeepForest (Zhou & Feng, 2020), a tree detection model; DINO (Caron et al., 2021), a DINO-Swin-L backbone pre-trained on COCO; and SelvaBox (Baudchon et al., 2026), a DINO-Swin-L detection model trained on tropical forest data."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "a0c71cb200456c7799206ebf8164779bddc3a62b",
          "title": "SelvaMask: Segmenting Trees in Tropical Forests and Beyond",
          "year": 2026,
          "authors": [
            {
              "authorId": "2408459502",
              "name": "Simon-Olivier Duguay"
            },
            {
              "authorId": "2372525561",
              "name": "Hugo Baudchon"
            },
            {
              "authorId": "2408459663",
              "name": "Etienne Lalibert'e"
            },
            {
              "authorId": "2408459996",
              "name": "Helene Muller-Landau"
            },
            {
              "authorId": "2388357150",
              "name": "Gonzalo Rivas-Torres"
            },
            {
              "authorId": "2264461387",
              "name": "Arthur Ouaknine"
            }
          ]
        }
      },
      {
        "contexts": [
          "We adopt the ViT-B/16 backbone pretrained on ImageNet-21K and ImageNet-1K, including strong supervised paradigms Sup-21K, Sup-21K/1K (Sup-21K fine-tuned on ImageNet-1K) (Ridnik et al., 2021; Dosovitskiy et al., 2020), and self-supervised paradigms iBOT-21K, iBOT-1K (Zhou et al., 2021), DINO-1K (Caron et al., 2021), and MoCo v3-1K (Chen et al., 2021).",
          "…and ImageNet-1K, including strong supervised paradigms Sup-21K, Sup-21K/1K (Sup-21K fine-tuned on ImageNet-1K) (Ridnik et al., 2021; Dosovitskiy et al., 2020), and self-supervised paradigms iBOT-21K, iBOT-1K (Zhou et al., 2021), DINO-1K (Caron et al., 2021), and MoCo v3-1K (Chen et al., 2021)."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "33d59d9f16beb35bcf72de0b441a208cf0e2a50e",
          "title": "FlyPrompt: Brain-Inspired Random-Expanded Routing with Temporal-Ensemble Experts for General Continual Learning",
          "year": 2026,
          "authors": [
            {
              "authorId": "2282118599",
              "name": "Hongwei Yan"
            },
            {
              "authorId": "2345154312",
              "name": "Guanglong Sun"
            },
            {
              "authorId": null,
              "name": "Kanglei Zhou"
            },
            {
              "authorId": "2386910265",
              "name": "Qian Li"
            },
            {
              "authorId": "2344961589",
              "name": "Liyuan Wang"
            },
            {
              "authorId": "2294823338",
              "name": "Yi Zhong"
            }
          ]
        }
      },
      {
        "contexts": [
          "The adversarial loss adopts the standard GAN formulation: where G θ is our Ada-RefSR model and D ω is the discriminator [5]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "f96db6720976b8e7011ba61dffe878d5f2a8ff87",
          "title": "Trust but Verify: Adaptive Conditioning for Reference-Based Diffusion Super-Resolution via Implicit Reference Correlation Modeling",
          "year": 2026,
          "authors": [
            {
              "authorId": null,
              "name": "Yuan Wang"
            },
            {
              "authorId": null,
              "name": "Yuhao Wan"
            },
            {
              "authorId": "2363676693",
              "name": "Siming Zheng"
            },
            {
              "authorId": "2408526179",
              "name": "Bo Li"
            },
            {
              "authorId": "2293594637",
              "name": "Qibin Hou"
            },
            {
              "authorId": null,
              "name": "Peng-Tao Jiang"
            }
          ]
        }
      },
      {
        "contexts": [
          "Positive-only self-distillation methods (for example, DINO-style training Caron et al. (2021)) may reduce false negatives under imbalance."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "b80d38a0f04423dab9c5abe98ebf87cba3bd3d7b",
          "title": "A Semi-Supervised Pipeline for Generalized Behavior Discovery from Animal-Borne Motion Time Series",
          "year": 2026,
          "authors": [
            {
              "authorId": "7141964",
              "name": "F. Karimi Nejadasl"
            },
            {
              "authorId": "2261870467",
              "name": "Judy Shamoun-Baranes"
            },
            {
              "authorId": "6588091",
              "name": "Eldar Rakhimberdiev"
            }
          ]
        }
      },
      {
        "contexts": [
          "We consider frozen visual encoders without any fine-tuning: • DINO ViT-S/16 Caron et al. (2021) , pretrained via self-supervised learning on ImageNet, • JEPA ViT-B/16 Assran et al. (2023) , a vision model pretrained with a joint embedding predictive architecture, learning structured and semantically meaningful latent representations via self-supervised prediction.",
          "We consider frozen visual encoders without any fine-tuning: • DINO ViT-S/16 Caron et al. (2021) , pretrained via self-supervised learning on ImageNet, • JEPA ViT-B/16 Assran et al. (2023) , a vision model pretrained with a joint embedding predictive architecture, learning structured and…"
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "56fe7716e8cb5291724bbe84f2603650f2abcfa6",
          "title": "Rethinking Test-Time Training: Tilting The Latent Distribution For Few-Shot Source-Free Adaptation",
          "year": 2026,
          "authors": [
            {
              "authorId": "2313095994",
              "name": "T. Syed"
            },
            {
              "authorId": "10849970",
              "name": "Behraj Khan"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "6e47e13f59a51219f783b8428424002a9598d1ea",
          "title": "Few-shot transformers for image classification using masked self-distillation and optimal tokens based global-local feature interactions",
          "year": 2026,
          "authors": [
            {
              "authorId": "2274598546",
              "name": "Binquan Li"
            },
            {
              "authorId": "2335163880",
              "name": "Xin Guo"
            },
            {
              "authorId": "2291532951",
              "name": "Lishuang Gong"
            }
          ]
        }
      },
      {
        "contexts": [
          "More recently, advanced methods [7], [10], [11], [12], [13], [26] have replaced traditional feature extraction backbones with foundation models such as CLIP [27] and DINO [28], [29], signiﬁcantly enhancing model generalisation and robustness across diverse environments."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "82caf395080201cb169e6876b2786fefca92b3e5",
          "title": "Dilated Superpixel Aggregation for Visual Place Recognition",
          "year": 2026,
          "authors": [
            {
              "authorId": "2238401424",
              "name": "Zichao Zeng"
            },
            {
              "authorId": "2274931483",
              "name": "June Moh Goo"
            },
            {
              "authorId": "2238221583",
              "name": "Jan Boehm"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "a12ff4d790ed770c3d1a6363bf0a5ab559b24743",
          "title": "Autonomous Detection of Concrete Cracks Using Self-supervised DinoV2",
          "year": 2026,
          "authors": [
            {
              "authorId": "2338813809",
              "name": "Taoyuan Zhu"
            },
            {
              "authorId": "3069261",
              "name": "Ali Braytee"
            },
            {
              "authorId": "2361756396",
              "name": "Karthick Thiyagarajan"
            },
            {
              "authorId": "2211020310",
              "name": "Xing Zi"
            },
            {
              "authorId": "2388257547",
              "name": "Samir Mustapha"
            },
            {
              "authorId": null,
              "name": "Xian Tao"
            },
            {
              "authorId": "2057223773",
              "name": "Mukesh Prasad"
            }
          ]
        }
      },
      {
        "contexts": [
          "…be categorized into 1) contrastive methods (Chen et al., 2020; He et al., 2020), 2) non-contrastive methods (Zbontar et al., 2021; Bardes et al., 2022; Ermolov et al., 2021), 3) self-distillation methods (Grill et al., 2020; Caron et al., 2021; Chen & He, 2020) based on Balestriero et al. (2023)."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "2e8ccd5997437eeb8001e58204e2b66eaad02142",
          "title": "Rectified LpJEPA: Joint-Embedding Predictive Architectures with Sparse and Maximum-Entropy Representations",
          "year": 2026,
          "authors": [
            {
              "authorId": "2408460095",
              "name": "Yilun Kuang"
            },
            {
              "authorId": "2408462621",
              "name": "Yash Dagade"
            },
            {
              "authorId": "2107677984",
              "name": "Tim G. J. Rudner"
            },
            {
              "authorId": "2277741253",
              "name": "Randall Balestriero"
            },
            {
              "authorId": "2270469816",
              "name": "Yann LeCun"
            }
          ]
        }
      },
      {
        "contexts": [
          "The visual features of DINO [16] have been used in Velocirap-tor [17] together with LiDAR to learn risk aware costmaps, and also in WVN [4] to learn a traversability map online using only vision and velocity feedback."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "2e7760c9ac5b6322a307bbb5d39fff0222dedaf4",
          "title": "Sem-NaVAE: Semantically-Guided Outdoor Mapless Navigation via Generative Trajectory Priors",
          "year": 2026,
          "authors": [
            {
              "authorId": "2408466349",
              "name": "Gonzalo Olguin"
            },
            {
              "authorId": "2408466326",
              "name": "Javier Ruiz-del-Solar"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "eab7ff07731e482830c5424dac959703a97c67a9",
          "title": "Stronger Semantic Encoders Can Harm Relighting Performance: Probing Visual Priors via Augmented Latent Intrinsics",
          "year": 2026,
          "authors": [
            {
              "authorId": "2300038714",
              "name": "Xiaoyan Xing"
            },
            {
              "authorId": null,
              "name": "Xiao Zhang"
            },
            {
              "authorId": "1968574",
              "name": "Sezer Karaoglu"
            },
            {
              "authorId": "2256692141",
              "name": "Theo Gevers"
            },
            {
              "authorId": "2406478072",
              "name": "Anand Bhattad"
            }
          ]
        }
      },
      {
        "contexts": [
          "DINO (Caron et al., 2021; Oquab et al., 2024) simplifies self-supervised training by directly predicting the output of a teacher network and excels at capturing fine-grained details.",
          "On the one hand, vision foundation models (VFMs) like DINO (Caron et al., 2021; Oquab et al., 1 Institute of Network Science and Intelligent Systems, Beijing Jiaotong University, Beijing, China 2 Zhejiang University, Hangzhou, Zhejiang, China.",
          "To compare the individual fine-tuning of DINO (Caron et al., 2021) and CLIP (Li et al., 2023b) with our method, we conducted experiments on Market-1501 and MSMT17."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "a2380e9ea2c3884e9d881a70d99cd756901cb346",
          "title": "DRFormer: A Dual-Regularized Bidirectional Transformer for Person Re-identification",
          "year": 2026,
          "authors": [
            {
              "authorId": "2408458738",
              "name": "Ying Shu"
            },
            {
              "authorId": "2408457387",
              "name": "Pujian Zhan"
            },
            {
              "authorId": null,
              "name": "Huiqi Yang"
            },
            {
              "authorId": null,
              "name": "Hehe Fan"
            },
            {
              "authorId": "2268430572",
              "name": "Youfang Lin"
            },
            {
              "authorId": "2268395114",
              "name": "Kai Lv"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "fcb05e4472d332d1f49959dc84f23876e02ba675",
          "title": "Non-Contrastive Vision-Language Learning with Predictive Embedding Alignment",
          "year": 2026,
          "authors": [
            {
              "authorId": "2366426730",
              "name": "Lukas Kuhn"
            },
            {
              "authorId": "2303655333",
              "name": "Giuseppe Serra"
            },
            {
              "authorId": "2303655270",
              "name": "Florian Buettner"
            }
          ]
        }
      },
      {
        "contexts": [
          "Although recent work has integrated learning-based improvements such as robust correspondence search [47], [55], learned features [56], [57], and differentiable bundle adjustment [3], [58]–[60], the pipeline remains slow, fragile, and cumbersome to deploy outside controlled datasets."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "87e1debf8d24f63c6381526ec2d8cbeba83b06cc",
          "title": "Distill3R: A Pipeline for Democratizing 3D Foundation Models on Commodity Hardware",
          "year": 2026,
          "authors": [
            {
              "authorId": "2408457705",
              "name": "Brandon Leblanc"
            },
            {
              "authorId": "2268759976",
              "name": "Charalambos Poullis"
            }
          ]
        }
      },
      {
        "contexts": [
          "In classical Transformers, visualizing attention matrices often provides meaningful insights into the structure of the input data [68]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "f758f278d87ac17798ab2340549fa65db3a2124b",
          "title": "Quantum Phase Recognition via Quantum Attention Mechanism",
          "year": 2026,
          "authors": [
            {
              "authorId": null,
              "name": "Jin-Long Chen"
            },
            {
              "authorId": "2376442220",
              "name": "Xin Li"
            },
            {
              "authorId": "2397713659",
              "name": "Zhang-Qi Yin"
            }
          ]
        }
      },
      {
        "contexts": [
          "We refer the reader to the references (He et al., 2020; Chen et al., 2020b; Caron et al., 2021) for more details.",
          "Unsupervised (self-supervised) representation learning (Radford et al., 2021; Caron et al., 2021; Chen et al., 2020b; He et al., 2020; Tian et al., 2020; Misra & Maaten, 2020) has shown considerable success in developing powerful and generalizable representations, but often lacks explainabil-ity.",
          "For alignment, we experimented with different pretrained models, including contrastive self-supervised approaches (MoCo V3 (Chen et al., 2021) and DINO (Caron et al., 2021)), a supervised method (ViT (Dosovitskiy et al., 2020)), and a copy detection model (SSCD (Pizzi et al., 2022)).",
          "This method is inspired by experimental findings that demonstrate the inherent capability of models like MOCO (He et al., 2020) and DINO (Caron et al., 2021) to achieve cross-domain alignment."
        ],
        "intents": [],
        "isInfluential": true,
        "citingPaper": {
          "paperId": "87580fe3c4e25150f23229f12f98d248cf7b4157",
          "title": "Unsupervised Synthetic Image Attribution: Alignment and Disentanglement",
          "year": 2026,
          "authors": [
            {
              "authorId": "2343934388",
              "name": "Zongfang Liu"
            },
            {
              "authorId": "2155315836",
              "name": "Guan-Hong Chen"
            },
            {
              "authorId": null,
              "name": "Boyang Sun"
            },
            {
              "authorId": null,
              "name": "Tongliang Liu"
            },
            {
              "authorId": "2334876728",
              "name": "Kun Zhang"
            }
          ]
        }
      },
      {
        "contexts": [
          "Subsequent works strengthen this reconstruction backbone: MambaAD (He et al., 2024a) replaces the decoder with a Mamba-based state space model to better capture long-range dependencies across categories, and Dinomaly (Guo et al., 2025) shows that a carefully regularized reconstruction head on top of strong DINO features already yields state-of-the-art multi-class performance.",
          "…the highest P-AUPRO among all foundations, followed by other contrastive and hybrid encoders such as DINOv2 (Oquab et al., 2023), iBOT (Zhou et al., 2021), DINO (Caron et al., 2021), D-iGPT (Ren et al., 2023), and MoCov3 (Chen et al., 2021), and then by supervised DeiT (Touvron et al., 2021).",
          "DINOv3 (Sim´eoni et al., 2025) yields the highest P-AUPRO among all foundations, followed by other contrastive and hybrid encoders such as DINOv2 (Oquab et al., 2023), iBOT (Zhou et al., 2021), DINO (Caron et al., 2021), D-iGPT (Ren et al., 2023), and MoCov3 (Chen et al., 2021), and then by supervised DeiT (Touvron et al., 2021).",
          "Importantly, the relative ranking of encoders is largely preserved across resolutions: contrastive and hybrid encoders (DINOv3, DINOv2, iBOT, DINO, D-iGPT, MoCoV3) consistently occupy the top of the hierarchy, followed by supervised DeiT and finally the MIM-only encoders.",
          "Our proposed RAD adopts a frozen DINOv3 (Sim´eoni et al., 2025) ViT-B/16 encoder."
        ],
        "intents": [],
        "isInfluential": true,
        "citingPaper": {
          "paperId": "60a9150784bb7320d6dcb7a17c8a08661cd643ac",
          "title": "Is Training Necessary for Anomaly Detection?",
          "year": 2026,
          "authors": [
            {
              "authorId": null,
              "name": "Xingwu Zhang"
            },
            {
              "authorId": "2381361110",
              "name": "Guanxuan Li"
            },
            {
              "authorId": "2268495645",
              "name": "Paul Henderson"
            },
            {
              "authorId": "2240297844",
              "name": "Gerardo Aragon-Camarasa"
            },
            {
              "authorId": "2381048781",
              "name": "Zijun Long"
            }
          ]
        }
      },
      {
        "contexts": [
          "DINO structure distance evaluates the structural similarity between hazy and restored image pairs in the space of DINO-ViT features [74], by leveraging deep spatial features extracted from DINO-ViT and using their similarity as structure distance."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "0809c13037f7883949d47bd5c2128372aafbf74a",
          "title": "Diffusion-leveraged GAN dehazing driven by classification: a two-stage framework for real-world monitoring imagery",
          "year": 2026,
          "authors": [
            {
              "authorId": "52625698",
              "name": "Moheb M. R. Henein"
            },
            {
              "authorId": "1716200538",
              "name": "Mahmoud Ayyad"
            },
            {
              "authorId": null,
              "name": "Kaijian Liu"
            },
            {
              "authorId": "2137535151",
              "name": "Maouane Temimi"
            }
          ]
        }
      },
      {
        "contexts": [
          "Subject Consistency Reward ( R I ): This reward evaluates the preservation of visual details by computing cosine similarity between the subject of the generated image and reference subject in visual feature space such as [21, 22]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "afc94bd3a8b1af3df680e5bedc0e1dcec86a4010",
          "title": "DreamVAR: Taming Reinforced Visual Autoregressive Model for High-Fidelity Subject-Driven Image Generation",
          "year": 2026,
          "authors": [
            {
              "authorId": "2408484020",
              "name": "Xin Jiang"
            },
            {
              "authorId": "2136211561",
              "name": "Jingwen Chen"
            },
            {
              "authorId": "3431141",
              "name": "Yehao Li"
            },
            {
              "authorId": "3202968",
              "name": "Yingwei Pan"
            },
            {
              "authorId": null,
              "name": "Kezhou Chen"
            },
            {
              "authorId": null,
              "name": "Zechao Li"
            },
            {
              "authorId": "2286333106",
              "name": "Ting Yao"
            },
            {
              "authorId": "2070183551",
              "name": "Tao Mei"
            }
          ]
        }
      },
      {
        "contexts": [
          "Since VGGT (Wang et al., 2025) is pre-trained on large-scale 3D datasets and DINO (Caron et al., 2021; Oquab et al., 2024) possesses strong visual understanding and segmentation capabilities, the reconstruction model exhibits a strong geometric prior."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "95de2b28e5c977c407856bb9f75c6469275a49d5",
          "title": "VideoGPA: Distilling Geometry Priors for 3D-Consistent Video Generation",
          "year": 2026,
          "authors": [
            {
              "authorId": null,
              "name": "Hongyang Du"
            },
            {
              "authorId": "2267498972",
              "name": "Junjie Ye"
            },
            {
              "authorId": "2355354968",
              "name": "Xiaoyan Cong"
            },
            {
              "authorId": "2408340676",
              "name": "Runhao Li"
            },
            {
              "authorId": "2408415798",
              "name": "Jingcheng Ni"
            },
            {
              "authorId": "2408350715",
              "name": "Aman Agarwal"
            },
            {
              "authorId": "2408300444",
              "name": "Zeqi Zhou"
            },
            {
              "authorId": null,
              "name": "Zekun Li"
            },
            {
              "authorId": "2408224972",
              "name": "Randall Balestriero"
            },
            {
              "authorId": null,
              "name": "Yue Wang"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "7443b5bf581bdee8db3f5e116125425a7ab462c3",
          "title": "DINO-SAE: DINO Spherical Autoencoder for High-Fidelity Image Reconstruction and Generation",
          "year": 2026,
          "authors": [
            {
              "authorId": "2408341697",
              "name": "Hun Chang"
            },
            {
              "authorId": "2349535520",
              "name": "Byunghee Cha"
            },
            {
              "authorId": "2408338317",
              "name": "Jong Chul Ye"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "6179a02ca6c7eb8e10a861377b83d6df2588c3f3",
          "title": "DVPNet: A New XAI-Based Interpretable Genetic Profiling Framework Using Nucleotide Transformer and Probabilistic Circuits",
          "year": 2026,
          "authors": [
            {
              "authorId": "2408390873",
              "name": "Taishi Kusumoto"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "75e07b67f1d588a41a0969eb06acfd397ebf181a",
          "title": "SANEval: Open-Vocabulary Compositional Benchmarks with Failure-mode Diagnosis",
          "year": 2026,
          "authors": [
            {
              "authorId": "2139439332",
              "name": "Rishav Pramanik"
            },
            {
              "authorId": "2121216719",
              "name": "Ian E. Nielsen"
            },
            {
              "authorId": null,
              "name": "Jeff Smith"
            },
            {
              "authorId": "2408464556",
              "name": "Saurav Pandit"
            },
            {
              "authorId": "2408465657",
              "name": "Ravi P. Ramachandran"
            },
            {
              "authorId": "2357544360",
              "name": "Zhaozheng Yin"
            }
          ]
        }
      },
      {
        "contexts": [
          "Our method outperforms all other approaches on CLIP and DINO metrics except IP-Adapter (Ye et al., 2023).",
          "For evaluation using vision foundation models, we adopt CLIP (Radford et al., 2021) and DINO (Caron et al., 2021) as image encoders to compute feature similarities as a measure of semantic consistency.",
          "For each image, we leverage the object names supplied by Subjects200k as prompts for GroundingDINO (Liu et al., 2024), which generates bounding boxes to accurately localize the objects of interest.",
          "First, for each image, we use the object names provided in Subjects200k as prompts to GroundingDINO (Liu et al., 2024) to extract the corresponding objects of interest.",
          "Specifically, we measure semantic consistency by computing CLIP (Radford et al., 2021) and DINO (Caron et al., 2021) feature similarities between the generated images and the multi-view renderings of the 3D assets."
        ],
        "intents": [],
        "isInfluential": true,
        "citingPaper": {
          "paperId": "932405e989b184da7f2a7eb658ae44e85989676d",
          "title": "RefAny3D: 3D Asset-Referenced Diffusion Models for Image Generation",
          "year": 2026,
          "authors": [
            {
              "authorId": null,
              "name": "Hanzhuo Huang"
            },
            {
              "authorId": "2407461031",
              "name": "Qingyang Bao"
            },
            {
              "authorId": null,
              "name": "Zekai Gu"
            },
            {
              "authorId": "2407685661",
              "name": "Zhongshuo Du"
            },
            {
              "authorId": "2279451650",
              "name": "Cheng Lin"
            },
            {
              "authorId": "2382840774",
              "name": "Yuan Liu"
            },
            {
              "authorId": null,
              "name": "Sibei Yang"
            }
          ]
        }
      },
      {
        "contexts": [
          "In vision, ViT (Dosovitskiy et al., 2021) demonstrated transformers can match convolutional networks, while self-supervised methods like DINO/DINOv2 (Caron et al., 2021; Oquab et al., 2024) and MAE (He et al., 2022) learn rich representations without labels."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "6834dc06c1d782d5829970cabd6e7b231eb69c67",
          "title": "Making Foundation Models Probabilistic via Singular Value Ensembles",
          "year": 2026,
          "authors": [
            {
              "authorId": "26978093",
              "name": "Mehmet Ozgur Turkoglu"
            },
            {
              "authorId": "2336732914",
              "name": "Dominik J. Muhlematter"
            },
            {
              "authorId": "2078701909",
              "name": "Alexander Becker"
            },
            {
              "authorId": "2407459003",
              "name": "Konrad Schindler"
            },
            {
              "authorId": "2302805988",
              "name": "Helge Aasen"
            }
          ]
        }
      },
      {
        "contexts": [
          "On the other hand, VFMs such as DINO (Caron et al., 2021) and SAM (Kirillov et al., 2023) have been widely adopted in foreground object segmentation tasks (Sim´eoni et al., 2021; 2022; Wang et al., 2022).",
          "Methods using only VFMs. Leveraging the robust visual representations of DINO series (Caron et al., 2021; Oquab et al., 2024; Sim´eoni et al., 2025), researchers have developed several unsupervised segmentation methods."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "b5d07eacf162df6f0ee4f4e710cd089437345678",
          "title": "Bidirectional Cross-Perception for Open-Vocabulary Semantic Segmentation in Remote Sensing Imagery",
          "year": 2026,
          "authors": [
            {
              "authorId": "2407517843",
              "name": "Jianzheng Wang"
            },
            {
              "authorId": "2407492434",
              "name": "Huan Ni"
            }
          ]
        }
      },
      {
        "contexts": [
          "Accuracy is measured by OwL-ViT [13], and fidelity is measured by DINO [3].",
          "For image editing, we focus on fidelity, using the DINO [3] metric, which measures how well the edited image preserves the original content while integrating the desired changes."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "c2ea10d654ef2efc3ab7fe7f4c1f8040c57708c7",
          "title": "SimGraph: A Unified Framework for Scene Graph-Based Image Generation and Editing",
          "year": 2026,
          "authors": [
            {
              "authorId": "2376328941",
              "name": "Thanh-Nhan Vo"
            },
            {
              "authorId": "2238535257",
              "name": "Trong-Thuan Nguyen"
            },
            {
              "authorId": "2196405720",
              "name": "Tam V. Nguyen"
            },
            {
              "authorId": "2372466410",
              "name": "Minh-Triet Tran"
            }
          ]
        }
      },
      {
        "contexts": [
          "This contrasts with image or video classification tasks, which operate effectively at lower resolutions (e.g., 224 × 224) [26], [27]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "e1bfb1d38129f8bf610081f88e8a790c837d602c",
          "title": "ViTMAlis: Towards Latency-Critical Mobile Video Analytics with Vision Transformers",
          "year": 2026,
          "authors": [
            {
              "authorId": "2112175756",
              "name": "Miao Zhang"
            },
            {
              "authorId": "2316763246",
              "name": "Guanzhen Wu"
            },
            {
              "authorId": "2218340999",
              "name": "Hao Fang"
            },
            {
              "authorId": "2401381034",
              "name": "Yifei Zhu"
            },
            {
              "authorId": "144935272",
              "name": "Fangxin Wang"
            },
            {
              "authorId": "2311637314",
              "name": "Ruixiao Zhang"
            },
            {
              "authorId": "2293765670",
              "name": "Jiangchuan Liu"
            }
          ]
        }
      },
      {
        "contexts": [
          "For producing the views, we opted to use the same recipe as DINO (Caron et al., 2021): at least two global views and N local views.",
          "We compare against a supervised baseline and six SSL methods: DINO (Caron et al., 2021), BYOL (Grill et al., 2020), Barlow Twins (Zbontar et al., 2021), SimCLR (Chen et al., 2020), VICReg (Bardes et al., 2022) and LeJEPA (Balestriero & Le-Cun, 2025)."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "e22c9eddb89eb8e1a3c672a4db3871c0e982d00a",
          "title": "Hypersolid: Emergent Vision Representations via Short-Range Repulsion",
          "year": 2026,
          "authors": [
            {
              "authorId": "2407463262",
              "name": "Esteban Rodr'iguez-Betancourt"
            },
            {
              "authorId": "2244660965",
              "name": "Edgar Casasola-Murillo"
            }
          ]
        }
      },
      {
        "contexts": [
          "R ELATED WORKS a) Pretrained vision based models for robot learning: The computer vision community has developed large-scale pretraining strategies for visual representation learning, yielding models such as MoCo [19], DINO [20], DINOv2 [21], and CLIP [22].",
          "Following DINOSAUR [12], we use a strong pretrained visual backbone (e.g. DINO [20]) and use a decoder to reconstruct the backbone features to avoid the pitfalls of pixel-level reconstruction.",
          "We updated the original DINO [20] backbone with the more recent DINOv2 [21]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "b80a2aa97b9d7777c5f224fb3406604698547755",
          "title": "Spotlighting Task-Relevant Features: Object-Centric Representations for Better Generalization in Robotic Manipulation",
          "year": 2026,
          "authors": [
            {
              "authorId": "2238243296",
              "name": "Alexandre Chapin"
            },
            {
              "authorId": "2362301203",
              "name": "Bruno Machado"
            },
            {
              "authorId": "2183481965",
              "name": "Emmanuel Dellandr'ea"
            },
            {
              "authorId": "2407250707",
              "name": "Liming Chen"
            }
          ]
        }
      },
      {
        "contexts": [
          "In a similar vein to vision SSL models (Chen et al., 2020; Caron et al., 2021; Oquab et al., 2023), this learning objective aims to learn invariant linguistic content from different augmented view of the input waveform, τ ( x ) , where x is waveform and τ ∼ T is data augmentation."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "0608bf2fdaef760ccd66ad9af52bcd577c5c3499",
          "title": "Sylber 2.0: A Universal Syllable Embedding",
          "year": 2026,
          "authors": [
            {
              "authorId": "2064541737",
              "name": "Cheol Jun Cho"
            },
            {
              "authorId": "2325480695",
              "name": "Nicholas Lee"
            },
            {
              "authorId": "2258955104",
              "name": "Alan W. Black"
            },
            {
              "authorId": "1692246",
              "name": "G. Anumanchipalli"
            }
          ]
        }
      },
      {
        "contexts": [
          "…Similarly, we can also exploit min Plug the softmax parametrized multinomial distribution of P ( z | x ) in (78) to the power iteration matching of stationary latent variable model (89) and (90), we obtain recovering the exact objective of DINO (Caron et al., 2021) and SwAV (Caron et al., 2020).",
          "• DINO (Caron et al., 2021) and SwAV (Caron et al., 2020).",
          "The major difference between DINO and SwAV lies in the implementation of P t ( z | x ) in (91): In DINO, P t ( z | x ) is simply exploiting the model in previous iteration; while in SwAV exploits the optimal transport solution to (84), the same as E-step in SeLa (Asano et al., 2019).",
          "Then, the t -th iteration update for P t +1 ( z | x ) can be written as a variational optimization, i.e. , min Similarly, we can also exploit min Plug the softmax parametrized multinomial distribution of P ( z | x ) in (78) to the power iteration matching of stationary latent variable model (89) and (90), we obtain recovering the exact objective of DINO (Caron et al., 2021) and SwAV (Caron et al., 2020).",
          "The DIstillation with NO label (DINO) (Caron et al., 2021; Oquab et al., 2023; Sim´eoni et al., 2025) and Swapping Assignments between multiple Views (SwAV) (Caron et al., 2020) can both be recast as latent-variable spectral representation."
        ],
        "intents": [],
        "isInfluential": true,
        "citingPaper": {
          "paperId": "cb73b3461eeb50f55e90d1447aad9e2af1d2d410",
          "title": "Spectral Ghost in Representation Learning: from Component Analysis to Self-Supervised Learning",
          "year": 2026,
          "authors": [
            {
              "authorId": "2295666916",
              "name": "Bo Dai"
            },
            {
              "authorId": "2327411681",
              "name": "Na Li"
            },
            {
              "authorId": "50319359",
              "name": "D. Schuurmans"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "e3a72ab2809b551490e3ae785f27b73cde959bb5",
          "title": "Test-Time Adaptation for Anomaly Segmentation via Topology-Aware Optimal Transport Chaining",
          "year": 2026,
          "authors": [
            {
              "authorId": "2247612737",
              "name": "Ali Zia"
            },
            {
              "authorId": "2355944131",
              "name": "Usman Ali"
            },
            {
              "authorId": "2355987530",
              "name": "Umer Ramzan"
            },
            {
              "authorId": "2386525768",
              "name": "Abdul Rehman"
            },
            {
              "authorId": "47414128",
              "name": "Abdelwahed Khamis"
            },
            {
              "authorId": "2386525167",
              "name": "Wei Xiang"
            }
          ]
        }
      },
      {
        "contexts": [
          "Many of these methods adopt Vision Transformer (ViT) architectures [10] and training objectives such as masked image modeling [13], contrastive learning [4,5,7], or hybrids that combine both paradigms [26,39]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "f8d3465b766269960925de0e160befa648b0098a",
          "title": "STORM: Slot-based Task-aware Object-centric Representation for robotic Manipulation",
          "year": 2026,
          "authors": [
            {
              "authorId": "2238243296",
              "name": "Alexandre Chapin"
            },
            {
              "authorId": "2183481965",
              "name": "Emmanuel Dellandr'ea"
            },
            {
              "authorId": "2407250707",
              "name": "Liming Chen"
            }
          ]
        }
      }
    ]
  },
  {
    "paperId": "736973165f98105fec3729b7db414ae4d80fcbeb",
    "title": "Scalable Diffusion Models with Transformers",
    "year": 2022,
    "abstract": "We explore a new class of diffusion models based on the transformer architecture. We train latent diffusion models of images, replacing the commonly-used U-Net backbone with a transformer that operates on latent patches. We analyze the scalability of our Diffusion Transformers (DiTs) through the lens of forward pass complexity as measured by Gflops. We find that DiTs with higher Gflops—through increased transformer depth/width or increased number of input tokens—consistently have lower FID. In addition to possessing good scalability properties, our largest DiT-XL/2 models outperform all prior diffusion models on the class-conditional ImageNet 512×512 and 256×256 benchmarks, achieving a state-of-the-art FID of 2.27 on the latter.",
    "url": "https://www.semanticscholar.org/paper/736973165f98105fec3729b7db414ae4d80fcbeb",
    "venue": "IEEE International Conference on Computer Vision",
    "publicationDate": "2022-12-19",
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2212.09748",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2212.09748, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "35235273",
        "name": "William S. Peebles",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1817030",
        "name": "Saining Xie",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 4442,
    "referenceCount": 68,
    "influentialCitationCount": 576,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "externalIds": {
      "DBLP": "journals/corr/abs-2212-09748",
      "ArXiv": "2212.09748",
      "DOI": "10.1109/ICCV51070.2023.00387",
      "CorpusId": 254854389
    },
    "journal": {
      "name": "2023 IEEE/CVF International Conference on Computer Vision (ICCV)",
      "volume": null,
      "pages": "4172-4182"
    },
    "tldr": "A new class of diffusion models based on the transformer architecture is explored, replacing the commonly-used U-Net backbone with a transformer that operates on latent patches that outperform all prior diffusion models on the class-conditional ImageNet 512×512 and 256×256 benchmarks.",
    "citations": [
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "09bd2a16aa65c6966a5c6a41699db9dc0d42fb6c",
          "title": "Unleash and integrate the power of pre-trained ViTs via feature fusion for open-vocabulary object detection",
          "year": 2026,
          "authors": [
            {
              "authorId": "2342652397",
              "name": "Xiangyu Gao"
            },
            {
              "authorId": "2294555228",
              "name": "Yu Dai"
            },
            {
              "authorId": "1993661016",
              "name": "Taijin Zhao"
            },
            {
              "authorId": "74787012",
              "name": "Benliu Qiu"
            },
            {
              "authorId": "1993431084",
              "name": "Lanxiao Wang"
            },
            {
              "authorId": "66125335",
              "name": "Heqian Qiu"
            },
            {
              "authorId": "144816629",
              "name": "Qingbo Wu"
            },
            {
              "authorId": "2211347525",
              "name": "Hongliang Li"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "edd0ee7e89f8d849d276b098a30c5ccc2ab10ed2",
          "title": "An unsupervised low-light image enhancement method for underground scenes using low- and high-frequency enhancement calibration",
          "year": 2026,
          "authors": [
            {
              "authorId": "2378217416",
              "name": "Zhuli Ren"
            },
            {
              "authorId": "2398751482",
              "name": "Hongyang Zhu"
            },
            {
              "authorId": "2398275436",
              "name": "Ruifu Yuan"
            },
            {
              "authorId": "2400861139",
              "name": "Jinlong Zhang"
            },
            {
              "authorId": "2399760849",
              "name": "Lei Zhao"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "bdd7267758c53d9d8a9853935c5b1555f4bd2162",
          "title": "EEGDiffuser: Label-guided EEG signals synthesis via diffusion model for BCI applications",
          "year": 2026,
          "authors": [
            {
              "authorId": "2110190007",
              "name": "Jiquan Wang"
            },
            {
              "authorId": "2335807684",
              "name": "Sha Zhao"
            },
            {
              "authorId": "2334998084",
              "name": "Zhiling Luo"
            },
            {
              "authorId": "2238215066",
              "name": "Yangxuan Zhou"
            },
            {
              "authorId": "2310841609",
              "name": "Shijian Li"
            },
            {
              "authorId": "2256580248",
              "name": "Gang Pan"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "5cb282119dacaad6f99633b6c514584bdc9b2b71",
          "title": "SG-DiTs: A physics-informed diffusion-transformer hybrid framework for reliable battery RUL prediction",
          "year": 2026,
          "authors": [
            {
              "authorId": "2408236534",
              "name": "Jing Sun"
            },
            {
              "authorId": "2407251485",
              "name": "Jingfan Gao"
            }
          ]
        }
      },
      {
        "contexts": [
          "Our Spiral RoPE achieves an FID of 1.74, outperforming both DiT (2.27) and SiT (2.06).",
          "We train DiT models of various sizes (S, B, L, XL) with patch sizes of 2 (denoted as /2 in the original paper) on ImageNet 256 × 256 .",
          "Table 4 compares our Spiral RoPE against the original DiT baseline and SiT.",
          "Class-conditional image generation on ImageNet 256 × 256 using DiT.",
          "We also evaluate Spiral RoPE on class-conditional image generation using Diffusion Transformers (DiT) (Pee-bles & Xie, 2023).",
          "We compare models trained for 400k steps with the standard training hyperparameters of DiT.",
          "All models are trained for 400k steps with the original training recipe of DiT (Peebles & Xie, 2023).",
          "Baseline results with supermarks * are taken from U-DiTs (Tian et al., 2024)."
        ],
        "intents": [],
        "isInfluential": true,
        "citingPaper": {
          "paperId": "2c7cf79e7a5b7e2d12fa7553f27ffe2a9284f094",
          "title": "Spiral RoPE: Rotate Your Rotary Positional Embeddings in the 2D Plane",
          "year": 2026,
          "authors": [
            {
              "authorId": null,
              "name": "Haoyu Liu"
            },
            {
              "authorId": null,
              "name": "Sucheng Ren"
            },
            {
              "authorId": null,
              "name": "Tingyu Zhu"
            },
            {
              "authorId": null,
              "name": "Peng Wang"
            },
            {
              "authorId": null,
              "name": "Cihang Xie"
            },
            {
              "authorId": "2253485882",
              "name": "Alan L. Yuille"
            },
            {
              "authorId": null,
              "name": "Zeyu Zheng"
            },
            {
              "authorId": null,
              "name": "Feng Wang"
            }
          ]
        }
      },
      {
        "contexts": [
          "By modifying text prompts, we can change the weather and time of scenes, as shown in Fig."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "ea4f07ad73250afa7f52720f322479f91f7973b4",
          "title": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation",
          "year": 2026,
          "authors": [
            {
              "authorId": "2353414153",
              "name": "Zhuoran Yang"
            },
            {
              "authorId": "2320315111",
              "name": "Xi Guo"
            },
            {
              "authorId": null,
              "name": "Chenjing Ding"
            },
            {
              "authorId": "2320304828",
              "name": "Chiyu Wang"
            },
            {
              "authorId": "2321149750",
              "name": "Wei Wu"
            },
            {
              "authorId": "2406536524",
              "name": "Yanyong Zhang"
            }
          ]
        }
      },
      {
        "contexts": [
          "Modern high-fidelity image generators are dominated by diffusion-based models [6, 14, 23, 29] and large autoregressive models [30, 32, 33]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "c799afcaa614a18eab95eb3c896b88923ea3ef33",
          "title": "Composable Visual Tokenizers with Generator-Free Diagnostics of Learnability",
          "year": 2026,
          "authors": [
            {
              "authorId": null,
              "name": "Bingchen Zhao"
            },
            {
              "authorId": null,
              "name": "Qiushan Guo"
            },
            {
              "authorId": null,
              "name": "Ye Wang"
            },
            {
              "authorId": null,
              "name": "Yixuan Huang"
            },
            {
              "authorId": "2273645764",
              "name": "Zhonghua Zhai"
            },
            {
              "authorId": null,
              "name": "Yu Tian"
            }
          ]
        }
      },
      {
        "contexts": [
          "Current conditioning strategies typically rely on class labels (Peebles & Xie, 2023) or textual descriptions (Rombach et al., 2022)."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "ce5f767274c577b30bc1b1461a9433dcd85b7a32",
          "title": "Test-Time Conditioning with Representation-Aligned Visual Features",
          "year": 2026,
          "authors": [
            {
              "authorId": "2323375448",
              "name": "Nicolas Sereyjol-Garros"
            },
            {
              "authorId": "2334568816",
              "name": "Ellington Kirby"
            },
            {
              "authorId": "2264977664",
              "name": "Victor Letzelter"
            },
            {
              "authorId": "2403986942",
              "name": "Victor Besnier"
            },
            {
              "authorId": "40313071",
              "name": "Nermin Samet"
            }
          ]
        }
      },
      {
        "contexts": [
          "Latent Diffusion Models (LDMs) [3, 24] improve efficiency by operating in compressed latent spaces, while DiT-based architectures [22] further enhance scalability and spatiotemporal consistency for video generation."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "64ae43d71b18f87d5d345a7b95c8eb2d220e6e9c",
          "title": "3D-Aware Implicit Motion Control for View-Adaptive Human Video Generation",
          "year": 2026,
          "authors": [
            {
              "authorId": null,
              "name": "Zhixue Fang"
            },
            {
              "authorId": null,
              "name": "Xu He"
            },
            {
              "authorId": null,
              "name": "Songlin Tang"
            },
            {
              "authorId": null,
              "name": "Haoxian Zhang"
            },
            {
              "authorId": null,
              "name": "Qingfeng Li"
            },
            {
              "authorId": null,
              "name": "Xiaoqiang Liu"
            },
            {
              "authorId": "2363570130",
              "name": "Pengfei Wan"
            },
            {
              "authorId": "2385564054",
              "name": "Kun Gai"
            }
          ]
        }
      },
      {
        "contexts": [
          "Diffusion Models (Sohl-Dickstein et al., 2015; Peebles & Xie, 2023) have demonstrated great potential in image synthesis (Dhariwal & Nichol, 2021; Rombach et al., 2022), video generation (Ho et al., 2022), and various discriminative tasks (Li et al., 2023), owing to their superior distribution…"
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "99f75c56a528aa63dc95d57f506f7cf024042864",
          "title": "Enhancing Foundation VLM Robustness to Missing Modality: Scalable Diffusion for Bi-directional Feature Restoration",
          "year": 2026,
          "authors": [
            {
              "authorId": null,
              "name": "Wei Dai"
            },
            {
              "authorId": null,
              "name": "Haoyu Wang"
            },
            {
              "authorId": null,
              "name": "Honghao Chang"
            },
            {
              "authorId": null,
              "name": "Lijun He"
            },
            {
              "authorId": null,
              "name": "Fan Li"
            },
            {
              "authorId": null,
              "name": "Jian Sun"
            },
            {
              "authorId": "2369966933",
              "name": "Haixia Bi"
            }
          ]
        }
      },
      {
        "contexts": [
          "In addition, we use a class token as the first input, which along with AdaLN [20] is used to condition on class labels with classifier-free guidance [10]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "cc10faf6501a40f6a6d80e7c70f5ac0031c96d8b",
          "title": "Progressive Checkerboards for Autoregressive Multiscale Image Generation",
          "year": 2026,
          "authors": [
            {
              "authorId": "2408533076",
              "name": "David Eigen"
            }
          ]
        }
      },
      {
        "contexts": [
          "The introduction of Diffusion Transformers (DiT) [20, 35] enabled better modeling of global spatiotemporal dependencies, leading to large-scale models like Sora [33], Seaweed [38], Hun-yuanVideo [31], and Wan [42]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "210bc92b584cb04346ef8d634f82fc8839b3c8ab",
          "title": "LIVE: Long-horizon Interactive Video World Modeling",
          "year": 2026,
          "authors": [
            {
              "authorId": null,
              "name": "Junchao Huang"
            },
            {
              "authorId": null,
              "name": "Ziyang Ye"
            },
            {
              "authorId": null,
              "name": "Xinting Hu"
            },
            {
              "authorId": null,
              "name": "Tianyu He"
            },
            {
              "authorId": null,
              "name": "Guiyu Zhang"
            },
            {
              "authorId": null,
              "name": "Shaoshuai Shi"
            },
            {
              "authorId": "2304555579",
              "name": "Jiang Bian"
            },
            {
              "authorId": null,
              "name": "Li Jiang"
            }
          ]
        }
      },
      {
        "contexts": [
          "Moreover, a single DiT (Peebles & Xie, 2023) backbone struggles to handle complex multimodal reasoning when scaling to diverse real-world personalized generation scenarios."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "0e6f60b86c50b170331c46a1dd339f20f473f54b",
          "title": "Hierarchical Concept-to-Appearance Guidance for Multi-Subject Image Generation",
          "year": 2026,
          "authors": [
            {
              "authorId": null,
              "name": "Yijia Xu"
            },
            {
              "authorId": null,
              "name": "Zihao Wang"
            },
            {
              "authorId": null,
              "name": "Jinshi Cui"
            }
          ]
        }
      },
      {
        "contexts": [
          "Most follow a two-stage pipeline: (i) learning a VAE [29, 30] that encodes 3D objects into a latent space [7, 31–33], and (ii) training a latent generative model on these latents [30, 34, 35]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "d9492f8f51aa316dce7434699700fc7a71436a17",
          "title": "PnP-U3D: Plug-and-Play 3D Framework Bridging Autoregression and Diffusion for Unified Understanding and Generation",
          "year": 2026,
          "authors": [
            {
              "authorId": null,
              "name": "Yongwei Chen"
            },
            {
              "authorId": null,
              "name": "Tianyi Wei"
            },
            {
              "authorId": null,
              "name": "Yushi Lan"
            },
            {
              "authorId": "2382765175",
              "name": "Zhaoyang Lyu"
            },
            {
              "authorId": null,
              "name": "Shangchen Zhou"
            },
            {
              "authorId": null,
              "name": "Xudong Xu"
            },
            {
              "authorId": null,
              "name": "Xingang Pan"
            }
          ]
        }
      },
      {
        "contexts": [
          "…DDPM (Ho et al., 2020) and accelerated sampling with DDIM (Song et al., 2021), latent diffusion (Rom-bach et al., 2022) that underpins Stable Diffusion and SDXL (Podell et al., 2024), and transformer-style diffusion backbones such as DiT (Peebles & Xie, 2023) and PixArt-α (Chen et al., 2024a).",
          "Representative advances include DDPM (Ho et al., 2020) and accelerated sampling with DDIM (Song et al., 2021), latent diffusion (Rom-bach et al., 2022) that underpins Stable Diffusion and SDXL (Podell et al., 2024), and transformer-style diffusion backbones such as DiT (Peebles & Xie, 2023) and PixArt-α (Chen et al., 2024a)."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "5b7b1bc6d7e5b4e1cd2eeb680aa787e1b64df763",
          "title": "ELIQ: A Label-Free Framework for Quality Assessment of Evolving AI-Generated Images",
          "year": 2026,
          "authors": [
            {
              "authorId": null,
              "name": "Xinyue Li"
            },
            {
              "authorId": null,
              "name": "Zhiming Xu"
            },
            {
              "authorId": null,
              "name": "Zhichao Zhang"
            },
            {
              "authorId": null,
              "name": "Zhaolin Cai"
            },
            {
              "authorId": null,
              "name": "Sijing Wu"
            },
            {
              "authorId": "2246414",
              "name": "Xiongkuo Min"
            },
            {
              "authorId": null,
              "name": "Yitong Chen"
            },
            {
              "authorId": "2335558386",
              "name": "Guangtao Zhai"
            }
          ]
        }
      },
      {
        "contexts": [
          "Following on these advancements, [45, 57, 66] apply diffusion transformer [37] to generate 3D geometry with generalization across diverse datasets through extensive training."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "a409565e78f27b463865ddfa4571cb2cb12c9f69",
          "title": "SceneLinker: Compositional 3D Scene Generation via Semantic Scene Graph from RGB Sequences",
          "year": 2026,
          "authors": [
            {
              "authorId": null,
              "name": "Seok-Young Kim"
            },
            {
              "authorId": null,
              "name": "Dooyoung Kim"
            },
            {
              "authorId": "2038492983",
              "name": "W. Cho"
            },
            {
              "authorId": null,
              "name": "Hail Song"
            },
            {
              "authorId": null,
              "name": "Suji Kang"
            },
            {
              "authorId": "2359456949",
              "name": "Woontack Woo"
            }
          ]
        }
      },
      {
        "contexts": [
          "In practice, high-fidelity and fast sampling typically rely on explicit time conditioning and tightly coupled schedules and parameterizations (Nichol & Dhariwal, 2021; Karras et al., 2022; Ho & Salimans, 2022; Lu et al., 2022; Peebles & Xie, 2023).",
          "Prior work improves sample quality or reduces sampling steps by refining schedules, coefficients, and conditioning mechanisms (Nichol & Dhariwal, 2021; Karras et al., 2022; Ho & Salimans, 2022; Lu et al., 2022; Peebles & Xie, 2023)."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "0c41659544ec18d54f2a38473d05fb1f5a847b6c",
          "title": "Distance Marching for Generative Modeling",
          "year": 2026,
          "authors": [
            {
              "authorId": null,
              "name": "Zimo Wang"
            },
            {
              "authorId": "2270017650",
              "name": "Ishit Mehta"
            },
            {
              "authorId": null,
              "name": "Haolin Lu"
            },
            {
              "authorId": null,
              "name": "Chung-En Sun"
            },
            {
              "authorId": null,
              "name": "Ge Yan"
            },
            {
              "authorId": "27836724",
              "name": "Tsui-Wei Weng"
            },
            {
              "authorId": null,
              "name": "Tzu-Mao Li"
            }
          ]
        }
      },
      {
        "contexts": [
          "A Diffusion Transformer (DiT) (Peebles & Xie, 2023) progressively denoises the latent tensors and yields the output ˆ V 1: T ∈ T × H × W × 3 after the VAE decoding process."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "7ab66b64cecb641b64e91be79d43ed34ebf97f60",
          "title": "BridgeV2W: Bridging Video Generation Models to Embodied World Models via Embodiment Masks",
          "year": 2026,
          "authors": [
            {
              "authorId": null,
              "name": "Yixiang Chen"
            },
            {
              "authorId": null,
              "name": "Peiyan Li"
            },
            {
              "authorId": null,
              "name": "Jiabing Yang"
            },
            {
              "authorId": null,
              "name": "Keji He"
            },
            {
              "authorId": null,
              "name": "Xiangnan Wu"
            },
            {
              "authorId": null,
              "name": "Yuan Xu"
            },
            {
              "authorId": null,
              "name": "Kai Wang"
            },
            {
              "authorId": null,
              "name": "Jing Liu"
            },
            {
              "authorId": null,
              "name": "Nianfeng Liu"
            },
            {
              "authorId": null,
              "name": "Yan Huang"
            },
            {
              "authorId": null,
              "name": "Liang Wang"
            }
          ]
        }
      },
      {
        "contexts": [
          "…for shallow versus deep DiT blocks, indicating that different generative stages utilize distinct semantic levels from the text encoder. et al., 2025; Kong et al., 2024; Ma et al., 2025; Cai et al., 2025; Wan et al., 2025), particularly via Diffusion Trans-formers (DiTs; Peebles & Xie, 2023).",
          "Text conditioning has evolved from U-Net cross-attention (Ronneberger et al., 2015) to DiT’s adaLN-Zero (Peebles & Xie, 2023)."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "80237ffe68a0c8fab7064d532f731e68181e05a9",
          "title": "Semantic Routing: Exploring Multi-Layer LLM Feature Weighting for Diffusion Transformers",
          "year": 2026,
          "authors": [
            {
              "authorId": null,
              "name": "Bozhou Li"
            },
            {
              "authorId": null,
              "name": "Yushuo Guan"
            },
            {
              "authorId": null,
              "name": "Haolin Li"
            },
            {
              "authorId": "2363576244",
              "name": "Bohan Zeng"
            },
            {
              "authorId": null,
              "name": "Yiyan Ji"
            },
            {
              "authorId": null,
              "name": "Yue Ding"
            },
            {
              "authorId": "2363570130",
              "name": "Pengfei Wan"
            },
            {
              "authorId": "2385564054",
              "name": "Kun Gai"
            },
            {
              "authorId": null,
              "name": "Yuanxing Zhang"
            },
            {
              "authorId": null,
              "name": "Wentao Zhang"
            }
          ]
        }
      },
      {
        "contexts": [
          "Recent PixNerd (Wang et al., 2025a) employs a DiT to predict neural field parameters for each patch, rendering pixel velocities akin to test-time training.",
          "Latent diffusion models (Rombach et al., 2022; Peebles & Xie, 2023; Ma et al., 2024; Labs, 2024) split generation into two stages.",
          "DeCo (Ma et al., 2025), DiP (Chen et al., 2025c), and PixelDiT (Yu et al., 2025) propose to introduce an additional pixel decoder to learn the hard high-frequency signals.",
          "REPA-E (Leng et al., 2025) attempts to jointly optimize the VAE and DiT in an end-to-end fashion.",
          "SiT (Ma et al., 2024) further validated the DiT with linear flow diffusion.",
          "Consequently, VAEs have become a fundamental component in modern diffusion models (Peebles & Xie, 2023; Karras et al., 2024; Yue et al., 2024; Wang et al., 2024; Teng et al., 2024; Song et al., 2025; Gao et al., 2023b; Yao et al., 2024; Gao et al., 2023a).",
          "The pioneering DiT (Peebles & Xie, 2023) introduced transformers into diffusion models, replacing the U-Net (Bao et al., 2023; Dhariwal & Nichol, 2021)."
        ],
        "intents": [],
        "isInfluential": true,
        "citingPaper": {
          "paperId": "82438730fc01688e509dfc900b3f8680793cfab8",
          "title": "PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss",
          "year": 2026,
          "authors": [
            {
              "authorId": "2305570681",
              "name": "Zehong Ma"
            },
            {
              "authorId": null,
              "name": "Ruihan Xu"
            },
            {
              "authorId": null,
              "name": "Shiliang Zhang"
            }
          ]
        }
      },
      {
        "contexts": [
          "For iCCDM, we use the CCDM UNet [23] as the network backbone for all experiments except for Steering Angle ( 256 × 256 ), where DiT [39] is adopted due to its superior performance.",
          "For the 256 × 256 experiments, we propose adopting the vision transformer-based DiT backbone [39] due to its superior performance."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "5f5b878082a5065166f0aaa959e30943ac4daaa0",
          "title": "Enhancing Diffusion-Based Quantitatively Controllable Image Generation via Matrix-Form EDM and Adaptive Vicinal Training",
          "year": 2026,
          "authors": [
            {
              "authorId": null,
              "name": "Xin Ding"
            },
            {
              "authorId": "2375042213",
              "name": "Yun Chen"
            },
            {
              "authorId": "2374926187",
              "name": "Sen Zhang"
            },
            {
              "authorId": "2300086934",
              "name": "Kao Zhang"
            },
            {
              "authorId": null,
              "name": "Nenglun Chen"
            },
            {
              "authorId": "2374474392",
              "name": "Peibei Cao"
            },
            {
              "authorId": null,
              "name": "Yongwei Wang"
            },
            {
              "authorId": null,
              "name": "Fei Wu"
            }
          ]
        }
      },
      {
        "contexts": [
          "Conditional generation enables data synthesis guided by rich contextual information, ranging from class labels (Peebles and Xie, 2023), input signals (Wang et al., 2024a) to auxiliary modalities (Esser et al., 2024; Wan et al., 2025; Wei et al., 2025)."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "e1d66f3b0ded3a4a4ec2d55c34c62a2d7c6f37f6",
          "title": "Geometry- and Relation-Aware Diffusion for EEG Super-Resolution",
          "year": 2026,
          "authors": [
            {
              "authorId": "2385443153",
              "name": "Laura Yao"
            },
            {
              "authorId": "2382943030",
              "name": "Gengwei Zhang"
            },
            {
              "authorId": null,
              "name": "Moajjem Chowdhury"
            },
            {
              "authorId": null,
              "name": "Yunmei Liu"
            },
            {
              "authorId": "2408466685",
              "name": "Tianlong Chen"
            }
          ]
        }
      },
      {
        "contexts": [
          "In particular, the latest break-throughs in video generation (Wan et al., 2025; Kong et al., 2024; Li et al., 2024) based on diffusion models (Song et al., 2020a;b; Peebles & Xie, 2023) have revealed the potential for modeling the physical world."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "5e58bf187597e7cbcc54f0190bd6b891793d3cae",
          "title": "Infinite-World: Scaling Interactive World Models to 1000-Frame Horizons via Pose-Free Hierarchical Memory",
          "year": 2026,
          "authors": [
            {
              "authorId": null,
              "name": "Ruiqi Wu"
            },
            {
              "authorId": "2408482918",
              "name": "Xuanhua He"
            },
            {
              "authorId": null,
              "name": "Meng Cheng"
            },
            {
              "authorId": null,
              "name": "Tianyu Yang"
            },
            {
              "authorId": null,
              "name": "Yong Zhang"
            },
            {
              "authorId": "2363849193",
              "name": "Zhuoliang Kang"
            },
            {
              "authorId": null,
              "name": "Xunliang Cai"
            },
            {
              "authorId": null,
              "name": "Xiaoming Wei"
            },
            {
              "authorId": "2314345003",
              "name": "Chunle Guo"
            },
            {
              "authorId": "2259644335",
              "name": "Chongyi Li"
            },
            {
              "authorId": null,
              "name": "Ming-Ming Cheng"
            }
          ]
        }
      },
      {
        "contexts": [
          "These diffusion-based approaches (Rombach et al., 2022; Zhang et al., 2025; Ramesh et al., 2022; Peebles & Xie, 2023) typically formulate image synthesis as text-conditioned iterative denoising."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "e451909086afa480078cf55c9b4b69554abd8ce8",
          "title": "InfoTok: Regulating Information Flow for Capacity-Constrained Shared Visual Tokenization in Unified MLLMs",
          "year": 2026,
          "authors": [
            {
              "authorId": "2110141073",
              "name": "Lv Tang"
            },
            {
              "authorId": null,
              "name": "Tianyi Zheng"
            },
            {
              "authorId": "2408526177",
              "name": "Bo Li"
            },
            {
              "authorId": null,
              "name": "Xingyu Li"
            }
          ]
        }
      },
      {
        "contexts": [
          "With diffusion transformers (DiTs) demonstrating strong scalability (Bao et al., 2023; Peebles & Xie, 2023), many works have introduced DiT-based large-scale video models (Lin et al., 2024; Zheng et al., 2024; Polyak et al., 2024; Yang et al., 2024; HaCohen et al., 2024; Kong et al., 2024; Ma et…"
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "a60b183ff647ffb41090edfa66eddde9d8fbdbe8",
          "title": "Causal Forcing: Autoregressive Diffusion Distillation Done Right for High-Quality Real-Time Interactive Video Generation",
          "year": 2026,
          "authors": [
            {
              "authorId": null,
              "name": "Hongzhou Zhu"
            },
            {
              "authorId": null,
              "name": "Min Zhao"
            },
            {
              "authorId": null,
              "name": "Guande He"
            },
            {
              "authorId": null,
              "name": "Hang Su"
            },
            {
              "authorId": "2399563",
              "name": "Chongxuan Li"
            },
            {
              "authorId": null,
              "name": "Jun Zhu"
            }
          ]
        }
      },
      {
        "contexts": [
          "We parameterize the diffusion backbone of our LoMDM with the modified diffusion transformer architecture (Peebles & Xie, 2023) from Lou et al. (2024); Sahoo et al. (2024a)."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "473bb8d5d0a7183d3ab002e4d96ad1a41a2b046f",
          "title": "Unifying Masked Diffusion Models with Various Generation Orders and Beyond",
          "year": 2026,
          "authors": [
            {
              "authorId": null,
              "name": "Chunsan Hong"
            },
            {
              "authorId": null,
              "name": "Sanghyun Lee"
            },
            {
              "authorId": "2391767144",
              "name": "Jong Chul Ye"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "e38e19a5a371450a72b009c1f9b583401cf98f52",
          "title": "ProxyImg: Towards Highly-Controllable Image Representation via Hierarchical Disentangled Proxy Embedding",
          "year": 2026,
          "authors": [
            {
              "authorId": null,
              "name": "Ye Chen"
            },
            {
              "authorId": "2373013517",
              "name": "Yupeng Zhu"
            },
            {
              "authorId": "2399105421",
              "name": "Xiongzhen Zhang"
            },
            {
              "authorId": null,
              "name": "Zhewen Wan"
            },
            {
              "authorId": "2408469559",
              "name": "Yingzhe Li"
            },
            {
              "authorId": "2371477260",
              "name": "Wenjun Zhang"
            },
            {
              "authorId": "2382917881",
              "name": "Bingbing Ni"
            }
          ]
        }
      },
      {
        "contexts": [
          "…progress across artificial intelligence generated content (AIGC), powering breakthroughs in natural language processing and reasoning (OpenAI, 2024; Hu et al., 2024), video generation (Peebles & Xie, 2023), agent work-flows (Wang et al., 2024a), and robotic controls (Sanghai & Brown, 2024)."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "120a22cf3c0cf76a157b8bac8575100c1137df4c",
          "title": "BAPS: A Fine-Grained Low-Precision Scheme for Softmax in Attention via Block-Aware Precision reScaling",
          "year": 2026,
          "authors": [
            {
              "authorId": null,
              "name": "Zisheng Ye"
            },
            {
              "authorId": null,
              "name": "Xiaoyu He"
            },
            {
              "authorId": "2408491876",
              "name": "Maoyuan Song"
            },
            {
              "authorId": "2408466853",
              "name": "Guoliang Qiu"
            },
            {
              "authorId": null,
              "name": "Chao Liao"
            },
            {
              "authorId": null,
              "name": "Chen Wu"
            },
            {
              "authorId": "2408506484",
              "name": "Yonggang Sun"
            },
            {
              "authorId": "2408419034",
              "name": "Zhichun Li"
            },
            {
              "authorId": null,
              "name": "Xiaoru Xie"
            },
            {
              "authorId": null,
              "name": "Yuanyong Luo"
            },
            {
              "authorId": "2367202331",
              "name": "Hu Liu"
            },
            {
              "authorId": null,
              "name": "Pinyan Lu"
            },
            {
              "authorId": null,
              "name": "Heng Liao"
            }
          ]
        }
      },
      {
        "contexts": [
          "A common instantiation combines an AR LLM with diffusion trans-formers (DiT) (Peebles & Xie, 2023) to translate high-level semantics into high-fidelity outputs.",
          "To support image generation, models like GLM-Image (Z.AI , 2026) typically use an AR LLM to understand the input and then connect it to a diffusion transformer (DiT) (Peebles & Xie, 2023) for visual synthesis."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "0d083b53a61a2f2329f31f9679313a481e00788d",
          "title": "vLLM-Omni: Fully Disaggregated Serving for Any-to-Any Multimodal Models",
          "year": 2026,
          "authors": [
            {
              "authorId": "2113653096",
              "name": "Peiqi Yin"
            },
            {
              "authorId": null,
              "name": "Jiangyun Zhu"
            },
            {
              "authorId": null,
              "name": "Han Gao"
            },
            {
              "authorId": null,
              "name": "Chenguang Zheng"
            },
            {
              "authorId": null,
              "name": "Yongxiang Huang"
            },
            {
              "authorId": "2408479804",
              "name": "Taichang Zhou"
            },
            {
              "authorId": null,
              "name": "Ruirui Yang"
            },
            {
              "authorId": "2408472272",
              "name": "Weizhi Liu"
            },
            {
              "authorId": "2408470855",
              "name": "Weiqing Chen"
            },
            {
              "authorId": "2408437820",
              "name": "Canlin Guo"
            },
            {
              "authorId": "2333237348",
              "name": "Didan Deng"
            },
            {
              "authorId": "2408424294",
              "name": "Zifeng Mo"
            },
            {
              "authorId": null,
              "name": "Cong Wang"
            },
            {
              "authorId": "2408444745",
              "name": "James Cheng"
            },
            {
              "authorId": null,
              "name": "Roger Wang"
            },
            {
              "authorId": "2408438488",
              "name": "Hongsheng Liu"
            }
          ]
        }
      },
      {
        "contexts": [
          "For language modeling experiments with MDLM, we follow the official model implementation and training setup 5 , where the backbone architecture is DiT (Peebles & Xie, 2023)."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "367a074dfa7f57cee78d4cc996dfc18c839c241b",
          "title": "Sentence Curve Language Models",
          "year": 2026,
          "authors": [
            {
              "authorId": "1941953949",
              "name": "DongNyeong Heo"
            },
            {
              "authorId": "2408478188",
              "name": "Heelyoul Choi"
            }
          ]
        }
      },
      {
        "contexts": [
          "…(Achiam et al., 2023; Bai et al., 2023; Yang et al., 2025a; Liu et al., 2024; Bai et al., 2025a; Chen et al., 2024b; Team et al., 2024; Lu et al., 2024) and diffusion models (Liu et al., 2022; Lipman et al., 2022; Labs, 2024; Peebles & Xie, 2023) have achieved remarkable results in various fields.",
          "With the advancement of diffusion models (Liu et al., 2022; Lipman et al., 2022; Labs, 2024; Peebles & Xie, 2023; Li et al., 2024; Shi et al., 2025b; Wang et al., 2025b; Tong et al., 2026; An et al., 2025), the quality of image, video generation and editing (Liu et al., 2025b; DeepMind, 2025;…"
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "e45e94459fdcfc7845dffde920765a86bcafdf09",
          "title": "Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks",
          "year": 2026,
          "authors": [
            {
              "authorId": "2363576244",
              "name": "Bohan Zeng"
            },
            {
              "authorId": null,
              "name": "Kaixin Zhu"
            },
            {
              "authorId": "2397615208",
              "name": "Daili Hua"
            },
            {
              "authorId": "2382942860",
              "name": "Bozhou Li"
            },
            {
              "authorId": "2382918622",
              "name": "Chengzhuo Tong"
            },
            {
              "authorId": null,
              "name": "Yuran Wang"
            },
            {
              "authorId": null,
              "name": "Xinyi Huang"
            },
            {
              "authorId": "2405067807",
              "name": "Yifan Dai"
            },
            {
              "authorId": null,
              "name": "Zixiang Zhang"
            },
            {
              "authorId": "2408443909",
              "name": "Yifan Yang"
            },
            {
              "authorId": null,
              "name": "Zhou Liu"
            },
            {
              "authorId": "2396449193",
              "name": "Hao Liang"
            },
            {
              "authorId": "2330059651",
              "name": "Xiaochen Ma"
            },
            {
              "authorId": "2363570661",
              "name": "Ruichuan An"
            },
            {
              "authorId": "2168279268",
              "name": "Tianyi Bai"
            },
            {
              "authorId": "2372582333",
              "name": "Hongcheng Gao"
            },
            {
              "authorId": null,
              "name": "Junbo Niu"
            },
            {
              "authorId": null,
              "name": "Yang Shi"
            },
            {
              "authorId": "2340719980",
              "name": "Xinlong Chen"
            },
            {
              "authorId": null,
              "name": "Yue Ding"
            },
            {
              "authorId": null,
              "name": "Minglei Shi"
            },
            {
              "authorId": "2331326539",
              "name": "Kai Zeng"
            },
            {
              "authorId": null,
              "name": "Yiwen Tang"
            },
            {
              "authorId": null,
              "name": "Yuanxing Zhang"
            },
            {
              "authorId": "2318189903",
              "name": "Peng-Jun Wan"
            },
            {
              "authorId": "2305033532",
              "name": "Xintao Wang"
            },
            {
              "authorId": "2372959123",
              "name": "Wentao Zhang"
            }
          ]
        }
      },
      {
        "contexts": [
          "We design a highly efficient yet effective diffusion backbone by combining the advantages of U-Net Ronneberger et al. (2015) and diffusion transformer (DiT) Peebles and Xie (2023)."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "95b088ec6ddd9afc37679e277d7a7a2dfcef222e",
          "title": "Teacher-Guided Student Self-Knowledge Distillation Using Diffusion Model",
          "year": 2026,
          "authors": [
            {
              "authorId": "2363927721",
              "name": "Yu Wang"
            },
            {
              "authorId": null,
              "name": "Chuanguang Yang"
            },
            {
              "authorId": "2127813",
              "name": "Zhulin An"
            },
            {
              "authorId": null,
              "name": "Weilun Feng"
            },
            {
              "authorId": "2349830182",
              "name": "Jiarui Zhao"
            },
            {
              "authorId": null,
              "name": "Chengqing Yu"
            },
            {
              "authorId": "2247874007",
              "name": "Libo Huang"
            },
            {
              "authorId": "3070516",
              "name": "Boyu Diao"
            },
            {
              "authorId": null,
              "name": "Yongjun Xu"
            }
          ]
        }
      },
      {
        "contexts": [
          "In recent years, large-scale Diffusion models [21, 22, 18, 2] have become the new paradigm for text-to-image tasks."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "a50dbee3eac6b29a0b703641e8e04b8449e558c3",
          "title": "Leveraging Latent Vector Prediction for Localized Control in Image Generation via Diffusion Models",
          "year": 2026,
          "authors": [
            {
              "authorId": "2408466750",
              "name": "Pablo Domingo-Gregorio"
            },
            {
              "authorId": "2408467486",
              "name": "Javier Ruiz-Hidalgo"
            }
          ]
        }
      },
      {
        "contexts": [
          "To reduce training and inference complexity, this work adopts a lightweight DiT [13] to learn the MIMO channel prior."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "c49fb8b679bc4c63c8ec7c35174c7e8ff30abedb",
          "title": "Sampling-Free Diffusion Transformers for Low-Complexity MIMO Channel Estimation",
          "year": 2026,
          "authors": [
            {
              "authorId": "2111320272",
              "name": "Zhixiong Chen"
            },
            {
              "authorId": "2330839851",
              "name": "Hyundong Shin"
            },
            {
              "authorId": "2279912677",
              "name": "Arumugam Nallanathan"
            }
          ]
        }
      },
      {
        "contexts": [
          "Diffusion Transformers (DiTs) (Peebles & Xie, 2023; Xing et al., 2024; Shen et al., 2025) have emerged as the dominant architecture for generative modeling by treating diverse modalities as sequences of latent tokens, effectively capi-talizing on the scalability of transformers."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "2de972d239e51d19459e8f43ae8b9bd9f4fe102f",
          "title": "Token Pruning for In-Context Generation in Diffusion Transformers",
          "year": 2026,
          "authors": [
            {
              "authorId": null,
              "name": "Junqing Lin"
            },
            {
              "authorId": "2408522918",
              "name": "Xingyu Zheng"
            },
            {
              "authorId": null,
              "name": "Pei Cheng"
            },
            {
              "authorId": "2408462462",
              "name": "Bin Fu"
            },
            {
              "authorId": "46969142",
              "name": "Jingwei Sun"
            },
            {
              "authorId": "2091052605",
              "name": "Guangzhong Sun"
            }
          ]
        }
      },
      {
        "contexts": [
          "Recently, Diffusion Transformers [13, 14] (DiTs) have served as the backbone of state-of-the-art generation frameworks, with flow-matching [15, 16] adopted as the prevailing training scheme."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "71e7b92426270a4a43105e68b7e03c7a0e7ef2be",
          "title": "UniReason 1.0: A Unified Reasoning Framework for World Knowledge Aligned Image Generation and Editing",
          "year": 2026,
          "authors": [
            {
              "authorId": null,
              "name": "Dianyi Wang"
            },
            {
              "authorId": "2408528103",
              "name": "Chaofan Ma"
            },
            {
              "authorId": null,
              "name": "Feng Han"
            },
            {
              "authorId": null,
              "name": "Size Wu"
            },
            {
              "authorId": null,
              "name": "Wei Song"
            },
            {
              "authorId": null,
              "name": "Yibin Wang"
            },
            {
              "authorId": "2338346463",
              "name": "Zhixiong Zhang"
            },
            {
              "authorId": null,
              "name": "Tianhang Wang"
            },
            {
              "authorId": null,
              "name": "Siyuan Wang"
            },
            {
              "authorId": "2307981771",
              "name": "Zhongyu Wei"
            },
            {
              "authorId": "2386931557",
              "name": "Jiaqi Wang"
            }
          ]
        }
      },
      {
        "contexts": [
          "Traditional video diffusion models [19, 24] denoise all frames simultaneously, using a fixed window size.",
          "By conditioning the network on ∆ t through adaptive layernorm (AdaLN, [24]), the model learns to modulate its internal activations as a function of both diffusion progress and physical timescale."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "38409cc7a742bdde4edb652751a5d3017e14a6a0",
          "title": "Scalable Spatio-Temporal SE(3) Diffusion for Long-Horizon Protein Dynamics",
          "year": 2026,
          "authors": [
            {
              "authorId": "2249827609",
              "name": "Nima Shoghi"
            },
            {
              "authorId": null,
              "name": "Yuxuan Liu"
            },
            {
              "authorId": "2292401311",
              "name": "Yuning Shen"
            },
            {
              "authorId": "2408464056",
              "name": "Rob Brekelmans"
            },
            {
              "authorId": "2408432340",
              "name": "Pan Li"
            },
            {
              "authorId": "2408440443",
              "name": "Quanquan Gu"
            }
          ]
        }
      },
      {
        "contexts": [
          "…2020) has led to remarkable advances across a wide range of applications, including image generation (Dhariwal & Nichol, 2021; Rombach et al., 2022; Peebles & Xie, 2023), text-to-image generation (Saharia et al., 2022; Zhang et al., 2023; Gu et al., 2022), and video synthesis (Ho et al., 2022;…"
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "269f81e9b9d22e4d5a2703a9cc0193fbb41d1f56",
          "title": "Self-Rewarding Sequential Monte Carlo for Masked Diffusion Language Models",
          "year": 2026,
          "authors": [
            {
              "authorId": "2283122604",
              "name": "Ziwei Luo"
            },
            {
              "authorId": "2401749126",
              "name": "Ziqi Jin"
            },
            {
              "authorId": null,
              "name": "Lei Wang"
            },
            {
              "authorId": "2363348163",
              "name": "Lidong Bing"
            },
            {
              "authorId": "2253398525",
              "name": "Thomas B. Schon"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "661a4040eaeb3438532ee503a9e7e75eb8b54381",
          "title": "Expert-Data Alignment Governs Generation Quality in Decentralized Diffusion Models",
          "year": 2026,
          "authors": [
            {
              "authorId": "2342275351",
              "name": "Marcos Villagra"
            },
            {
              "authorId": "2384135588",
              "name": "Bidhan Roy"
            },
            {
              "authorId": "1411436226",
              "name": "Raihan Seraj"
            },
            {
              "authorId": null,
              "name": "Zhiying Jiang"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "678e44ae4a536dfb0b568a6b2666395a99b9f2eb",
          "title": "GDiT: A graph-prior-guided diffusion transformer for semantic-controllable remote sensing image synthesis",
          "year": 2026,
          "authors": [
            {
              "authorId": "2327741732",
              "name": "Kai Deng"
            },
            {
              "authorId": "2258530062",
              "name": "Xiangyun Hu"
            },
            {
              "authorId": "2112720363",
              "name": "Yibing Xiong"
            },
            {
              "authorId": "2367676265",
              "name": "Aokun Liang"
            },
            {
              "authorId": "2403872436",
              "name": "Jiong Xu"
            }
          ]
        }
      },
      {
        "contexts": [
          "The Transformer encoder is composed of several key components: positional encoding, multihead attention layers, feedforward neural networks, residual connections, and layer normalization [23]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "5baeb988bb9f1ef38bc2899f95c4ec3d9c3a8186",
          "title": "Fault Detection of Microcracks in Permanent Magnet of Permanent Magnet Synchronous Linear Motors Based on Magnetic Surface Information Time-Domain and Gray Texture Features",
          "year": 2026,
          "authors": [
            {
              "authorId": "2274912905",
              "name": "Le Yin"
            },
            {
              "authorId": "2109761312",
              "name": "Jiwen Zhao"
            },
            {
              "authorId": "9311645",
              "name": "Zhenbao Pan"
            },
            {
              "authorId": "2274677009",
              "name": "Zixiang Yu"
            },
            {
              "authorId": "2274917122",
              "name": "Rui Xu"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "872907173028d5778d3a68fa615f0d086108b602",
          "title": "Unstructured diffusion generative design of metamaterials for irregular design domains",
          "year": 2026,
          "authors": [
            {
              "authorId": "2326554633",
              "name": "Haoyu Wang"
            },
            {
              "authorId": "2407146273",
              "name": "Zongliang Du"
            },
            {
              "authorId": "2406543620",
              "name": "Yue Mei"
            },
            {
              "authorId": "2406825587",
              "name": "Xiudong Li"
            },
            {
              "authorId": "2386087892",
              "name": "Shan Tang"
            }
          ]
        }
      },
      {
        "contexts": [
          "…related work lies in image editing meth-ods (Brooks et al., 2023; Kawar et al., 2023; Shi et al., 2024; Zhang et al., 2023; 2025), which have made remarkable progress with the advancement of text-to-image diffusion models (Podell et al., 2023; Esser et al., 2024; Peebles & Xie, 2023; Labs, 2024)."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "2a566fa7c1e4f129fc1202f6b513210f71aa4da6",
          "title": "ReLayout: Versatile and Structure-Preserving Design Layout Editing via Relation-Aware Design Reconstruction",
          "year": 2026,
          "authors": [
            {
              "authorId": "2266401773",
              "name": "Jiawei Lin"
            },
            {
              "authorId": "2313479906",
              "name": "Shizhao Sun"
            },
            {
              "authorId": null,
              "name": "Danqing Huang"
            },
            {
              "authorId": "2337781408",
              "name": "Ting Liu"
            },
            {
              "authorId": "2337777907",
              "name": "Ji Li"
            },
            {
              "authorId": "2330188810",
              "name": "Jiang Bian"
            }
          ]
        }
      },
      {
        "contexts": [
          "It consists of 8 flow transformer layers, each conditioned on observation features and the flow-matching timestep via an adaptive layer norm (adaLN) MLP [56]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "088211a917be33e10ffe9f9060fede92d83066e8",
          "title": "A Systematic Study of Data Modalities and Strategies for Co-training Large Behavior Models for Robot Manipulation",
          "year": 2026,
          "authors": [
            {
              "authorId": "2408490826",
              "name": "Fanqi Lin"
            },
            {
              "authorId": "2284685268",
              "name": "Kushal Arora"
            },
            {
              "authorId": "72847120",
              "name": "Jean-Pierre Mercat"
            },
            {
              "authorId": "2349476640",
              "name": "Haruki Nishimura"
            },
            {
              "authorId": "2300432426",
              "name": "Paarth Shah"
            },
            {
              "authorId": null,
              "name": "Chen Xu"
            },
            {
              "authorId": null,
              "name": "Mengchao Zhang"
            },
            {
              "authorId": "10708100",
              "name": "Mark Zolotas"
            },
            {
              "authorId": "2372919634",
              "name": "Maya Angeles"
            },
            {
              "authorId": "2372918868",
              "name": "Owen Pfannenstiehl"
            },
            {
              "authorId": "82375749",
              "name": "Andrew Beaulieu"
            },
            {
              "authorId": "2260336204",
              "name": "Jose Barreiros"
            }
          ]
        }
      },
      {
        "contexts": [
          "Diffusion Transformers (DiTs) (Peebles & Xie, 2023) have demonstrated impressive performance and scalability in generating high-fidelity images and videos, leading to their widespread adoption across diverse visual generation tasks (Arnab et al., 2021; Hong et al., 2022; Wan et al., 2025)."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "ee2fc22212ae914edc35e15b3050f80c4722b250",
          "title": "PISA: Piecewise Sparse Attention Is Wiser for Efficient Diffusion Transformers",
          "year": 2026,
          "authors": [
            {
              "authorId": "2316677355",
              "name": "Haopeng Li"
            },
            {
              "authorId": "2320725011",
              "name": "Shitong Shao"
            },
            {
              "authorId": null,
              "name": "Wenliang Zhong"
            },
            {
              "authorId": null,
              "name": "Zikai Zhou"
            },
            {
              "authorId": null,
              "name": "Lichen Bai"
            },
            {
              "authorId": null,
              "name": "Hui Xiong"
            },
            {
              "authorId": "2311626289",
              "name": "Zeke Xie"
            }
          ]
        }
      },
      {
        "contexts": [
          "Meanwhile, the second category employs Transformer-based denoisers [22], including Latte [19], SORA [4], CogVideoX [34], Vidu [2], and Hunyuan-DiT [18]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "c3925bf72abe91e7b28370279863c278266bbba6",
          "title": "MTC-VAE: Multi-Level Temporal Compression with Content Awareness",
          "year": 2026,
          "authors": [
            {
              "authorId": null,
              "name": "Yubo Dong"
            },
            {
              "authorId": null,
              "name": "Linchao Zhu"
            }
          ]
        }
      },
      {
        "contexts": [
          "Consistent with prior work (Sahoo et al., 2024; Schiff et al., 2024; Sahoo et al., 2025), we parameterize both XDLM and baselines using the modified Diffusion Transformer (DiT) architecture (Peebles & Xie, 2023) adapted from (Lou et al., 2023)."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "11ef08210ccb33c04671fdc8f7ebd654c0a81389",
          "title": "Balancing Understanding and Generation in Discrete Diffusion Models",
          "year": 2026,
          "authors": [
            {
              "authorId": "2279865167",
              "name": "Yue Liu"
            },
            {
              "authorId": "2279864665",
              "name": "Yuzhong Zhao"
            },
            {
              "authorId": null,
              "name": "Zheyong Xie"
            },
            {
              "authorId": null,
              "name": "Qixiang Ye"
            },
            {
              "authorId": "2397371936",
              "name": "Jianbin Jiao"
            },
            {
              "authorId": "2375157229",
              "name": "Yao Hu"
            },
            {
              "authorId": null,
              "name": "Shaosheng Cao"
            },
            {
              "authorId": null,
              "name": "Yunfan Liu"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "87e1debf8d24f63c6381526ec2d8cbeba83b06cc",
          "title": "Distill3R: A Pipeline for Democratizing 3D Foundation Models on Commodity Hardware",
          "year": 2026,
          "authors": [
            {
              "authorId": "2408457705",
              "name": "Brandon Leblanc"
            },
            {
              "authorId": "2268759976",
              "name": "Charalambos Poullis"
            }
          ]
        }
      },
      {
        "contexts": [
          "Training-free Alignment Framework for Flow Matching Given that state-of-the-art models are grounded in Diffusion Transformers [30] and flow matching [23], we present the exact form of flow-matching guidance in the theorem below."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "cf1b85f371588846523673af667d70878a43ace6",
          "title": "Alignment of Diffusion Model and Flow Matching for Text-to-Image Generation",
          "year": 2026,
          "authors": [
            {
              "authorId": "1796267433",
              "name": "Yidong Ouyang"
            },
            {
              "authorId": null,
              "name": "Liyan Xie"
            },
            {
              "authorId": "2382764733",
              "name": "Hongyuan Zha"
            },
            {
              "authorId": null,
              "name": "Guang Cheng"
            }
          ]
        }
      },
      {
        "contexts": [
          "Pre-trained in high-quality real-world images, large-scale image generation models (Esser et al., 2021; Rombach et al., 2022; Peebles & Xie, 2023; Tian et al., 2024; Esser et al., 2024; Liu et al., 2024) are considered proficient in fitting complex image distributions."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "9f05238ef6c3119e7283dc6afac278c10e3eba7c",
          "title": "Bridging Degradation Discrimination and Generation for Universal Image Restoration",
          "year": 2026,
          "authors": [
            {
              "authorId": "2290061849",
              "name": "Jiakui Hu"
            },
            {
              "authorId": null,
              "name": "Zhengjian Yao"
            },
            {
              "authorId": "2128093991",
              "name": "Lujia Jin"
            },
            {
              "authorId": "2336275856",
              "name": "Yanye Lu"
            }
          ]
        }
      }
    ]
  },
  {
    "paperId": "47f7ec3d0a5e6e83b6768ece35206a94dc81919c",
    "title": "Taming Transformers for High-Resolution Image Synthesis",
    "year": 2020,
    "abstract": "Designed to learn long-range interactions on sequential data, transformers continue to show state-of-the-art results on a wide variety of tasks. In contrast to CNNs, they contain no inductive bias that prioritizes local interactions. This makes them expressive, but also computationally infeasible for long sequences, such as high-resolution images. We demonstrate how combining the effectiveness of the inductive bias of CNNs with the expressivity of transformers enables them to model and thereby synthesize high-resolution images. We show how to (i) use CNNs to learn a context-rich vocabulary of image constituents, and in turn (ii) utilize transformers to efficiently model their composition within high-resolution images. Our approach is readily applied to conditional synthesis tasks, where both non-spatial information, such as object classes, and spatial information, such as segmentations, can control the generated image. In particular, we present the first results on semantically-guided synthesis of megapixel images with transformers. Project page at https://git.io/JLlvY.",
    "url": "https://www.semanticscholar.org/paper/47f7ec3d0a5e6e83b6768ece35206a94dc81919c",
    "venue": "Computer Vision and Pattern Recognition",
    "publicationDate": "2020-12-17",
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2012.09841",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2012.09841, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "35175531",
        "name": "Patrick Esser",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1660819540",
        "name": "Robin Rombach",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1796707",
        "name": "B. Ommer",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 3842,
    "referenceCount": 82,
    "influentialCitationCount": 626,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "externalIds": {
      "ArXiv": "2012.09841",
      "MAG": "3111551570",
      "DBLP": "journals/corr/abs-2012-09841",
      "DOI": "10.1109/CVPR46437.2021.01268",
      "CorpusId": 229297973
    },
    "journal": {
      "name": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
      "volume": null,
      "pages": "12868-12878"
    },
    "tldr": "It is demonstrated how combining the effectiveness of the inductive bias of CNNs with the expressivity of transformers enables them to model and thereby synthesize high-resolution images.",
    "citations": [
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "4e627dcfc91130dae48de7893914397b1335f948",
          "title": "Controllable image-Guided generation via dynamic gaussian spectral modulation",
          "year": 2026,
          "authors": [
            {
              "authorId": "1390900165",
              "name": "Shuocheng Wang"
            },
            {
              "authorId": "2299432530",
              "name": "Qingfeng Wu"
            },
            {
              "authorId": "2368484504",
              "name": "Yuanbo Xing"
            },
            {
              "authorId": "2339474619",
              "name": "Mengyuan Ge"
            }
          ]
        }
      },
      {
        "contexts": [
          "Subsequent work improved perceptual fidelity and sample sharpness via adversarial and perceptual losses [9] and increased representational capacity via multi-stage or residual quantization [17].",
          "Latent diffusion [27] popularized performing diffusion in the latent space of a pretrained tokenizer [9], substantially reducing compute and enabling scalable text/image-conditional generation."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "c799afcaa614a18eab95eb3c896b88923ea3ef33",
          "title": "Composable Visual Tokenizers with Generator-Free Diagnostics of Learnability",
          "year": 2026,
          "authors": [
            {
              "authorId": null,
              "name": "Bingchen Zhao"
            },
            {
              "authorId": null,
              "name": "Qiushan Guo"
            },
            {
              "authorId": null,
              "name": "Ye Wang"
            },
            {
              "authorId": null,
              "name": "Yixuan Huang"
            },
            {
              "authorId": "2273645764",
              "name": "Zhonghua Zhai"
            },
            {
              "authorId": null,
              "name": "Yu Tian"
            }
          ]
        }
      },
      {
        "contexts": [
          "Token-based approaches like Reparo [18] advanced the field using VQ-GAN [11] codebooks to map video content into discrete indices, achieving dramatic bitrate reductions while maintaining perceptual quality."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "e8024a57218021c12dfacbd9ce3b33877ad22d92",
          "title": "Morphe: High-Fidelity Generative Video Streaming with Vision Foundation Model",
          "year": 2026,
          "authors": [
            {
              "authorId": "2352937151",
              "name": "Tianyi Gong"
            },
            {
              "authorId": null,
              "name": "Zijian Cao"
            },
            {
              "authorId": null,
              "name": "Zixing Zhang"
            },
            {
              "authorId": null,
              "name": "Jiangkai Wu"
            },
            {
              "authorId": null,
              "name": "Xinggong Zhang"
            },
            {
              "authorId": null,
              "name": "Shuguang Cui"
            },
            {
              "authorId": null,
              "name": "Fangxin Wang"
            }
          ]
        }
      },
      {
        "contexts": [
          "For token embeddings, we use quantized latent codes from a VAE-based autoencoder [24, 27, 13, 4]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "cc10faf6501a40f6a6d80e7c70f5ac0031c96d8b",
          "title": "Progressive Checkerboards for Autoregressive Multiscale Image Generation",
          "year": 2026,
          "authors": [
            {
              "authorId": "2408533076",
              "name": "David Eigen"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "d9492f8f51aa316dce7434699700fc7a71436a17",
          "title": "PnP-U3D: Plug-and-Play 3D Framework Bridging Autoregression and Diffusion for Unified Understanding and Generation",
          "year": 2026,
          "authors": [
            {
              "authorId": null,
              "name": "Yongwei Chen"
            },
            {
              "authorId": null,
              "name": "Tianyi Wei"
            },
            {
              "authorId": null,
              "name": "Yushi Lan"
            },
            {
              "authorId": "2382765175",
              "name": "Zhaoyang Lyu"
            },
            {
              "authorId": null,
              "name": "Shangchen Zhou"
            },
            {
              "authorId": null,
              "name": "Xudong Xu"
            },
            {
              "authorId": null,
              "name": "Xingang Pan"
            }
          ]
        }
      },
      {
        "contexts": [
          "For fast convergence, in Stage 1, we encode the continuous robot actions into discrete tokens with Residual Vector Quantization (RVQ) (Van Den Oord et al., 2017; Esser et al., 2021; Lee et al., 2022) and then train the VLM by minimizing the cross-entropy loss."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "b75aec636e4ec06a05c5e43cf150c64e3ffa1639",
          "title": "RDT2: Exploring the Scaling Limit of UMI Data Towards Zero-Shot Cross-Embodiment Generalization",
          "year": 2026,
          "authors": [
            {
              "authorId": null,
              "name": "Songming Liu"
            },
            {
              "authorId": null,
              "name": "Bangguo Li"
            },
            {
              "authorId": null,
              "name": "Kai Ma"
            },
            {
              "authorId": null,
              "name": "Lingxuan Wu"
            },
            {
              "authorId": null,
              "name": "Hengkai Tan"
            },
            {
              "authorId": "2408532582",
              "name": "Ouyang Xiao"
            },
            {
              "authorId": null,
              "name": "Hang Su"
            },
            {
              "authorId": null,
              "name": "Jun Zhu"
            }
          ]
        }
      },
      {
        "contexts": [
          "A lightweight 6 -layer ViT serves as the decoder predicting either MaskGIT-VQGAN tokens (Esser et al., 2021; Chang et al., 2022)."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "adb186d899a0c8b250ff5bad5daf52a91ba3c9cd",
          "title": "Learning Sparse Visual Representations via Spatial-Semantic Factorization",
          "year": 2026,
          "authors": [
            {
              "authorId": null,
              "name": "Theodore Zhengde Zhao"
            },
            {
              "authorId": "39620434",
              "name": "Sid Kiblawi"
            },
            {
              "authorId": "2408440243",
              "name": "Jianwei Yang"
            },
            {
              "authorId": "2637252",
              "name": "N. Usuyama"
            },
            {
              "authorId": "73441526",
              "name": "Reuben Tan"
            },
            {
              "authorId": "2268318452",
              "name": "Noel C. F. Codella"
            },
            {
              "authorId": "2264107059",
              "name": "Tristan Naumann"
            },
            {
              "authorId": "2277607019",
              "name": "H. Poon"
            },
            {
              "authorId": null,
              "name": "Mu Wei"
            }
          ]
        }
      },
      {
        "contexts": [
          "L rgb is calculated as follows: where LPIPS ( · ) denotes the LPIPS (Zhang et al., 2018) function, L GAN is GAN loss as in (Esser et al., 2021; Zheng et al., 2025a). ω l and ω g are weights for LPIPS and GAN loss and are set to 1 ."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "9245088c20d303f33ba748b413a980a9a1abdd09",
          "title": "UniDWM: Towards a Unified Driving World Model via Multifaceted Representation Learning",
          "year": 2026,
          "authors": [
            {
              "authorId": "2279663389",
              "name": "Shuai Liu"
            },
            {
              "authorId": null,
              "name": "Siheng Ren"
            },
            {
              "authorId": "2408453062",
              "name": "Xiaoyao Zhu"
            },
            {
              "authorId": null,
              "name": "Quanmin Liang"
            },
            {
              "authorId": null,
              "name": "Zefeng Li"
            },
            {
              "authorId": null,
              "name": "Qiang Li"
            },
            {
              "authorId": null,
              "name": "Xin Hu"
            },
            {
              "authorId": null,
              "name": "Kai Huang"
            }
          ]
        }
      },
      {
        "contexts": [
          "VQ-VAE (van den Oord et al., 2017) learns a discrete latent codebook that supports high-fidelity reconstruction and autoregressive priors, while VQ-GAN (Esser et al., 2021) couples a learned code-book in order to synthesize high-resolution images."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "28f3d916a10062b3a7dd4628708c5adfca77c505",
          "title": "Revisiting Adaptive Rounding with Vectorized Reparameterization for LLM Quantization",
          "year": 2026,
          "authors": [
            {
              "authorId": "2110321280",
              "name": "Yuli Zhou"
            },
            {
              "authorId": null,
              "name": "Qingxuan Chen"
            },
            {
              "authorId": "2323368873",
              "name": "Luca Benini"
            },
            {
              "authorId": "2408508468",
              "name": "Guolei Sun"
            },
            {
              "authorId": "2323432053",
              "name": "Yawei Li"
            }
          ]
        }
      },
      {
        "contexts": [
          "Concretely, we follow a VQGAN-style pipeline [8] in which an encoder–decoder autoencoder maps a 256 × 256 RGB image to a low-resolution latent grid and back to pixel space [51], with a VQ layer in between that converts latents into discrete indices.",
          "At the same time, vector-quantized modules are widely used as semantic tokenizers in modern systems [5, 8], turning continuous representations of images, audio, and items into discrete indices that can be modeled efficiently by autoregressive or retrieval-style architectures [9, 36].",
          "Approaches such as VQ-VAE [1] and its residual or product-quantization variants [2, 24, 25] show that codebooks can function as effective semantic tokenizers [8, 26], but they generally treat the nearest-neighbor step as a fixed operator and rely on auxiliary losses to make training viable [4, 5].",
          "This paradigm underpins recent advances in generation, autoregressive modeling, and recommendation [7], where increasingly large codebooks act as learned vocabularies for complex data [8–10]."
        ],
        "intents": [],
        "isInfluential": true,
        "citingPaper": {
          "paperId": "bc9e947f0c1d515131c3a548629825b7aac05fcf",
          "title": "Generalized Radius and Integrated Codebook Transforms for Differentiable Vector Quantization",
          "year": 2026,
          "authors": [
            {
              "authorId": "2368634062",
              "name": "Haochen You"
            },
            {
              "authorId": "2408507567",
              "name": "Heng Zhang"
            },
            {
              "authorId": "2408478145",
              "name": "Hongyang He"
            },
            {
              "authorId": "2408473426",
              "name": "Yuqi Li"
            },
            {
              "authorId": null,
              "name": "Baojing Liu"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "4cac26f4538de3a0f41b56b77469f43bd894002b",
          "title": "SWFTI: Facial Template Inversion via StyleSwin Mapping",
          "year": 2026,
          "authors": [
            {
              "authorId": null,
              "name": "Zixuan Shen"
            },
            {
              "authorId": null,
              "name": "Zhihua Xia"
            },
            {
              "authorId": "2408411799",
              "name": "Kaikai Gan"
            },
            {
              "authorId": null,
              "name": "Peipeng Yu"
            },
            {
              "authorId": "2392883232",
              "name": "Xiaoyu Zhou"
            }
          ]
        }
      },
      {
        "contexts": [
          "VQGAN [8] is a popular image VQ-VAE (f8-8192 version) [40]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "c3925bf72abe91e7b28370279863c278266bbba6",
          "title": "MTC-VAE: Multi-Level Temporal Compression with Content Awareness",
          "year": 2026,
          "authors": [
            {
              "authorId": null,
              "name": "Yubo Dong"
            },
            {
              "authorId": null,
              "name": "Linchao Zhu"
            }
          ]
        }
      },
      {
        "contexts": [
          "The VQGAN is pretrained separately.",
          "In addition to the standard VQGAN objective (Esser et al., 2021), which jointly optimizes reconstruction fidelity, codebook commitment, and adversarial losses, we incorporate a correlation regularization term that explicitly maximizes the correlation between reconstructed spectrograms and the ground-truth targets.",
          "2, the VQGAN is composed of a encoder E , a decoder D and a learnable codebook The encoder takes a spectrogram and produces a latent feature grid ˆ z = E ( X EEG ) ∈ R h × w × d , and then each feature vector is replaced by its nearest neighbor from the codebook, z q = arg min z k ∈Z || ˆ z − z k || 2 , to obtain the quantized token sequence z q .",
          "We further compress the spectrogram into a sequence of discrete EEG tokens using a VQGAN (Esser et al., 2021).",
          "In addition to the standard VQGAN objective (Esser et al., 2021), which jointly optimizes reconstruction fidelity, codebook commitment, and adversarial losses, we incorporate a correlation regularization term that explicitly maximizes the correlation between reconstructed spectrograms and the…",
          "The predicted tokens are subsequently decoded by the frozen VQGAN to yield the final EEG spectrogram."
        ],
        "intents": [],
        "isInfluential": true,
        "citingPaper": {
          "paperId": "5cbda6dd370d86e72078b6e466b2d0c26c5d402c",
          "title": "Physiology as Language: Translating Respiration to Sleep EEG",
          "year": 2026,
          "authors": [
            {
              "authorId": "2343501471",
              "name": "Kaiwen Zha"
            },
            {
              "authorId": null,
              "name": "Chao Li"
            },
            {
              "authorId": null,
              "name": "Hao He"
            },
            {
              "authorId": "2284772666",
              "name": "Peng Cao"
            },
            {
              "authorId": "2408533597",
              "name": "Tianhong Li"
            },
            {
              "authorId": "2385460349",
              "name": "Ali Mirzazadeh"
            },
            {
              "authorId": null,
              "name": "Ellen Zhang"
            },
            {
              "authorId": null,
              "name": "Jong Woo Lee"
            },
            {
              "authorId": null,
              "name": "Yoon Kim"
            },
            {
              "authorId": "2334478248",
              "name": "Dina Katabi"
            }
          ]
        }
      },
      {
        "contexts": [
          "Pre-trained in high-quality real-world images, large-scale image generation models (Esser et al., 2021; Rombach et al., 2022; Peebles & Xie, 2023; Tian et al., 2024; Esser et al., 2024; Liu et al., 2024) are considered proficient in fitting complex image distributions."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "9f05238ef6c3119e7283dc6afac278c10e3eba7c",
          "title": "Bridging Degradation Discrimination and Generation for Universal Image Restoration",
          "year": 2026,
          "authors": [
            {
              "authorId": "2290061849",
              "name": "Jiakui Hu"
            },
            {
              "authorId": null,
              "name": "Zhengjian Yao"
            },
            {
              "authorId": "2128093991",
              "name": "Lujia Jin"
            },
            {
              "authorId": "2336275856",
              "name": "Yanye Lu"
            }
          ]
        }
      },
      {
        "contexts": [
          "Following the advent of the vector quantization (VQVAE [32], VQGAN [7]), AR methods shifts towards handling discrete latent-level to-kens (ImageGPT [6], RQ-Transformer [16], LAR-SR [9])."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "fd014928bafb98c695551d2b919e8193c71b5a56",
          "title": "HSI-VAR: Rethinking Hyperspectral Restoration through Spatial-Spectral Visual Autoregression",
          "year": 2026,
          "authors": [
            {
              "authorId": null,
              "name": "Xiangming Wang"
            },
            {
              "authorId": null,
              "name": "Benteng Sun"
            },
            {
              "authorId": null,
              "name": "Yungeng Liu"
            },
            {
              "authorId": null,
              "name": "Haijin Zeng"
            },
            {
              "authorId": "2375078715",
              "name": "Yongyong Chen"
            },
            {
              "authorId": null,
              "name": "Jingyong Su"
            },
            {
              "authorId": null,
              "name": "Jie Liu"
            }
          ]
        }
      },
      {
        "contexts": [
          "Architechture We use U-ViT (Bao et al., 2023) to model the flow matching vector field and use VQGAN (Esser et al., 2021) to encode (resp. decode) each frame of the video to (resp. from) the latent space with the following configurations Model Hyperparameters See Table 13.",
          "We utilize a pre-trained VQGAN (Esser et al., 2021) to encode (resp. decode) each frame of the video to (resp. from) the latent space."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "9efce4761fc69974f6024e126d60996baf1575d8",
          "title": "Improving Flow Matching by Aligning Flow Divergence",
          "year": 2026,
          "authors": [
            {
              "authorId": "2315268910",
              "name": "Yuhao Huang"
            },
            {
              "authorId": "2276482001",
              "name": "Taos Transue"
            },
            {
              "authorId": "2293314973",
              "name": "Shih-Hsin Wang"
            },
            {
              "authorId": "2396488872",
              "name": "William M. Feldman"
            },
            {
              "authorId": "2396556892",
              "name": "Hong Zhang"
            },
            {
              "authorId": "2315373609",
              "name": "Bao Wang"
            }
          ]
        }
      },
      {
        "contexts": [
          "Subsequent works extend these foundations along multiple dimensions, including adversarial (Esser et al., 2021; Chang et al., 2022) and perceptual (Zhang et al., 2018) training objectives for improving visual fidelity, transformer-based tok-enizers (Yu et al., 2021; Cao et al., 2023) for enhanced…"
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "6a52584328e1884be79202bab53bb493852e258d",
          "title": "ImgCoT: Compressing Long Chain of Thought into Compact Visual Tokens for Efficient Reasoning of Large Language Model",
          "year": 2026,
          "authors": [
            {
              "authorId": "2312100322",
              "name": "Xiaoshu Chen"
            },
            {
              "authorId": "2516087",
              "name": "Sihang Zhou"
            },
            {
              "authorId": "2024445866",
              "name": "K. Liang"
            },
            {
              "authorId": "2374199618",
              "name": "Taichun Zhou"
            },
            {
              "authorId": "2279159599",
              "name": "Xinwang Liu"
            }
          ]
        }
      },
      {
        "contexts": [
          "This paradigm has successfully transferred to Computer Vision, where works like ImageGPT (Chen et al., 2020), VQGAN (Esser et al., 2021), and Parti (Yu et al., 2022) demonstrate that visual representations can be learned via sequences of discrete tokens."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "509fd5d58d0b568dc80cb95c3f1edf1004935bc0",
          "title": "MoVE: Mixture of Value Embeddings -- A New Axis for Scaling Parametric Memory in Autoregressive Models",
          "year": 2026,
          "authors": [
            {
              "authorId": "2408346733",
              "name": "Yangyan Li"
            }
          ]
        }
      },
      {
        "contexts": [
          "Existing generation methods primarily include three paradigms: GANs[13, 15, 20, 21], diffusion models[14, 18, 25, 26, 28], and autoregressive models[5, 10, 12, 16, 23, 30, 32, 36].",
          "Over the past few years, the vision community has witnessed remarkable progress in deep generative models, such as diffusion models[14, 18, 25, 26, 28] and autoregressive models[5, 10, 12, 16, 23, 30, 32, 36], elevating image generation quality to unprecedented levels."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "ce8be16eced4254858820d6bb76b1daf66a678a9",
          "title": "NativeTok: Native Visual Tokenization for Improved Image Generation",
          "year": 2026,
          "authors": [
            {
              "authorId": "2408382183",
              "name": "Bin Wu"
            },
            {
              "authorId": null,
              "name": "Mengqi Huang"
            },
            {
              "authorId": null,
              "name": "Weinan Jia"
            },
            {
              "authorId": null,
              "name": "Zhendong Mao"
            }
          ]
        }
      },
      {
        "contexts": [
          "Moreover, latent flow models have benefited from adding an adversarial loss to the usual VAE losses [15]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "918a90d6768ebd49dbbbd20a3d109c61a060eed1",
          "title": "Unconditional flow-based time series generation with equivariance-regularised latent spaces",
          "year": 2026,
          "authors": [
            {
              "authorId": "2361260781",
              "name": "Camilo Carvajal Reyes"
            },
            {
              "authorId": "2407244620",
              "name": "Felipe Tobar"
            }
          ]
        }
      },
      {
        "contexts": [
          "…with disturbances in synthetic samples [6, 11], study [18] observed that the FID improved with initial levels of distortion under noise in the VQGAN [19] For the TTS, Bi´nkowski et al. [10] computed Fr´echet Deep-Speech Distance and Kernel DeepSpeech Distance based on Deep-Speech2 [20] embeddings."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "bd6d4e8e756650f5ef36e5b73e91ca1c5ca1ceae",
          "title": "Understanding Frechet Speech Distance for Synthetic Speech Quality Evaluation",
          "year": 2026,
          "authors": [
            {
              "authorId": "2407496548",
              "name": "June-Woo Kim"
            },
            {
              "authorId": "2407454434",
              "name": "Dhruv Agarwal"
            },
            {
              "authorId": "2347672163",
              "name": "Federica Cerina"
            }
          ]
        }
      },
      {
        "contexts": [
          "…diffusion has progressed from vector-quantized tokenizers such as VQGAN to higher-capacity hierarchical VAEs like NVAE, with the shared goal of improving reconstruction fidelity and perceptual quality in latent diffusion pipelines (Esser et al., 2021; Vahdat & Kautz, 2020; Shi et al., 2025a)."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "86af85f25d1b3d320976d12cce43489c3b9510f4",
          "title": "A Tilted Seesaw: Revisiting Autoencoder Trade-off for Controllable Diffusion",
          "year": 2026,
          "authors": [
            {
              "authorId": "2203366502",
              "name": "Pu Cao"
            },
            {
              "authorId": null,
              "name": "Yiyang Ma"
            },
            {
              "authorId": "2273950538",
              "name": "Feng Zhou"
            },
            {
              "authorId": null,
              "name": "Xuedan Yin"
            },
            {
              "authorId": null,
              "name": "Qing Song"
            },
            {
              "authorId": null,
              "name": "Lu Yang"
            }
          ]
        }
      },
      {
        "contexts": [
          "This reduces the effective discrete capacity and can limit reconstruction fidelity even when the nominal codebook capacity is large (Yu et al., 2021; Zheng et al., 2022; Esser et al., 2021b; Esser et al., 2021a; Chang et al., 2022; Rombach et al., 2022).",
          "…a tokenizer that maps images into a lower-dimensional (often discrete) representation on which a separate probabilistic model is trained, effectively decoupling representation learning from distribution modeling (Ramesh et al., 2021; Esser et al., 2021b; Rombach et al., 2022; Fayyaz et al., 2022)."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "7e3c062d93f18394ef5dc8ca4fdbecde6d01c949",
          "title": "Is Hierarchical Quantization Essential for Optimal Reconstruction?",
          "year": 2026,
          "authors": [
            {
              "authorId": "2323367920",
              "name": "Shirin Reyhanian"
            },
            {
              "authorId": "2252493141",
              "name": "Laurenz Wiskott"
            }
          ]
        }
      },
      {
        "contexts": [
          "VQ-VAE enabled visual sequence modeling [65], while VQGAN enhanced fidelity via adversarial objectives [66].",
          "Discrete (codebook-based) tokenizers encode inputs into latents and quantize them into codebook indices, as in VQ-VAE [65] and VQ-GAN [66]; the resulting sequences are typically modeled with autoregressive or diffusion priors.",
          "…VideoLLaMA[143] Continuous MLLM Understanding InternVideo[144] Continuous MLLM Understanding VQ-VAE[65] Discrete Diffusion / MLLM Generation VQ-GAN[66] Discrete Diffusion / MLLM Generation DALL · E[146] Discrete MLLM Generation MaskGIT[147] Discrete MLLM Generation MAGVIT2[145] Discrete MLLM Both…",
          "Discrete visual tokenizers initially designed for generative models, have recently gained attention in multimodal understanding [63] Continuous MLLM Understanding BLIP-2[94] Continuous MLLM Understanding LLaVA[14] Continuous MLLM Understanding Qwen2-VL[2] Continuous MLLM Understanding VideoLLaMA[143] Continuous MLLM Understanding InternVideo[144] Continuous MLLM Understanding VQ-VAE[65] Discrete Diffusion / MLLM Generation VQ-GAN[66] Discrete Diffusion / MLLM Generation DALL · E[146] Discrete MLLM Generation MaskGIT[147] Discrete MLLM Generation MAGVIT2[145] Discrete MLLM Both Gen & Understanding CM3[135] Discrete MLLM Both Gen & Understanding LDM[148] Continuous Diffusion Generation REPA[149] Continuous Diffusion Generation RAE[150] Continuous Diffusion Generation and unified modeling.",
          "Text tokenization provides the discrete modeling blueprint[213], image tokenization extends it to perceptual semantics[66], and video tokenization largely inherits and adapts image techniques[156].",
          "However, [10, 11, 172] and generation-oriented [65, 66]), dual-branch cooperative frameworks [139, 136], and single unified tokenizers [138, 173, 174].",
          "Another family, represented by generation-oriented tokenizers (usually in discrete form) such as VQ-VAE[65] and VQ-GAN[66], enables high-fidelity image synthesis, generation, and editing.",
          "Generation-oriented tokenizers (VQ-VAE[65], VQGAN[66]) prioritize high-fidelity reconstruction, but heterogeneous codebooks and objectives can hinder portability.",
          "Subsequent research introduced learned tokenizers—such as VQ-VAE [65], VQGAN [66], and MAGVIT [156]—that replaced raw embeddings with discrete codebook indices, offering compact symbolic representations compatible with transformer and language-model-based architectures.",
          "This quantization process such as in VQ-VAE[65], VQ-GAN[66], bridges perception and generation: each index corresponds to a visual word learned from the data."
        ],
        "intents": [],
        "isInfluential": true,
        "citingPaper": {
          "paperId": "017b21dc3d5f3562729a5a6842d535b6a33bd89d",
          "title": "Compression Tells Intelligence: Visual Coding, Visual Token Technology, and the Unification",
          "year": 2026,
          "authors": [
            {
              "authorId": "2255933158",
              "name": "Xin Jin"
            },
            {
              "authorId": "2275541226",
              "name": "Jinming Liu"
            },
            {
              "authorId": null,
              "name": "Yuntao Wei"
            },
            {
              "authorId": "2316562001",
              "name": "Junyan Lin"
            },
            {
              "authorId": "2407312569",
              "name": "Zhicheng Wang"
            },
            {
              "authorId": "2376399075",
              "name": "Jianguo Huang"
            },
            {
              "authorId": "2376397061",
              "name": "Xudong Yang"
            },
            {
              "authorId": "2407424403",
              "name": "Yanxiao Liu"
            },
            {
              "authorId": "2247938835",
              "name": "Wenjun Zeng"
            }
          ]
        }
      },
      {
        "contexts": [
          "Additionally, VQ-GAN (Esser et al., 2021) integrates adversarial feedback to significantly improve the expressivity and perceptual quality of the codebook."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "d79a3bb402abd6a7b7f1ec1e1a40d86bb88077fc",
          "title": "SIGMA-PPG: Statistical-prior Informed Generative Masking Architecture for PPG Foundation Model",
          "year": 2026,
          "authors": [
            {
              "authorId": "2379724700",
              "name": "Zongheng Guo"
            },
            {
              "authorId": "2379721333",
              "name": "Tao Chen"
            },
            {
              "authorId": "2291001884",
              "name": "Yang Jiao"
            },
            {
              "authorId": "2376527092",
              "name": "Yi Pan"
            },
            {
              "authorId": "2407519284",
              "name": "Xiao Hu"
            },
            {
              "authorId": "2407456984",
              "name": "Manuela Ferrario"
            }
          ]
        }
      },
      {
        "contexts": [
          "Representative designs include VQ-VAE-2 [16], VQGAN [17], and the dVAE used in DALL · E [18]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "d2f10add43b3e8c69ee0879eb74b712d9a8fff93",
          "title": "AR-Omni: A Unified Autoregressive Model for Any-to-Any Generation",
          "year": 2026,
          "authors": [
            {
              "authorId": "2404607994",
              "name": "Dongjie Cheng"
            },
            {
              "authorId": "2273661632",
              "name": "Ruifeng Yuan"
            },
            {
              "authorId": "2257099189",
              "name": "Yongqi Li"
            },
            {
              "authorId": "2350737926",
              "name": "Runyang You"
            },
            {
              "authorId": null,
              "name": "Wenjie Wang"
            },
            {
              "authorId": "2270021997",
              "name": "Liqiang Nie"
            },
            {
              "authorId": "2406855100",
              "name": "Lei Zhang"
            },
            {
              "authorId": "2284641622",
              "name": "Wenjie Li"
            }
          ]
        }
      },
      {
        "contexts": [
          "To optimize VQGAN tokenizer training for IF-to-H&E image synthesis, we systematically evaluated codebook size and architectural configurations (Supp.",
          "H&E and IF images were tokenized using their respective pretrained VQGAN by flattening the 4x4-dimensional latent representations for each IF channel and H&E image and concatenating each 16-token sequence together.",
          "To tokenize paired H&E and multiplex IF images, we considered two approaches: one single VQGAN for both modalities, and separate VQGANs trained for each modality.",
          "Following prior work 29,30 , we employed vector-quantized generative adversarial networks (VQGAN) 31 to learn compact, discrete codebooks for each modality, enabling high-fidelity compression of image patches into low-dimensional latent representations (Fig."
        ],
        "intents": [],
        "isInfluential": true,
        "citingPaper": {
          "paperId": "6b5cdea0e446c4569bc246289883bf8c4b45dc65",
          "title": "miniMTI: minimal multiplex tissue imaging enhances biomarker expression prediction from histology",
          "year": 2026,
          "authors": [
            {
              "authorId": "2300473864",
              "name": "Zachary Sims"
            },
            {
              "authorId": "2349609177",
              "name": "Sandhya Govindarajan"
            },
            {
              "authorId": "2320224816",
              "name": "Kaoutar Ait-Ahmad"
            },
            {
              "authorId": "51188145",
              "name": "Ç. Ak"
            },
            {
              "authorId": "2406999883",
              "name": "Marigold Kuykendall"
            },
            {
              "authorId": "2349609635",
              "name": "Gordon B. Mills"
            },
            {
              "authorId": "2213892614",
              "name": "Ece Eksi"
            },
            {
              "authorId": "2258802873",
              "name": "Young Hwan Chang"
            }
          ]
        }
      },
      {
        "contexts": [
          "The local DNN transforms CSI data into a “compressed” latent representation based on a vector quantization-based generative adversarial network (VQGAN) [17], thus decreasing networking overhead while retaining high-quality CSI reconstruction and accurate HPE.",
          "This integration is crucial for maintaining high perceptual quality, even at higher compression rates [17].",
          "Unlike existing compression methods [28]–[32] that adopt scalar quantization, we integrate CNN inductive biases and principles from neural discrete representation learning [17]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "aa64d2180af42d02e93deb32cafd9ee9f86f4e75",
          "title": "TinySense: Effective CSI Compression for Scalable and Accurate Wi-Fi Sensing",
          "year": 2026,
          "authors": [
            {
              "authorId": "2185510556",
              "name": "Toan D. Gian"
            },
            {
              "authorId": "2375734240",
              "name": "Dung T. Tran"
            },
            {
              "authorId": "2406048064",
              "name": "Viet Quoc Pham"
            },
            {
              "authorId": "2406047653",
              "name": "Francesco Restuccia"
            },
            {
              "authorId": "2331156605",
              "name": "Van-Dinh Nguyen"
            }
          ]
        }
      },
      {
        "contexts": [
          "Autoregressive generative models, such as VQ-VAEs, model images as sequences of discrete latent tokens, learning compact and semantically meaningful representations that separate high-level structure from low-level appearance [24]–[26]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "fce4774fae0d98cec5cab9ceb2da8c1aa9de065a",
          "title": "PAINT: Pathology-Aware Integrated Next-Scale Transformation for Virtual Immunohistochemistry",
          "year": 2026,
          "authors": [
            {
              "authorId": "2406216131",
              "name": "Rongze Ma"
            },
            {
              "authorId": "143746093",
              "name": "Mengkang Lu"
            },
            {
              "authorId": "2406048761",
              "name": "Zhenyu Xiang"
            },
            {
              "authorId": "2268798668",
              "name": "Yongsheng Pan"
            },
            {
              "authorId": "2331869940",
              "name": "Yicheng Wu"
            },
            {
              "authorId": null,
              "name": "Qingjie Zeng"
            },
            {
              "authorId": "2256029665",
              "name": "Yong Xia"
            }
          ]
        }
      },
      {
        "contexts": [
          "Autoregressive models (Esser et al., 2021; Ramesh et al., 2021; Yu et al., 2022a) predict image tokens sequentially, with recent work including RAR-XL (Yu et al., 2024) and MaskBit (Weber et al., 2024).",
          "Autoregressive models (Esser et al., 2021) quantize continuous latents into discrete tokens via Q : R h × w × c → { 1 , . . . , K } h × w ."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "57f661169766cabf0a825b12e11c5ef285d9c2dc",
          "title": "Learning to Watermark in the Latent Space of Generative Models",
          "year": 2026,
          "authors": [
            {
              "authorId": "8478422",
              "name": "Sylvestre-Alvise Rebuffi"
            },
            {
              "authorId": "2281833102",
              "name": "Tuan Tran"
            },
            {
              "authorId": "2364013852",
              "name": "Valeriu Lacatusu"
            },
            {
              "authorId": "2363989512",
              "name": "Pierre Fernandez"
            },
            {
              "authorId": "2399158580",
              "name": "Tom'avs Souvcek"
            },
            {
              "authorId": "2406042920",
              "name": "Nikola Jovanovi'c"
            },
            {
              "authorId": "2283934407",
              "name": "Tom Sander"
            },
            {
              "authorId": "2381059850",
              "name": "Hady Elsahar"
            },
            {
              "authorId": "2278834438",
              "name": "Alex Mourachko"
            }
          ]
        }
      },
      {
        "contexts": [
          "Early approaches used GANs [10,17,19,45,48, 49,69] or autoregressive generators [1,8,12,104]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "72995996a681a04c1f593f50020047d0dee9d6c7",
          "title": "360Anything: Geometry-Free Lifting of Images and Videos to 360{\\deg}",
          "year": 2026,
          "authors": [
            {
              "authorId": "2406310091",
              "name": "Ziyi Wu"
            },
            {
              "authorId": "2310610325",
              "name": "Daniel Watson"
            },
            {
              "authorId": "2237987366",
              "name": "Andrea Tagliasacchi"
            },
            {
              "authorId": "2273160782",
              "name": "David J. Fleet"
            },
            {
              "authorId": "2333243278",
              "name": "Marcus A. Brubaker"
            },
            {
              "authorId": "2054003577",
              "name": "Saurabh Saxena"
            }
          ]
        }
      },
      {
        "contexts": [
          "VQ-GAN [10] adds adversarial training to reduce blur, while ViT-VQGAN [68] replaces CNNs with Vision Transformers [9] for long-range modeling.",
          "Videos exhibit rich structure across multiple spatial and temporal scales, but single-scale quantization methods [10, 52] tend to overfit global patterns or miss fine-grained details.",
          "In particular, discrete VAEs [10, 52] have proven especially effective, as their learned code-books quantize the latent space into discrete tokens, enabling scalable and high-quality video synthesis."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "d12ac91ca16780d5644a169f9e654fad5d1ca075",
          "title": "PyraTok: Language-Aligned Pyramidal Tokenizer for Video Understanding and Generation",
          "year": 2026,
          "authors": [
            {
              "authorId": "2149412487",
              "name": "Onkar Susladkar"
            },
            {
              "authorId": "2406042608",
              "name": "Tushar Prakash"
            },
            {
              "authorId": "51935721",
              "name": "Adheesh Juvekar"
            },
            {
              "authorId": "2299298657",
              "name": "Kiet A. Nguyen"
            },
            {
              "authorId": "2406044238",
              "name": "Dong-Hwan Jang"
            },
            {
              "authorId": "1783667",
              "name": "I. Dhillon"
            },
            {
              "authorId": "2099420",
              "name": "Ismini Lourentzou"
            }
          ]
        }
      },
      {
        "contexts": [
          "• VQGAN [11] An AR generator that operates over discrete codes obtained from a VQGAN tokenizer and reconstructs images through its decoder.",
          "In each epoch, we randomly select one view and extract its discrete codes using a pretrained VQ-GAN [11].",
          "Subsequent works like VQVAE [42], VQGAN [11], and DALL-E [31] tokenize images into discrete codes.",
          "Let an image X ∈ R H × W × 3 be represented by a sequence of discrete tokens x = [ x 1 , x 2 , . . . , x N ] , where each x n ∈ { 1 , . . . , V } indexes a code from a learned visual vocabulary of size V , typically obtained from a pretrained tokenizer ( e.g ., VQVAE [42] or VQGAN [11])."
        ],
        "intents": [],
        "isInfluential": true,
        "citingPaper": {
          "paperId": "6f948c48c180dfb59a4cc0bf6e38c8017074d9fb",
          "title": "Mirai: Autoregressive Visual Generation Needs Foresight",
          "year": 2026,
          "authors": [
            {
              "authorId": "2407142047",
              "name": "Yonghao Yu"
            },
            {
              "authorId": "2323000521",
              "name": "Lang Huang"
            },
            {
              "authorId": "2108726323",
              "name": "Zerun Wang"
            },
            {
              "authorId": "2405978345",
              "name": "Runyi Li"
            },
            {
              "authorId": "2347539446",
              "name": "Toshihiko Yamasaki"
            }
          ]
        }
      },
      {
        "contexts": [
          "This principle has been widely applied across various domains, from text models [3, 36] to high-dimensional data such as images [6, 12, 16, 47] and videos [32, 53], as well as large-scale AR modeling across various data modalities [7, 10, 26, 27, 35, 49]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "2a53532ed6687389b2022511409bd62ccb1322d8",
          "title": "Towards Holistic Modeling for Video Frame Interpolation with Auto-regressive Diffusion Transformers",
          "year": 2026,
          "authors": [
            {
              "authorId": "2283631687",
              "name": "Xinyu Peng"
            },
            {
              "authorId": "2196588523",
              "name": "Han Li"
            },
            {
              "authorId": "2271750932",
              "name": "Yuyang Huang"
            },
            {
              "authorId": "66252246",
              "name": "Ziyang Zheng"
            },
            {
              "authorId": "2119050516",
              "name": "Yaoming Wang"
            },
            {
              "authorId": "2406120432",
              "name": "Xin Chen"
            },
            {
              "authorId": "3207464",
              "name": "Wenrui Dai"
            },
            {
              "authorId": "144535686",
              "name": "Chenglin Li"
            },
            {
              "authorId": "38871632",
              "name": "Junni Zou"
            },
            {
              "authorId": "2258553760",
              "name": "Hongkai Xiong"
            }
          ]
        }
      },
      {
        "contexts": [
          "Inspired by the success of VQ-GAN (Esser et al., 2021), early unified tokenizers predominantly adapt a discrete token design."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "5a00e08337c50d25b824abc6e4ed1cec9a6b95a4",
          "title": "OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation",
          "year": 2026,
          "authors": [
            {
              "authorId": "2378722806",
              "name": "Letian Zhang"
            },
            {
              "authorId": "2269463974",
              "name": "Sucheng Ren"
            },
            {
              "authorId": "2332449023",
              "name": "Yanqing Liu"
            },
            {
              "authorId": "2108261491",
              "name": "Xianhang Li"
            },
            {
              "authorId": "2256986120",
              "name": "Zeyu Wang"
            },
            {
              "authorId": "2253873141",
              "name": "Yuyin Zhou"
            },
            {
              "authorId": null,
              "name": "Huaxiu Yao"
            },
            {
              "authorId": "2378598930",
              "name": "Zeyu Zheng"
            },
            {
              "authorId": "2406044137",
              "name": "Weili Nie"
            },
            {
              "authorId": "2269510103",
              "name": "Guilin Liu"
            },
            {
              "authorId": "2406811051",
              "name": "Zhiding Yu"
            },
            {
              "authorId": "2239227247",
              "name": "Cihang Xie"
            }
          ]
        }
      },
      {
        "contexts": [
          "Models based on Generative Adversarial Networks (GANs) can handle complex mask shapes and have achieved impressive inpainting re-sults [7]–[15]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "bf1da5e10f1c6369dd31950e359637d7ddb2f0d4",
          "title": "Aligned Stable Inpainting: Mitigating Unwanted Object Insertion and Preserving Color Consistency",
          "year": 2026,
          "authors": [
            {
              "authorId": "2108853258",
              "name": "Yikai Wang"
            },
            {
              "authorId": "2316259986",
              "name": "Junqiu Yu"
            },
            {
              "authorId": "143858726",
              "name": "Chenjie Cao"
            },
            {
              "authorId": "2266267479",
              "name": "Xiangyang Xue"
            },
            {
              "authorId": "2267228201",
              "name": "Yanwei Fu"
            }
          ]
        }
      },
      {
        "contexts": [
          "Improvements to VQ-VAEs have been widely explored, including architecture [7, 31, 46, 65], training objectives [16, 45], and quantization techniques [15, 39, 66, 71].",
          "…and SiT [37]; autoregressive models with bi-directional attention including MaskGIT [5], MAGVIT-v2 [66], MAR [33], and TiTok [68]; autoregressive models with causal attention including VQGAN [16], RQTransformer [31], Open-MAGVIT-v2 [36], VAR [57], SAR [35], RAR [67], RandAR [41], and LlamaGen [56].",
          "Another category, GPT-style autoregressive generative models [16, 31, 56, 65] are based on decoder-only architectures with casual attention.",
          "We compare with 2D image tokenizers including Taming VQ-GAN [16], MaskGIT VQ-GAN [5], Open MAGVIT-v2 [36], and the tokenizer in LlamaGen [56]; 1D image tokenizers including TiTok [68], FlexTok [2], and One-D-Piece [40]."
        ],
        "intents": [],
        "isInfluential": true,
        "citingPaper": {
          "paperId": "6a8d709f6165d1b415a180a004185ca19f32c5be",
          "title": "Soft Tail-dropping for Adaptive Visual Tokenization",
          "year": 2026,
          "authors": [
            {
              "authorId": "2249707751",
              "name": "Zeyuan Chen"
            },
            {
              "authorId": "2405734770",
              "name": "Kai Zhang"
            },
            {
              "authorId": "2249555604",
              "name": "Zhuowen Tu"
            },
            {
              "authorId": "2304381708",
              "name": "Yuanjun Xiong"
            }
          ]
        }
      },
      {
        "contexts": [
          "Models like Stable Diffusion [9] generate visual data that is substantially more realistic and semantically coherent than outputs from earlier generative frameworks such as GANs [10], VAEs [11], or transformer-based synthesizers [12]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "1611bea70af34a06967c77b98605cfec71649fe3",
          "title": "Federated Balanced Learning",
          "year": 2026,
          "authors": [
            {
              "authorId": "2323070226",
              "name": "Jiaze Li"
            },
            {
              "authorId": "2388005784",
              "name": "Haoran Xu"
            },
            {
              "authorId": "2341460466",
              "name": "Wanyi Wu"
            },
            {
              "authorId": "2373692192",
              "name": "Changwei Wang"
            },
            {
              "authorId": "2405866343",
              "name": "Shuaiguang Li"
            },
            {
              "authorId": "2317982861",
              "name": "Jianzhong Ju"
            },
            {
              "authorId": "2363405807",
              "name": "Zhenbo Luo"
            },
            {
              "authorId": "2317980688",
              "name": "Jian Luan"
            },
            {
              "authorId": "2363690953",
              "name": "Y. Qu"
            },
            {
              "authorId": "2267870488",
              "name": "Longxiang Gao"
            },
            {
              "authorId": "2405824957",
              "name": "Xudong Yang"
            },
            {
              "authorId": "2405810718",
              "name": "Lumin Xing"
            }
          ]
        }
      },
      {
        "contexts": [
          "During the evaluation process, the facial scans were first converted into SGR and then reconstructed using the VQVAE model.",
          "We refer to the VQVAE with mesh quality regularization as VQVAE-mesh and the one without it as VQVAE-base .",
          "Following the approach in Taming Transformers (Esser et al. 2021), we set the weight λ adv = 0 .",
          "To demonstrate the effect of VQVAE, we conducted a qualitative evaluation on the test set.",
          "In this section, we describe all the loss functions used in the VQVAE training process.",
          "Although a standard VQVAE (Esser et al. 2021) architecture can be applied, we significantly enhance its performance by tailoring the model to the unique properties of our SGR.",
          "We then describe our Vector Quantized Variational Autoencoder (VQVAE) tailored for geometry SGR. Finally, we present a conditional diffusion strategy for generating high-quality 3D faces.",
          "The training of VQVAE involves two main components: Reconstruction Loss and Geometric Regularization , which will be discussed separately below.",
          "For a fair comparison, we tested different weight configurations and retrained the VQVAE model for 100 epochs for each setup.",
          "However, traditional VQVAE suffers from high training costs and slow convergence.",
          "To quantitatively compare the impact of Geometric Regularization , we conducted a preliminary experiment using VQVAE without Geometric Regularization as the baseline.",
          "VQVAE-mesh (with regularization) and VQVAE-base (without regularization) are equally trained for 30,000 steps.",
          "As indicated by the results, Geometric Regularization penalizes deformed mesh outputs, enabling VQVAE to converge more quickly during the early stages of learning.",
          "The total loss of our geometric VQVAE is given by where α nor , α lap , and α edg are the respective weights for each loss term, allowing controlled emphasis on different aspects of Geometric Regularization ."
        ],
        "intents": [],
        "isInfluential": true,
        "citingPaper": {
          "paperId": "39f85051e0e4ac3c48bf8a22f9a3d9e92e1e69e1",
          "title": "Spherical Geometry Diffusion: Generating High-quality 3D Face Geometry via Sphere-anchored Representations",
          "year": 2026,
          "authors": [
            {
              "authorId": "2405823230",
              "name": "Junyi Zhang"
            },
            {
              "authorId": "2324521138",
              "name": "Yiming Wang"
            },
            {
              "authorId": "2352048257",
              "name": "Yunhong Lu"
            },
            {
              "authorId": "2351823899",
              "name": "Qichao Wang"
            },
            {
              "authorId": "2405754041",
              "name": "Wenzhe Qian"
            },
            {
              "authorId": "2144003360",
              "name": "Xiaoyin Xu"
            },
            {
              "authorId": "2405771392",
              "name": "David Gu"
            },
            {
              "authorId": "2405826794",
              "name": "Min Zhang"
            }
          ]
        }
      },
      {
        "contexts": [
          "* Corresponding author: lujing@nju.edu.cn neural codecs are based on the VQ-GAN architecture [4] and employ diverse discriminators [5, 6, 7] to improve the perceptual quality of the reconstructed speech."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "91299ef1ffee830ed63f6cecbcde9b12e5a80d08",
          "title": "VoCodec: An Efficient Lightweight Low-Bitrate Speech Codec",
          "year": 2026,
          "authors": [
            {
              "authorId": "2406381336",
              "name": "Leyan Yang"
            },
            {
              "authorId": "2387881273",
              "name": "Ronghui Hu"
            },
            {
              "authorId": "2406458171",
              "name": "Yang Xu"
            },
            {
              "authorId": "2405820617",
              "name": "Jing Lu"
            }
          ]
        }
      },
      {
        "contexts": [
          "Among Transformer-based generative frameworks, the Vector Quantized Generative Adversarial Network (VQGAN) 26 is particularly notable for integrating the strengths of VAEs and GANs while leveraging self-attention to synthesize realistic images with rich textures and fine details."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "8728bdcb4f6492c5cd9541df88d1b79ad351ded8",
          "title": "Audio-driven single image talking face animation with transformers",
          "year": 2026,
          "authors": [
            {
              "authorId": "2405833466",
              "name": "Yixin Li"
            },
            {
              "authorId": "2329871762",
              "name": "Xizhong Shen"
            }
          ]
        }
      },
      {
        "contexts": [
          "Building on VQVAE[51], VQGAN[15] quantizes image patches into discrete tokens and employs a decoder-only Transformer to predict these tokens, enabling high-fidelity, high-resolution synthesis."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "a161794e888d26fa84a77c380a4ebc5becf6da84",
          "title": "TimeMar: Multi-Scale Autoregressive Modeling for Unconditional Time Series Generation",
          "year": 2026,
          "authors": [
            {
              "authorId": "2405621349",
              "name": "Xiangyu Xu"
            },
            {
              "authorId": "2355876544",
              "name": "Qingsong Zhong"
            },
            {
              "authorId": "2405508663",
              "name": "Jilin Hu"
            }
          ]
        }
      },
      {
        "contexts": [
          "In addition, most unified medical foundation models rely on discrete generation methods based on visual bag-of-words [10, 31]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "0f0a48aec3bf0d797bfa8df7679f6d87f2e9b1aa",
          "title": "UniX: Unifying Autoregression and Diffusion for Chest X-Ray Understanding and Generation",
          "year": 2026,
          "authors": [
            {
              "authorId": "2400406940",
              "name": "Ruiheng Zhang"
            },
            {
              "authorId": "2405512705",
              "name": "Jingfeng Yao"
            },
            {
              "authorId": "2363990986",
              "name": "Huangxuan Zhao"
            },
            {
              "authorId": "2405954782",
              "name": "Hao Yan"
            },
            {
              "authorId": "2385839186",
              "name": "Xiao He"
            },
            {
              "authorId": "2339340382",
              "name": "Lei Chen"
            },
            {
              "authorId": "2405511034",
              "name": "Zhou Wei"
            },
            {
              "authorId": "2363678782",
              "name": "Yong Luo"
            },
            {
              "authorId": "2145656",
              "name": "Zengmao Wang"
            },
            {
              "authorId": "2282189838",
              "name": "Lefei Zhang"
            },
            {
              "authorId": "2397764730",
              "name": "Dacheng Tao"
            },
            {
              "authorId": "2395666580",
              "name": "Bo Du"
            }
          ]
        }
      },
      {
        "contexts": [
          "…the potential of collaborative approaches over traditional prompt selection. on the Vision Transformer (ViT) [10, 48], which is trained using a Masked Autoencoder (MAE) [17] objective (denoted as ViT-MAE), and a decoder taken from a Vector Quantised Generative Adversarial Network (VQGAN) [11]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "997aec55a175f67fcceef568d5a640819d4c7f1f",
          "title": "Enhancing Visual In-Context Learning by Multi-Faceted Fusion",
          "year": 2026,
          "authors": [
            {
              "authorId": null,
              "name": "Wenwen Liao"
            },
            {
              "authorId": "2269420886",
              "name": "Jianbo Yu"
            },
            {
              "authorId": "2406300558",
              "name": "Yuansong Wang"
            },
            {
              "authorId": "2408220458",
              "name": "Qingchao Jiang"
            },
            {
              "authorId": "2174522637",
              "name": "Xiaofeng Yang"
            }
          ]
        }
      },
      {
        "contexts": [
          "MAE-VQGAN combines a Vision Transformer (ViT) [9, 21] encoder, trained with a Masked Autoencoder (MAE) objective [13], and a Vector Quantised Generative Adversarial Network (VQGAN) [10] decoder."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "ea409c2672136d2fbfd1892e9c8c132a7dc8708e",
          "title": "Beyond Single Prompts: Synergistic Fusion and Arrangement for VICL",
          "year": 2026,
          "authors": [
            {
              "authorId": null,
              "name": "Wenwen Liao"
            },
            {
              "authorId": "2269420886",
              "name": "Jianbo Yu"
            },
            {
              "authorId": "2406300558",
              "name": "Yuansong Wang"
            },
            {
              "authorId": "2321038873",
              "name": "Shifu Yan"
            },
            {
              "authorId": "2174522637",
              "name": "Xiaofeng Yang"
            }
          ]
        }
      },
      {
        "contexts": [
          "The process of quantization, while enabling discrete representations, can lead to a loss of fine-grained details and potentially reduce the representational capacity for modeling intricate visual structures [36, 37]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "98ed06913f33765bce74bfa15bf70af386f0836a",
          "title": "VQ-Seg: Vector-Quantized Token Perturbation for Semi-Supervised Medical Image Segmentation",
          "year": 2026,
          "authors": [
            {
              "authorId": "2321323897",
              "name": "Sicheng Yang"
            },
            {
              "authorId": "153107262",
              "name": "Zhaohu Xing"
            },
            {
              "authorId": "2367751280",
              "name": "Lei Zhu"
            }
          ]
        }
      },
      {
        "contexts": [
          "LlamaGen (Sun et al. 2024a) is the first to adopt and fintune the LLaMA base model for image generation by training a VQ-VAE image tokenizer (Van Den Oord, Vinyals et al. 2017; Esser, Rombach, and Ommer 2021)."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "fd57e7791f33f3725d28133edb6a96f8fa114aba",
          "title": "Annealed Relaxation of Speculative Decoding for Faster Autoregressive Image Generation",
          "year": 2026,
          "authors": [
            {
              "authorId": "2336920667",
              "name": "Xingyao Li"
            },
            {
              "authorId": "2326162492",
              "name": "Fengzhuo Zhang"
            },
            {
              "authorId": "2325980280",
              "name": "Cunxiao Du"
            },
            {
              "authorId": "2405067724",
              "name": "Hui Ji"
            }
          ]
        }
      },
      {
        "contexts": [
          "A range of powerful deep generative architectures has been developed to model complex data distributions, including diffusion models [20, 21], autoregressive transformers [22, 23], gener-4 ative adversarial networks (GANs) [24] and variational autoencoders (VAEs) [25]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "e693abd5d8f3771ecd1edaadecb7c7f4fe7ce78d",
          "title": "Interpretability and Individuality in Knee MRI: Patient-Specific Radiomic Fingerprint with Reconstructed Healthy Personas",
          "year": 2026,
          "authors": [
            {
              "authorId": "2313040008",
              "name": "Yaxi Chen"
            },
            {
              "authorId": "2350728107",
              "name": "Simin Ni"
            },
            {
              "authorId": "2404565051",
              "name": "Shuai Li"
            },
            {
              "authorId": "1802257196",
              "name": "Shaheer U. Saeed"
            },
            {
              "authorId": "2312918607",
              "name": "Aleksandra Ivanova"
            },
            {
              "authorId": "6986497",
              "name": "R. Hargunani"
            },
            {
              "authorId": "2333525464",
              "name": "Jie Huang"
            },
            {
              "authorId": "2313187387",
              "name": "Chaozong Liu"
            },
            {
              "authorId": "2262923478",
              "name": "Yipeng Hu"
            }
          ]
        }
      },
      {
        "contexts": [
          "The ﬁrst stage of Prior Learning aims to reconstruct HQ images by learning an encoder-decoder backbone and an HQ codebook [21] with a discrete VQ process.",
          "This idea is ﬁrst introduced in VQVAE [20] by learning codebook priors and further in VQGAN [21] with better perceptual quality induced by autoregressive transformer [52].",
          "Beneﬁted by the power of vector-quantization (VQ) [20], [21] in image generation, VQ-based BIR methods [2], [10], [22], [23], [47] have been developed to utilize HQ prior for robust restoration."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "cbfbb3375537101d87680a19e381d93ad25ed15a",
          "title": "Diagnosing and Improving Vector-Quantization-Based Blind Image Restoration",
          "year": 2026,
          "authors": [
            {
              "authorId": "2405661438",
              "name": "Hongyu Li"
            },
            {
              "authorId": "2266807629",
              "name": "Tianyi Xu"
            },
            {
              "authorId": "2404506595",
              "name": "Zengyou Wang"
            },
            {
              "authorId": "2354885905",
              "name": "Xiantong Zhen"
            },
            {
              "authorId": "2314969872",
              "name": "Ran Gu"
            },
            {
              "authorId": "2140414603",
              "name": "David Zhang"
            },
            {
              "authorId": "2312032204",
              "name": "Jun Xu"
            }
          ]
        }
      },
      {
        "contexts": [
          "Since the introduction of the Transformer architecture [49], self-attention has become the dominant mechanism across a wide range of domains, including natural language processing [4, 17], computer vision [18, 28, 33, 58], and generative modeling [19, 42]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "e88a2fbbcdd3fb02515b5e9b2d696e7aba8d64d3",
          "title": "MHLA: Restoring Expressivity of Linear Attention via Token-Level Multi-Head",
          "year": 2026,
          "authors": [
            {
              "authorId": "2404382962",
              "name": "Kewei Zhang"
            },
            {
              "authorId": "2404009951",
              "name": "Ye Huang"
            },
            {
              "authorId": "2405685072",
              "name": "Yufan Deng"
            },
            {
              "authorId": "2193887687",
              "name": "Jincheng Yu"
            },
            {
              "authorId": "2212250873",
              "name": "Junsong Chen"
            },
            {
              "authorId": "2403982889",
              "name": "Huan Ling"
            },
            {
              "authorId": "2320149516",
              "name": "Enze Xie"
            },
            {
              "authorId": "2404260527",
              "name": "Daquan Zhou"
            }
          ]
        }
      }
    ]
  },
  {
    "paperId": "5f404dbba07619cc7f28d75d03f124a52290046e",
    "title": "Are Transformers Effective for Time Series Forecasting?",
    "year": 2022,
    "abstract": "Recently, there has been a surge of Transformer-based solutions for the long-term time series forecasting (LTSF) task. Despite the growing performance over the past few years, we question the validity of this line of research in this work. Specifically, Transformers is arguably the most successful solution to extract the semantic correlations among the elements in a long sequence. However, in time series modeling, we are to extract the temporal relations in an ordered set of continuous points. While employing positional encoding and using tokens to embed sub-series in Transformers facilitate preserving some ordering information, the nature of the permutation-invariant self-attention mechanism inevitably results in temporal information loss. \nTo validate our claim, we introduce a set of embarrassingly simple one-layer linear models named LTSF-Linear for comparison. Experimental results on nine real-life datasets show that LTSF-Linear surprisingly outperforms existing sophisticated Transformer-based LTSF models in all cases, and often by a large margin. Moreover, we conduct comprehensive empirical studies to explore the impacts of various design elements of LTSF models on their temporal relation extraction capability. We hope this surprising finding opens up new research directions for the LTSF task. We also advocate revisiting the validity of Transformer-based solutions for other time series analysis tasks (e.g., anomaly detection) in the future.",
    "url": "https://www.semanticscholar.org/paper/5f404dbba07619cc7f28d75d03f124a52290046e",
    "venue": "AAAI Conference on Artificial Intelligence",
    "publicationDate": "2022-05-26",
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2205.13504",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2205.13504, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "51286000",
        "name": "Ailing Zeng",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "122004634",
        "name": "Mu-Hwa Chen",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "47058944",
        "name": "L. Zhang",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2149106517",
        "name": "Qiang Xu",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 3033,
    "referenceCount": 36,
    "influentialCitationCount": 362,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "externalIds": {
      "DBLP": "journals/corr/abs-2205-13504",
      "ArXiv": "2205.13504",
      "DOI": "10.48550/arXiv.2205.13504",
      "CorpusId": 249097444
    },
    "journal": {
      "name": null,
      "volume": null,
      "pages": "11121-11128"
    },
    "tldr": "Experimental results on nine real-life datasets show that LTSF-Linear surprisingly outperforms existing sophisticated Transformer-based L TSF models in all cases, and often by a large margin.",
    "citations": [
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "7a874e606ad231b84f1cbe89e015355fd3b6d6f2",
          "title": "SFEformer: Frequency-enhanced model for wind speed prediction",
          "year": 2026,
          "authors": [
            {
              "authorId": "2283712417",
              "name": "Yuewei Xue"
            },
            {
              "authorId": "2283138956",
              "name": "Jing Sun"
            },
            {
              "authorId": "2257912424",
              "name": "Shaopeng Guan"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "a40f654d419e14d61b739b52fceb6945b645e825",
          "title": "Hierarchical prediction of irregular multivariate time series from a multi-granularity perspective",
          "year": 2026,
          "authors": [
            {
              "authorId": "2155704118",
              "name": "Jing Zhang"
            },
            {
              "authorId": "2397800637",
              "name": "HuiHui Yu"
            },
            {
              "authorId": "2380140560",
              "name": "Rui Ye"
            },
            {
              "authorId": "2256851934",
              "name": "Qun Dai"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "8ed7580c0c6e3117b3aa400c13c5cad92a38b00d",
          "title": "MCPT-CAF-BiGRU: A multi-scale CNN and ProbSparse-Masked Transformer model with cross-attention fusion and BiGRU for hourly wind speed forecasting",
          "year": 2026,
          "authors": [
            {
              "authorId": "2267891557",
              "name": "Jinsheng Fan"
            },
            {
              "authorId": "2368473106",
              "name": "Guo-An Yu"
            },
            {
              "authorId": "2365316786",
              "name": "Mingmeng Zhao"
            },
            {
              "authorId": "2367882118",
              "name": "Hucheng Zong"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "2e4091f8c3bbca75cdf9e748e8c911c6a1eb5908",
          "title": "A dual large language model framework for forecasting maritime greenhouse gas emissions",
          "year": 2026,
          "authors": [
            {
              "authorId": "30748589",
              "name": "Shuojiang Xu"
            },
            {
              "authorId": "2404431896",
              "name": "Kelin Zhu"
            },
            {
              "authorId": "122384410",
              "name": "F. Zeng"
            },
            {
              "authorId": "2404291105",
              "name": "Min Guo"
            },
            {
              "authorId": "34578251",
              "name": "Benying Tan"
            },
            {
              "authorId": "2404209971",
              "name": "Yiqing Tian"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "4517d25cfe9ab9fcb7fde115427156d2fd523299",
          "title": "EPformer: Unlocking day-ahead electricity price forecasting accuracy using the time–frequency domain feature learning strategy considering renewable energy",
          "year": 2026,
          "authors": [
            {
              "authorId": "2358594045",
              "name": "Hang Fan"
            },
            {
              "authorId": "2373555241",
              "name": "Weican Liu"
            },
            {
              "authorId": "2357221386",
              "name": "Zuhan Zhang"
            },
            {
              "authorId": "2357131354",
              "name": "Wencai Run"
            },
            {
              "authorId": "2309800990",
              "name": "Yunjie Duan"
            },
            {
              "authorId": "2266115296",
              "name": "Dunnan Liu"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "a2cbe4cade2baf08167565441de42d65701872dc",
          "title": "DDformer: Transformer with dynamic variable fusion and dynamic difference attention for multivariate time series long-term forecasting",
          "year": 2026,
          "authors": [
            {
              "authorId": "2406810505",
              "name": "Zhao Li"
            },
            {
              "authorId": "2197900850",
              "name": "Hua Wang"
            },
            {
              "authorId": "2153307863",
              "name": "Fan Zhang"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "ba6b76304d27ad535475d1f3914a3b3677130004",
          "title": "Predictability of complex systems",
          "year": 2026,
          "authors": [
            {
              "authorId": "2278427679",
              "name": "En Xu"
            },
            {
              "authorId": "2278428159",
              "name": "Yilin Bi"
            },
            {
              "authorId": "2386649812",
              "name": "Hongwei Hu"
            },
            {
              "authorId": "2387297893",
              "name": "Xin Chen"
            },
            {
              "authorId": "2386905645",
              "name": "Zhiwen Yu"
            },
            {
              "authorId": "2154404465",
              "name": "Yong Li"
            },
            {
              "authorId": "2388035541",
              "name": "Yanqing Hu"
            },
            {
              "authorId": "2277233831",
              "name": "Tao Zhou"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "c4cfaebf7041face064600699359bf9345db7795",
          "title": "DuoTransFormer for nonlinear seismic response prediction: Cover large, focus local, and enforce law",
          "year": 2026,
          "authors": [
            {
              "authorId": "2407810537",
              "name": "Xinyi Hu"
            },
            {
              "authorId": "2407402453",
              "name": "Congzhen Xiao"
            },
            {
              "authorId": "2406858768",
              "name": "Zhiqiang Zhang"
            },
            {
              "authorId": "2406939675",
              "name": "Jie Gao"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "6b036be879c5d0688a9d82682bdc4b47f49c05f5",
          "title": "NPP-GPT: Forecasting nuclear power plants operating parameters using pre-trained large language model",
          "year": 2026,
          "authors": [
            {
              "authorId": "2353884743",
              "name": "Ling Chang"
            },
            {
              "authorId": "2236067626",
              "name": "Haibo Yu"
            },
            {
              "authorId": "151474022",
              "name": "Minghan Yang"
            },
            {
              "authorId": "2313148824",
              "name": "Ziheng Zhang"
            },
            {
              "authorId": "2352537812",
              "name": "Shuai Chen"
            },
            {
              "authorId": "2353901475",
              "name": "Jianye Wang"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "4ca638b0cd7619a379e545142a129253b30f0e07",
          "title": "K-shape clustering and STL decomposition with transformer fusion RF user power consumption forecasting model",
          "year": 2026,
          "authors": [
            {
              "authorId": "2305841731",
              "name": "Fengjun Shang"
            },
            {
              "authorId": "2387948055",
              "name": "Qianye Liu"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "8c9d9c20ab7b9b128d34285ad1cfcdf60bd26296",
          "title": "Explainable zero-shot trading using multi-agent LLM architecture: A backtested approach for Bitcoin price",
          "year": 2026,
          "authors": [
            {
              "authorId": "2159053025",
              "name": "H. Jung"
            },
            {
              "authorId": "2110309240",
              "name": "Haein Lee"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "2cd099f7a0f6e7a0f5bb83e2300f363bd8881934",
          "title": "A transfer condition-focused model for battery capacity forecast",
          "year": 2026,
          "authors": [
            {
              "authorId": "2384393105",
              "name": "Zhuoyi Qian"
            },
            {
              "authorId": "2117102108",
              "name": "Zhen Chen"
            },
            {
              "authorId": "2353708",
              "name": "E. Pan"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "16171ed3c42032142b1b0b24d2819367d003067c",
          "title": "Multimodal Data Fusion and Deep Learning for Predicting Air Pollution at Construction Sites",
          "year": 2026,
          "authors": [
            {
              "authorId": "2393246810",
              "name": "Ruichuan Zhang"
            },
            {
              "authorId": "2393277480",
              "name": "Jharana Sapkota"
            },
            {
              "authorId": "2393277987",
              "name": "Jenna Krall"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "4376346c4c5bb04aac8bde20526107f8b0b7e6a6",
          "title": "Enhancing person-job fit through multi-temporal career trajectory modeling",
          "year": 2026,
          "authors": [
            {
              "authorId": "51319795",
              "name": "Junmei Feng"
            },
            {
              "authorId": "2393680567",
              "name": "Jiarui Yang"
            },
            {
              "authorId": "2393651099",
              "name": "Shuchun Li"
            },
            {
              "authorId": "2298016824",
              "name": "Qiguang Miao"
            },
            {
              "authorId": "2278688757",
              "name": "Yue Xi"
            },
            {
              "authorId": "2394130675",
              "name": "Zhaoqiang Xia"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "e3993112854f59b3966b165d8775559b213c0874",
          "title": "Clinically informed imputation of medical data using parallel diffusion models",
          "year": 2026,
          "authors": [
            {
              "authorId": "2276693713",
              "name": "Shuaixun Wang"
            },
            {
              "authorId": "2396535990",
              "name": "Xueer Zhang"
            },
            {
              "authorId": "2396600187",
              "name": "Sharon Jewell"
            },
            {
              "authorId": "2396606994",
              "name": "Martyn Boutelle"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "cad89d5f71a88f48a2d1ad7b2a54b6cd5a221108",
          "title": "Dynamic Kolmogorov-Arnold networks for time-varying degradation modeling in solid oxide fuel cells",
          "year": 2026,
          "authors": [
            {
              "authorId": "2298120803",
              "name": "Mohamadali Tofigh"
            },
            {
              "authorId": "2298760556",
              "name": "Daniel J. Smith"
            },
            {
              "authorId": "4964746",
              "name": "A. Hanifi"
            },
            {
              "authorId": "2244531660",
              "name": "C. R. Koch"
            },
            {
              "authorId": "2240893018",
              "name": "Mahdi Shahbakhti"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "b221d67482467b57b60452f75538c821b96134b8",
          "title": "Lightweight grade prediction in froth flotation using interpretable group Contribution-Enabled Weight-Sharing grouped RNN",
          "year": 2026,
          "authors": [
            {
              "authorId": "2110890066",
              "name": "Hu Zhang"
            },
            {
              "authorId": "2374015590",
              "name": "Minyi Yang"
            },
            {
              "authorId": "2287343154",
              "name": "Zhaohui Tang"
            },
            {
              "authorId": "2373991853",
              "name": "Jian Chen"
            },
            {
              "authorId": "2262199433",
              "name": "Yongfang Xie"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "bf13b0c79ec851d03a864665ebfea06ffe8f030c",
          "title": "Hierarchical spatio-temporal dynamic dependency learning for multivariate operating status prediction of pump station units",
          "year": 2026,
          "authors": [
            {
              "authorId": "2402759998",
              "name": "Panpan Qiu"
            },
            {
              "authorId": "2275568349",
              "name": "Jianzhuo Yan"
            },
            {
              "authorId": "3467108",
              "name": "Yongchuan Yu"
            },
            {
              "authorId": "47995429",
              "name": "Hongxia Xu"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "a942e61cfc827a335dd3ec4ee9ed9400cbf07d83",
          "title": "LiteFormer: A lightweight encoder-only Transformer for efficient financial time series forecasting across global stock indices",
          "year": 2026,
          "authors": [
            {
              "authorId": "2290007223",
              "name": "Nguyen Quoc Anh"
            },
            {
              "authorId": "2404696285",
              "name": "Tran Truong Tuan Phat"
            },
            {
              "authorId": "2352714150",
              "name": "H. X. Son"
            },
            {
              "authorId": "2404691232",
              "name": "Thai Thi Thanh Nhan"
            },
            {
              "authorId": "79577237",
              "name": "Nguyen Ngoc Phien"
            },
            {
              "authorId": "2333909713",
              "name": "Trung Phan Hoang Tuan"
            },
            {
              "authorId": "2404989192",
              "name": "Ngan Nguyen Thi Kim"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "b920c6d42e245e90aa4efff1dcd197feb18ce681",
          "title": "Temporal feature mixed inverted transformer: An inverted transformer for effective real-time electricity price forecasting",
          "year": 2026,
          "authors": [
            {
              "authorId": "2237547518",
              "name": "Baichun Wang"
            },
            {
              "authorId": "2406337249",
              "name": "Baoxian Huang"
            },
            {
              "authorId": "2134798666",
              "name": "Qinglun Zhang"
            },
            {
              "authorId": "2395484778",
              "name": "Yan Shi"
            },
            {
              "authorId": "2050274953",
              "name": "Hong-Kun Men"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "1d8e771c3ab24f254abdbe3ec2149a89910c4961",
          "title": "Modeling carbon price dynamics driven by external factors: A hybrid framework combining Mamba state-space model and transformer",
          "year": 2026,
          "authors": [
            {
              "authorId": "2334825145",
              "name": "Zhenkai Qin"
            },
            {
              "authorId": "2334474957",
              "name": "Baozhong Wei"
            },
            {
              "authorId": "2330227687",
              "name": "Qining Luo"
            },
            {
              "authorId": "2346554933",
              "name": "Dongze Wu"
            },
            {
              "authorId": "2346708000",
              "name": "Weiqi Qin"
            },
            {
              "authorId": "2373712761",
              "name": "Xiaochuan Yu"
            },
            {
              "authorId": "2365842187",
              "name": "Ziqian Lin"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "a3451967c1d9f8a77953680eab835c47f54ca42c",
          "title": "Dynamic multi-periodic spatio-temporal graph neural network for multivariate time series forecasting",
          "year": 2026,
          "authors": [
            {
              "authorId": "2350600036",
              "name": "Shuai Zhang"
            },
            {
              "authorId": "2259391994",
              "name": "Zhuolin Li"
            },
            {
              "authorId": "2116457707",
              "name": "Jie Yu"
            },
            {
              "authorId": "2243392302",
              "name": "Lingyu Xu"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "35c9ddd01cd9a17d8bf42af6158d198fa41203db",
          "title": "Component-wise independent adaptive learning and local optimization for long-term forecasting",
          "year": 2026,
          "authors": [
            {
              "authorId": "2362920756",
              "name": "Fei Chen"
            },
            {
              "authorId": "2268444587",
              "name": "Ke Cheng"
            },
            {
              "authorId": "2377634745",
              "name": "Shitong Wang"
            },
            {
              "authorId": "2268482994",
              "name": "Yuanquan Wang"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "4ccdf15a07a20a64d459b1be2b3421b8a672eca8",
          "title": "Decentralized off-grid vehicle-to-building (V2B) operation: A reinforcement learning approach for optimal charge–discharge control of energy storage systems",
          "year": 2026,
          "authors": [
            {
              "authorId": "2322743243",
              "name": "Sanghoon Kim"
            },
            {
              "authorId": "2248099431",
              "name": "M. K. Sim"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "519fa0d3a70667f4fd51a880eebedd592e260c93",
          "title": "Real-time data reconstruction-based joint estimation of state-of-charge and state-of-health of lithium-ion batteries using quantitative feature informed deep learning framework",
          "year": 2026,
          "authors": [
            {
              "authorId": "2334667312",
              "name": "Md Shoaib Akhter Rafi"
            },
            {
              "authorId": "2365815304",
              "name": "Muhammad Anisuzzaman Talukder"
            },
            {
              "authorId": null,
              "name": "Md. Kamrul Hasan"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "54ca8a80283095304d01e9a8fa1bf24572142e17",
          "title": "FiTiformer: A fluctuation–trend modulation model for multi-step forecasting of wastewater influent parameters",
          "year": 2026,
          "authors": [
            {
              "authorId": "2311076745",
              "name": "Yiqi Liu"
            },
            {
              "authorId": "2375965679",
              "name": "Shuyang Ren"
            },
            {
              "authorId": "2216657961",
              "name": "Gang Fang"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "fcebf22d79a58c6ad8d87452b6ff6528602bd194",
          "title": "A time-frequency dual-branch feature dynamic fusion prediction network for tail gas sulfur content prediction in the wet flue gas desulfurization process",
          "year": 2026,
          "authors": [
            {
              "authorId": null,
              "name": "Siheng Zeng"
            },
            {
              "authorId": "2385739918",
              "name": "Hongqiu Zhu"
            },
            {
              "authorId": "2282555437",
              "name": "Sibo Xia"
            },
            {
              "authorId": "2290625758",
              "name": "Bochun Yue"
            }
          ]
        }
      },
      {
        "contexts": [
          "For deep learning–based approaches, we evaluate PatchTST (Nie et al., 2023), iTransformer (Liu et al., 2024b), TimeXer (Wang et al., 2024), ConvTimeNet (Cheng et al., 2025c), and DLinear (Zeng et al., 2023), which leverage advanced neural architectures such as Transformers, CNNs, and MLPs models to effectively capture temporal dependencies.",
          "…evaluate PatchTST (Nie et al., 2023), iTransformer (Liu et al., 2024b), TimeXer (Wang et al., 2024), ConvTimeNet (Cheng et al., 2025c), and DLinear (Zeng et al., 2023), which leverage advanced neural architectures such as Transformers, CNNs, and MLPs models to effectively capture temporal…",
          "In addition, MLP-based approaches (Zeng et al., 2023) demonstrate that simple architectures can achieve competitive performance with improved computational efficiency.",
          "Deep learning baselines (e.g., PatchTST, DLinear, iTransformer), generally suffer from the ”over-smoothing” problem; while they capture the general trend, they consistently underestimate the magnitude of sharp peaks and valleys and exhibit noticeable temporal lag."
        ],
        "intents": [],
        "isInfluential": true,
        "citingPaper": {
          "paperId": "2faa51610b01173bafabf5b42f44f4d1ade2e89b",
          "title": "MemCast: Memory-Driven Time Series Forecasting with Experience-Conditioned Reasoning",
          "year": 2026,
          "authors": [
            {
              "authorId": null,
              "name": "Xiaoyu Tao"
            },
            {
              "authorId": null,
              "name": "Mingyue Cheng"
            },
            {
              "authorId": null,
              "name": "Ze Guo"
            },
            {
              "authorId": null,
              "name": "Shuo Yu"
            },
            {
              "authorId": null,
              "name": "Yaguo Liu"
            },
            {
              "authorId": null,
              "name": "Qi Liu"
            },
            {
              "authorId": null,
              "name": "Shijin Wang"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "685bef8eacd50055976e10da2f9fe5562451d7ed",
          "title": "Visual Reasoning over Time Series via Multi-Agent System",
          "year": 2026,
          "authors": [
            {
              "authorId": "2295987708",
              "name": "Weilin Ruan"
            },
            {
              "authorId": null,
              "name": "Yuxuan Liang"
            }
          ]
        }
      },
      {
        "contexts": [
          "Linear-based: DLin-ear (Zeng et al., 2023)"
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "d33b3f6530a420cd9cb78cff4ac19efa64cf376b",
          "title": "The Label Horizon Paradox: Rethinking Supervision Targets in Financial Forecasting",
          "year": 2026,
          "authors": [
            {
              "authorId": null,
              "name": "Chen-Hui Song"
            },
            {
              "authorId": null,
              "name": "Shuoling Liu"
            },
            {
              "authorId": null,
              "name": "Liyuan Chen"
            }
          ]
        }
      },
      {
        "contexts": [
          "In such settings, prior time-series studies (Zeng et al., 2023; Chen et al., 2023) have observed that when the underlying signal is governed mainly by fixed position-dependent structure rather than rich context-dependent interactions across covariates, multivariate Transformer models can suffer…",
          "…at capturing complex inter-element dependencies and dynamic interactions, they are less suited for settings where tokens have fixed position-dependent roles and the target signal primarily depends on its own temporal position rather than complex interactions (Zeng et al., 2023; Chen et al., 2023)."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "2e2f499cfc5705dcc220e46cbfaede1b98e96cba",
          "title": "Chain-of-Goals Hierarchical Policy for Long-Horizon Offline Goal-Conditioned RL",
          "year": 2026,
          "authors": [
            {
              "authorId": null,
              "name": "Jinwoo Choi"
            },
            {
              "authorId": null,
              "name": "Sang-Hyun Lee"
            },
            {
              "authorId": null,
              "name": "Seung-Woo Seo"
            }
          ]
        }
      },
      {
        "contexts": [
          "Empirical studies further show that many such variants fail to consistently outperform simple linear projections (Zeng et al., 2022).",
          "We evaluate against seven strong baselines: Olinear (Yue et al., 2025), TimeMixer++ and TimeMixer (Wang et al., 2025), iTransformer (Liu et al., 2024), PatchTST (Nie et al., 2023), TimesNet (Wu et al., 2023), and DLinear (Zeng et al., 2022).",
          "Recent empirical results further confirm that this baseline is competitive with Transformer-based models on this dataset (Zeng et al., 2022).",
          "However, a single softmax normalization entangles the classic decomposition of trends, shocks, and seasonality, a challenge emphasized in recent surveys of statistical and deep learning-based time series forecasting (Lim & Zohren, 2021; Wen et al., 2023; Zeng et al., 2022)."
        ],
        "intents": [],
        "isInfluential": true,
        "citingPaper": {
          "paperId": "ba60b72a7e94eb07e45983e74860e0b16d97e2c3",
          "title": "CAPS: Unifying Attention, Recurrence, and Alignment in Transformer-based Time Series Forecasting",
          "year": 2026,
          "authors": [
            {
              "authorId": "2408532342",
              "name": "Viresh Pati"
            },
            {
              "authorId": null,
              "name": "Yubin Kim"
            },
            {
              "authorId": "2408533781",
              "name": "Vinh Pham"
            },
            {
              "authorId": "2408532430",
              "name": "Jevon Twitty"
            },
            {
              "authorId": null,
              "name": "Shihao Yang"
            },
            {
              "authorId": null,
              "name": "Jiecheng Lu"
            }
          ]
        }
      },
      {
        "contexts": [
          "…inputs to encode the entire history; (ii) Compared to other recurrent neural networks, transformers are able to capture long-term dependencies (Zeng et al., 2023; Shi & Shide, 2025), ensuring that all relevant historical variables that may contribute to the optimal policy are effectively…"
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "87c5582b5b28c15a2737959fd62cbe02e6c08446",
          "title": "Designing Time Series Experiments in A/B Testing with Transformer Reinforcement Learning",
          "year": 2026,
          "authors": [
            {
              "authorId": null,
              "name": "Xiangkun Wu"
            },
            {
              "authorId": "2293392556",
              "name": "Qianglin Wen"
            },
            {
              "authorId": "2170543560",
              "name": "Yingying Zhang"
            },
            {
              "authorId": null,
              "name": "Hongtu Zhu"
            },
            {
              "authorId": "2239063313",
              "name": "Ting Li"
            },
            {
              "authorId": "2265521745",
              "name": "Chengchun Shi"
            }
          ]
        }
      },
      {
        "contexts": [
          "DLinear (Zeng et al., 2022) shows that, under the same benchmark protocol, a single linear layer can match or nearly match many increasingly elaborate architectures; previous context-length scaling analyses (Shi et al., 2024) further suggest that simply extending the history window can even hurt…"
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "178f0b59842787a05b8cd5b3d695ec2d6ff7d990",
          "title": "Position: The Inevitable End of One-Architecture-Fits-All-Domains in Time Series Forecasting",
          "year": 2026,
          "authors": [
            {
              "authorId": null,
              "name": "Qinwei Ma"
            },
            {
              "authorId": null,
              "name": "Jingzhe Shi"
            },
            {
              "authorId": null,
              "name": "Jiahao Qiu"
            },
            {
              "authorId": "2294681757",
              "name": "Zaiwen Yang"
            }
          ]
        }
      },
      {
        "contexts": [
          "We employ the channel-independent strategy [Nie et al., 2023] [Zeng et al., 2023] for our approach, where each vari-ate in the input time series is fed independently into SpecTF."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "84af3767e4a2f76531013015dc6098b599e86052",
          "title": "Spectral Text Fusion: A Frequency-Aware Approach to Multimodal Time-Series Forecasting",
          "year": 2026,
          "authors": [
            {
              "authorId": null,
              "name": "Huu Hiep Nguyen"
            },
            {
              "authorId": null,
              "name": "Minh Hoang Nguyen"
            },
            {
              "authorId": null,
              "name": "Dung Nguyen"
            },
            {
              "authorId": null,
              "name": "Hung Le"
            }
          ]
        }
      },
      {
        "contexts": [
          "…in which the model preferentially forecasts single frequencies, not due to bias in the training data, but rather interaction of the model’s time-axis hyperparameters (e.g. patch size) with the context (Zhang et al., 2024; Yu et al., 2025a; Zhou et al., 2022; Wu et al., 2021; Zeng et al., 2023)."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "cc8cce4bca464fd5b71b52bfdd55a9f251e6a542",
          "title": "Universal Redundancies in Time Series Foundation Models",
          "year": 2026,
          "authors": [
            {
              "authorId": "2362503684",
              "name": "Anthony Bao"
            },
            {
              "authorId": "2320924992",
              "name": "Venkata Hasith Vattikuti"
            },
            {
              "authorId": "2363114925",
              "name": "Jeffrey Lai"
            },
            {
              "authorId": "2322501431",
              "name": "William Gilpin"
            }
          ]
        }
      },
      {
        "contexts": [
          "DLinear [36]: A simple yet effective baseline that decomposes time series into trend and remainder components, processing each with a single linear layer."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "fbfe9f3f8759b921d0062b6cc29fdf2167d8b7d2",
          "title": "ASGMamba: Adaptive Spectral Gating Mamba for Multivariate Time Series Forecasting",
          "year": 2026,
          "authors": [
            {
              "authorId": null,
              "name": "Qianyang Li"
            },
            {
              "authorId": null,
              "name": "Xingjun Zhang"
            },
            {
              "authorId": null,
              "name": "Shaoxun Wang"
            },
            {
              "authorId": "2141318475",
              "name": "Jia Wei"
            },
            {
              "authorId": null,
              "name": "Yueqi Xing"
            }
          ]
        }
      },
      {
        "contexts": [
          "In recent years, the method-based Transformer has made great progress in the research of temporal data prediction [4]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "c8c9288d8a9d287c65775fd634d71905e699ccf7",
          "title": "CBEC inventory optimization model design based on spatiotemporal attention and transformer architecture",
          "year": 2026,
          "authors": [
            {
              "authorId": null,
              "name": "Zongping Lin"
            },
            {
              "authorId": null,
              "name": "Yingyi Huang"
            },
            {
              "authorId": null,
              "name": "Jing Yang"
            },
            {
              "authorId": null,
              "name": "Chunhu Cui"
            },
            {
              "authorId": "2344987109",
              "name": "Yabin Lian"
            },
            {
              "authorId": null,
              "name": "Honglei Zhang"
            }
          ]
        }
      },
      {
        "contexts": [
          "This pre-processed version has been widely used in subsequent studies (e.g., [9], [30], [120]) and continues to be applied in recent research.",
          "We conducted our experiments using the public codebase provided by the authors of DLinear 8 [9], TiRex 9 [125], and Mamba4Cast 10 [126].",
          "…c [71] HAR-PSO-ESN M&F [106] RF-DE-LSTM Traf ﬁ c [33] FedFormer GP [119] SCINet GP [28] CNN-LSTM Eco, CC & Wh [120] N-Hits GP [121] DA-Conv-LSTM GP [9] N/DLinear GP [32] PatchTST GP [14] RLinear GP size of the models, unlike the autoregressive models), many variations and combinations are used in…",
          "Random walks [132] and naive models [9] also belong to the Linear category.",
          "While these methods are typically useful in short-term forecasting, recent studies have also proven interesting mid-to-long-term forecasting accuracy [9], [14], [24], [47].",
          "Among the challenges in this ﬁ eld [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], this paper focuses on Long-Term Forecasting (LTF), which stands out as one of the most intriguing and critical open issues.",
          "A signi ﬁ cant shift occurred with [9], which demonstrated that linear models could outperform Transform-ers, showing a 22.89% improvement.",
          "However, most works that used this dataset (e.g., [9], [33], [119]) relied on the code from [117], which truncates the dataset to 14,400 and 57,600 points, respectively, without documenting it.",
          "In conclusion, it is evident that while many LTF data-sets have played a crucial role in benchmarking models, their quality and transparency remain a problem [9].",
          "This strategy, whose goal is to compute the mean of the MSEs for all forecasts generated on x Test , is the most prevalent method for evaluating time series forecasting models (e.g., [9], [34], [117], [120])."
        ],
        "intents": [],
        "isInfluential": true,
        "citingPaper": {
          "paperId": "e95838b4d96b8be97ac1233deffc5c110f3c81e7",
          "title": "Long-Term Time Series Forecasting: The Good, the Bad, and the Ugly",
          "year": 2026,
          "authors": [
            {
              "authorId": "2348824253",
              "name": "Lorenzo Epifani"
            },
            {
              "authorId": "119796696",
              "name": "Alessandro Falcetta"
            },
            {
              "authorId": "2265680067",
              "name": "Manuel Roveri"
            }
          ]
        }
      },
      {
        "contexts": [
          "…(Wu et al., 2023), PatchTST (Nie et al., 2023), Autoformer (Wu et al., 2021), ETSformer (Woo et al., 2022), FiLM (Zhou et al., 2022), DLinear (Zeng et al., 2023), GP-VAE (Fortuin et al., 2020), and CSDI (Tashiro et al., 2021), we adopt the optimized hyperparameters reported in (Du et al.,…",
          "…(Wu et al., 2023), PatchTST (Nie et al., 2023), Autoformer (Wu et al., 2021), ETSformer (Woo et al., 2022), FiLM (Zhou et al., 2022), DLinear (Zeng et al., 2023), GP-VAE (Fortuin et al., 2020), CSDI (Tashiro et al., 2021), Glocal (Yang et al., 2025b), Sinkhorn (Muzellec et al., 2020), TDM…",
          "For Crossformer (Zhang & Yan, 2023), TimesNet (Wu et al., 2023), PatchTST (Nie et al., 2023), Autoformer (Wu et al., 2021), ETSformer (Woo et al., 2022), FiLM (Zhou et al., 2022), DLinear (Zeng et al., 2023), GP-VAE (Fortuin et al., 2020), and CSDI (Tashiro et al., 2021), we adopt the optimized hyperparameters reported in (Du et al., 2024).",
          "Since this paper focuses on the TSDI task, we adopt a set of widely used TSDI methods as base-lines to evaluate the effectiveness of the proposed SPIRIT framework, including Crossformer (Zhang & Yan, 2023), TimesNet (Wu et al., 2023), PatchTST (Nie et al., 2023), Autoformer (Wu et al., 2021), ETSformer (Woo et al., 2022), FiLM (Zhou et al., 2022), DLinear (Zeng et al., 2023), GP-VAE (Fortuin et al., 2020), CSDI (Tashiro et al., 2021), Glocal (Yang et al., 2025b), Sinkhorn (Muzellec et al., 2020), TDM (Zhao et al., 2023), and PSW-I (Wang et al., 2025b).",
          "We categorize Crossformer, TimesNet, PatchTST, Autoformer, ETSformer, FiLM, and DLinear as discriminative TSDI approaches, as they learn an internal time-series prediction model and use it to perform imputation.",
          "In this manuscript, we adopt a set of widely used TSDI methods as baselines to evaluate the effectiveness of the proposed SPIRIT framework, including Crossformer (Zhang & Yan, 2023), TimesNet (Wu et al., 2023), PatchTST (Nie et al., 2023), Autoformer (Wu et al., 2021), ETSformer (Woo et al., 2022), FiLM (Zhou et al., 2022), DLinear (Zeng et al., 2023), GP-VAE (Fortuin et al., 2020), CSDI (Tashiro et al., 2021), Glocal (Yang et al., 2025b), Sinkhorn (Muzellec et al., 2020), TDM (Zhao et al., 2023), and PSW-I (Wang et al., 2025b)."
        ],
        "intents": [],
        "isInfluential": true,
        "citingPaper": {
          "paperId": "d5742c46c132761213e7d2e5309819bed2445984",
          "title": "Analyzing and Improving Diffusion Models for Time-Series Data Imputation: A Proximal Recursion Perspective",
          "year": 2026,
          "authors": [
            {
              "authorId": "2162399554",
              "name": "Zhichao Chen"
            },
            {
              "authorId": null,
              "name": "Hao Wang"
            },
            {
              "authorId": null,
              "name": "Fangyikang Wang"
            },
            {
              "authorId": null,
              "name": "Licheng Pan"
            },
            {
              "authorId": null,
              "name": "Zhengnan Li"
            },
            {
              "authorId": "2373606503",
              "name": "Yunfei Teng"
            },
            {
              "authorId": "2262403185",
              "name": "Haoxuan Li"
            },
            {
              "authorId": "2346896471",
              "name": "Zhouchen Lin"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "c3f8f095f976866831c5083904ba34b8919ea886",
          "title": "A stable technical feature with GRU-CNN-GA fusion",
          "year": 2026,
          "authors": [
            {
              "authorId": "2363558026",
              "name": "Zong Ke"
            },
            {
              "authorId": "2395582067",
              "name": "Jiaqing Shen"
            },
            {
              "authorId": "2394757478",
              "name": "Xuanyi Zhao"
            },
            {
              "authorId": "2394821038",
              "name": "Xinghao Fu"
            },
            {
              "authorId": "2344513108",
              "name": "Yang Wang"
            },
            {
              "authorId": "2309671367",
              "name": "Zichao Li"
            },
            {
              "authorId": "2380637167",
              "name": "Lingjie Liu"
            },
            {
              "authorId": "2394826387",
              "name": "Huailing Mu"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "4ae3701bf5f06c5efad8d12b98da562aa2ed497b",
          "title": "Short-term wind speed forecasting using temporal fusion transformers with causal dilated convolution",
          "year": 2026,
          "authors": [
            {
              "authorId": "2268066380",
              "name": "Krishna Prakash Natarajan"
            },
            {
              "authorId": "2370515875",
              "name": "Jai Govind Singh"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "f70abe74b03e419988ad84f2befba97fe6ba6e47",
          "title": "How will arctic shipping emissions evolve? A spatiotemporal topology-aware transformer approach",
          "year": 2026,
          "authors": [
            {
              "authorId": "2299119780",
              "name": "Younghwi Kim"
            },
            {
              "authorId": "2397299632",
              "name": "Jaehoon Lee"
            },
            {
              "authorId": "2287085045",
              "name": "Hyerim Bae"
            },
            {
              "authorId": "8620441",
              "name": "Sunghyun Sim"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "d38fa5e6b85ec170656f3f91f89513a6e0fa488f",
          "title": "Deep multimodal fusion of spectral and visual data for laser welding defect classification",
          "year": 2026,
          "authors": [
            {
              "authorId": "2315586235",
              "name": "Qin Zhang"
            },
            {
              "authorId": "2315628637",
              "name": "Zhongyou Zhao"
            },
            {
              "authorId": "2323322434",
              "name": "Zhenming Wang"
            },
            {
              "authorId": "2377314025",
              "name": "Zixuan Wan"
            },
            {
              "authorId": "2358582992",
              "name": "Hui-ping Wang"
            },
            {
              "authorId": "2358728149",
              "name": "Guangze Li"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "b983bfee8b8bd83a7de493a0997e516fdd0c4917",
          "title": "A hybrid deep learning model integrating interpretability and cloud model for dam deformation and dynamic risk early warning",
          "year": 2026,
          "authors": [
            {
              "authorId": "2397988516",
              "name": "Wencheng Wang"
            },
            {
              "authorId": "2141625084",
              "name": "Xiuwen Li"
            },
            {
              "authorId": "2283302402",
              "name": "Hao Wu"
            },
            {
              "authorId": "2347835052",
              "name": "Qiang Yue"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "29a33216ef1e21f82aac6daa150d3914d59d8225",
          "title": "AMSP-Net: Adaptive multi-scale patch network for long time-series forecasting",
          "year": 2026,
          "authors": [
            {
              "authorId": "2367837564",
              "name": "Yangbo Xu"
            },
            {
              "authorId": "2368071456",
              "name": "Dongsheng Liu"
            },
            {
              "authorId": "2368303473",
              "name": "Tong Wu"
            },
            {
              "authorId": "2368826511",
              "name": "Junjie Lin"
            },
            {
              "authorId": "2109017396",
              "name": "Yahui Chen"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "804352173bc03ff7c58393c3c6a2a2e173540f70",
          "title": "Joint estimation of the state-of-charge and state-of-energy of lithium-ion batteries under diverse temperatures and discharging conditions using a data-driven model",
          "year": 2026,
          "authors": [
            {
              "authorId": "2313676694",
              "name": "Baoliang Chen"
            },
            {
              "authorId": "2398616665",
              "name": "Jujin Pan"
            },
            {
              "authorId": "2336536222",
              "name": "Yonggui Liu"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "04aa307867ade5a66c1e6bcaf57d66291a9b2df7",
          "title": "SeaTraNet: A local-global feature fusion network for abnormal behavior recognition of single trawler",
          "year": 2026,
          "authors": [
            {
              "authorId": "2401119949",
              "name": "Lingkai Kong"
            },
            {
              "authorId": "2493146",
              "name": "Zhuhua Hu"
            },
            {
              "authorId": "3234288",
              "name": "Yaochi Zhao"
            },
            {
              "authorId": "2316751504",
              "name": "Wei Wu"
            },
            {
              "authorId": "2296383673",
              "name": "Yanming Gu"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "fc18ff00bc31480d5a911553a5c3ffc4f9ced689",
          "title": "Adaptive progressive learning for minimizing false alarms in fire detection with limited Re-training data",
          "year": 2026,
          "authors": [
            {
              "authorId": "2276637421",
              "name": "Yusun Ahn"
            },
            {
              "authorId": "2276758094",
              "name": "Soocheol Kim"
            },
            {
              "authorId": "2114180472",
              "name": "Kyuwon Han"
            },
            {
              "authorId": "2145140121",
              "name": "K. Lee"
            },
            {
              "authorId": "46402353",
              "name": "HoeSung Yang"
            },
            {
              "authorId": "2179972352",
              "name": "Kwangsoo Cho"
            },
            {
              "authorId": "2228481331",
              "name": "Jin Hwa Ryu"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "ed5e14f439cb82156ac0544522086dd02da5c7e3",
          "title": "A dual-branch multi-scale encoding and fusion model for multivariate time series forecasting",
          "year": 2026,
          "authors": [
            {
              "authorId": "2352306876",
              "name": "Jiachao Li"
            },
            {
              "authorId": "2401203061",
              "name": "Mengxiao Yin"
            },
            {
              "authorId": "2401234790",
              "name": "Junyuan Huang"
            },
            {
              "authorId": "2404428917",
              "name": "Tao Luo"
            }
          ]
        }
      }
    ]
  },
  {
    "paperId": "7519a1e9e7371df79bd8a21cee871feb0ec597a5",
    "title": "UNETR: Transformers for 3D Medical Image Segmentation",
    "year": 2021,
    "abstract": "Fully Convolutional Neural Networks (FCNNs) with contracting and expanding paths have shown prominence for the majority of medical image segmentation applications since the past decade. In FCNNs, the encoder plays an integral role by learning both global and local features and contextual representations which can be utilized for semantic output prediction by the decoder. Despite their success, the locality of convolutional layers in FCNNs, limits the capability of learning long-range spatial dependencies. Inspired by the recent success of transformers for Natural Language Processing (NLP) in long-range sequence learning, we reformulate the task of volumetric (3D) medical image segmentation as a sequence-to-sequence prediction problem. We introduce a novel architecture, dubbed as UNEt TRansformers (UNETR), that utilizes a transformer as the encoder to learn sequence representations of the input volume and effectively capture the global multi-scale information, while also following the successful \"U-shaped\" network design for the encoder and decoder. The transformer encoder is directly connected to a decoder via skip connections at different resolutions to compute the final semantic segmentation output. We have validated the performance of our method on the Multi Atlas Labeling Beyond The Cranial Vault (BTCV) dataset for multi-organ segmentation and the Medical Segmentation Decathlon (MSD) dataset for brain tumor and spleen segmentation tasks. Our benchmarks demonstrate new state-of-the-art performance on the BTCV leaderboard.",
    "url": "https://www.semanticscholar.org/paper/7519a1e9e7371df79bd8a21cee871feb0ec597a5",
    "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
    "publicationDate": "2021-03-18",
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2103.10504",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2103.10504, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "31374559",
        "name": "Ali Hatamizadeh",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "144041873",
        "name": "Dong Yang",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "144531567",
        "name": "H. Roth",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "3262394",
        "name": "Daguang Xu",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 2295,
    "referenceCount": 59,
    "influentialCitationCount": 280,
    "fieldsOfStudy": [
      "Computer Science",
      "Engineering"
    ],
    "externalIds": {
      "DBLP": "journals/corr/abs-2103-10504",
      "ArXiv": "2103.10504",
      "DOI": "10.1109/WACV51458.2022.00181",
      "CorpusId": 232290634
    },
    "journal": {
      "name": "2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)",
      "volume": null,
      "pages": "1748-1758"
    },
    "tldr": "This work reformulates the task of volumetric (3D) medical image segmentation as a sequence-to-sequence prediction problem and introduces a novel architecture, dubbed as UNEt TRansformers (UNETR), that utilizes a transformer as the encoder to learn sequence representations of the input volume and effectively capture the global multi-scale information.",
    "citations": [
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "28fba5253a4afa0e3411c96ee6a1777f73e012fe",
          "title": "DiffMoE-UNet: A differential transformer with Mixture-of-Experts for accurate medical image segmentation",
          "year": 2026,
          "authors": [
            {
              "authorId": "2281317014",
              "name": "Jaouad Tagnamas"
            },
            {
              "authorId": "3438903",
              "name": "Hiba Ramadan"
            },
            {
              "authorId": "2519197",
              "name": "Ali Yahyaouy"
            },
            {
              "authorId": "2358292860",
              "name": "Hamid Tairi"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "5699336c21012b45709a2c701b0eab0e72b24b7a",
          "title": "A unified deep network for thin- and dense-slice reconstruction: Improving through-plane resolution in clinical MRI",
          "year": 2026,
          "authors": [
            {
              "authorId": "2390939383",
              "name": "Rong Zhang"
            },
            {
              "authorId": "2371368431",
              "name": "Lingtong Zhang"
            },
            {
              "authorId": "2389486153",
              "name": "Jiaen Wang"
            },
            {
              "authorId": "2366859721",
              "name": "Xuhe Huangfu"
            },
            {
              "authorId": "2174341024",
              "name": "Kecheng Yuan"
            },
            {
              "authorId": "2064466237",
              "name": "B. Qiu"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "627c0904e7e7ae0ab398c97bb1c9b78bc3f04dea",
          "title": "MNSeg: Mamba-based 3D neuron segmentation integrated with bidirectional attention mechanism and topological loss",
          "year": 2026,
          "authors": [
            {
              "authorId": "2408344252",
              "name": "Xinle Dai"
            },
            {
              "authorId": "2394170529",
              "name": "Qiufu Li"
            },
            {
              "authorId": "2276486592",
              "name": "Linlin Shen"
            },
            {
              "authorId": "2119295832",
              "name": "Wenting Chen"
            },
            {
              "authorId": "2302559038",
              "name": "Weijia Fan"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "2e7764de286a9ddf22db10864b6f5098acb6b792",
          "title": "End-to-end predictions of trabecular bone structural and mechanical properties from resolution adaptive CT imaging",
          "year": 2026,
          "authors": [
            {
              "authorId": "2306517228",
              "name": "Peixuan Ge"
            },
            {
              "authorId": "2257894235",
              "name": "P. Wong"
            },
            {
              "authorId": "2145338760",
              "name": "Shibo Li"
            },
            {
              "authorId": "2306471676",
              "name": "Shuwei Zhang"
            },
            {
              "authorId": "2130211949",
              "name": "Lihai Zhang"
            },
            {
              "authorId": "2294380351",
              "name": "Qiong Wang"
            },
            {
              "authorId": "33216594",
              "name": "Baoliang Zhao"
            },
            {
              "authorId": "2196192074",
              "name": "Ying Hu"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "1a8c2c4a36048e1c4e91db5c696ab0ed59c81509",
          "title": "MRFMA: A hybrid paradigm integrating multi-receptive field network with mediator attention for 3D multi-organ segmentation",
          "year": 2026,
          "authors": [
            {
              "authorId": "145319486",
              "name": "Hengfei Cui"
            },
            {
              "authorId": "2340351748",
              "name": "Jiatong Li"
            },
            {
              "authorId": "2340246305",
              "name": "Dianrong Du"
            },
            {
              "authorId": "2261248415",
              "name": "Yanning Zhang"
            },
            {
              "authorId": "2256029669",
              "name": "Yong Xia"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "68470661b8dabc968ee2561642228ac56b1cc466",
          "title": "Multi-source multi-task meta-learning with task-oriented distribution alignment for gastric cancer analysis in CT images",
          "year": 2026,
          "authors": [
            {
              "authorId": "1962324382",
              "name": "Yongtao Zhang"
            },
            {
              "authorId": "2269331435",
              "name": "Hongwei Yu"
            },
            {
              "authorId": "2071058354",
              "name": "Ning Yuan"
            },
            {
              "authorId": "2260613390",
              "name": "Yiyao Liu"
            },
            {
              "authorId": "2149183276",
              "name": "Yingpeng Xie"
            },
            {
              "authorId": "2350606835",
              "name": "Huang Chen"
            },
            {
              "authorId": "2331902669",
              "name": "Yu Ren"
            },
            {
              "authorId": "1900037337",
              "name": "Jixin Luan"
            },
            {
              "authorId": "2134206836",
              "name": "Kuan Lv"
            },
            {
              "authorId": "2155412526",
              "name": "Tianfu Wang"
            },
            {
              "authorId": "2319092881",
              "name": "Lei Dong"
            },
            {
              "authorId": "2118629167",
              "name": "Jing Qin"
            },
            {
              "authorId": "2261094842",
              "name": "Linlin Shen"
            },
            {
              "authorId": "2339753910",
              "name": "Guolin Ma"
            },
            {
              "authorId": "2251028190",
              "name": "Baiying Lei"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "441043c77e2b8193e02245021bd4db4f0407289a",
          "title": "HSPF-Net: Hybrid CNN-transformer with serial-parallel fusion for skin lesion segmentation",
          "year": 2026,
          "authors": [
            {
              "authorId": "2400385670",
              "name": "Hao Fang"
            },
            {
              "authorId": "2240246458",
              "name": "Yu Sun"
            },
            {
              "authorId": "2399222127",
              "name": "Shuai Zhang"
            },
            {
              "authorId": "2354582180",
              "name": "Xuyang Teng"
            },
            {
              "authorId": "2354670353",
              "name": "Xiaohui Li"
            },
            {
              "authorId": "2240372501",
              "name": "Xiaodong Yu"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "1cf661142fa3b9903c80df654f8b9eaf080705f1",
          "title": "Contrastive coarse-to-fine medical segmentation with prototype guidance and dual-granularity fusion",
          "year": 2026,
          "authors": [
            {
              "authorId": "2380849060",
              "name": "Zekai Liu"
            },
            {
              "authorId": "2403718975",
              "name": "Muxi Li"
            },
            {
              "authorId": "2406388312",
              "name": "Fei Yang"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "dac5df02d49a09c94de851885043da5655a5ec02",
          "title": "Cross-Granularity Fusion Vision Mamba UNet for medical image segmentation",
          "year": 2026,
          "authors": [
            {
              "authorId": "2392428623",
              "name": "Tuersunjiang Baidi"
            },
            {
              "authorId": "2368479632",
              "name": "Zitong Ren"
            },
            {
              "authorId": "1869945",
              "name": "K. Ubul"
            },
            {
              "authorId": "2119595",
              "name": "Alimjan Aysa"
            },
            {
              "authorId": "2374465981",
              "name": "Boyuan Li"
            },
            {
              "authorId": "2349948856",
              "name": "Shihao Wang"
            }
          ]
        }
      },
      {
        "contexts": [
          "● For SL comparison methods, we choose UNETR [14], SwinUNETR [15] and DS-Former [43 ].",
          "For example, UNETR [14] employs a Transformer encoder to learn the sequential representation and global multi-scale information of the input volume, and uses skip connections along with a 3D CNN decoder to supplement information and localize targets.",
          "Next, fine-tune the supervised segmentation model UNETR [ 14], which has loaded the pre-trained weights, on the training images to output segmentation masks."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "b81049cb17223d29d168e00e5e347895cae32d4f",
          "title": "GFPP-MAE: gradient-guided frequency reconstruction and position predictions advance MAE for 3D CT image segmentation",
          "year": 2026,
          "authors": [
            {
              "authorId": null,
              "name": "Yuping Peng"
            },
            {
              "authorId": null,
              "name": "Xing Wu"
            },
            {
              "authorId": null,
              "name": "Xing Xiao"
            },
            {
              "authorId": "2267193544",
              "name": "Chengliang Wang"
            },
            {
              "authorId": "2109603114",
              "name": "Hongqian Wang"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "e6717ddb770a0842eb7c4c6485cf4f85afd67f87",
          "title": "GCIFormer: Global Context Interaction Transformer for volumetric medical image segmentation",
          "year": 2026,
          "authors": [
            {
              "authorId": "2375688739",
              "name": "Jiaxu Jiang"
            },
            {
              "authorId": "2375233853",
              "name": "Heng-Chao Li"
            },
            {
              "authorId": "2299943976",
              "name": "Sen Lei"
            },
            {
              "authorId": "2087010518",
              "name": "Nanqing Liu"
            },
            {
              "authorId": "2377754866",
              "name": "Kezhou Li"
            },
            {
              "authorId": "2375338023",
              "name": "Yongjian Sun"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "6b5987384c54f97a0689bd4b43ad7ff3fbef291e",
          "title": "Multimodal synthetic images generation and aggregation framework for low-cost and high-accuracy nasopharyngeal carcinoma tumor segmentation",
          "year": 2026,
          "authors": [
            {
              "authorId": "51062343",
              "name": "Yongbao Li"
            },
            {
              "authorId": "2117916985",
              "name": "Xuanru Zhou"
            },
            {
              "authorId": "2292715803",
              "name": "Huali Li"
            },
            {
              "authorId": "2399029368",
              "name": "Yinda Du"
            },
            {
              "authorId": "2398294107",
              "name": "Ruofei Li"
            },
            {
              "authorId": "2292836206",
              "name": "Linghong Zhou"
            },
            {
              "authorId": "2342101484",
              "name": "Ting Song"
            }
          ]
        }
      },
      {
        "contexts": [
          "ModelsforMRIorCT scanscontinuedtobuildontheautoencoderstructureofthe U-Net[20],[21]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "62408bf430439e8c823b5de42fa608194407b4a8",
          "title": "OCT Imaging for Pose Estimation and Feedback Control of an Articulated Magnetic Surgical Tool",
          "year": 2026,
          "authors": [
            {
              "authorId": "2082812554",
              "name": "Erik Fredin"
            },
            {
              "authorId": "2310824842",
              "name": "Nirmal Pol"
            },
            {
              "authorId": "2310825179",
              "name": "Anton Zaliznyi"
            },
            {
              "authorId": "2398413510",
              "name": "Dmytro Fishman"
            },
            {
              "authorId": "2310825177",
              "name": "Eric D. Diller"
            },
            {
              "authorId": "2118604743",
              "name": "L. Kahrs"
            }
          ]
        }
      },
      {
        "contexts": [
          "These methods [6, 7] enhance the accuracy of organ segmentation [8] and the precision of lesion identiﬁcation [9], particularly in tasks of retinal vessel [10], and cardiac segmentation [11]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "686bbe2cbd1b916591d0ca9a9da3fd4e0ae60163",
          "title": "Intrinsic feature consistency learning based on dual-branch network for accurate semi-supervised medical image segmentation",
          "year": 2026,
          "authors": [
            {
              "authorId": null,
              "name": "Yunjun Yu"
            },
            {
              "authorId": null,
              "name": "Ping Zhao"
            },
            {
              "authorId": null,
              "name": "Chaohao Yu"
            },
            {
              "authorId": "47301232",
              "name": "Hongwei Tao"
            },
            {
              "authorId": "2408476410",
              "name": "Jiaoyu Yu"
            },
            {
              "authorId": null,
              "name": "Yubo Gong"
            },
            {
              "authorId": "2408478668",
              "name": "Min Chen"
            }
          ]
        }
      },
      {
        "contexts": [
          "Our results demonstrate that this framework comprehensively surpasses state-of-the-art DL models, including 3D U-Net [16], 3D V-Net [35], 3D UNETR [36], and 3D nn-Unet [37] in both LV segmentation and LVEF classification."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "5892c2e14c95c0431d754d6efd5c11ecde48a82a",
          "title": "Interpretable and backpropagation-free Green Learning for efficient multi-task echocardiographic segmentation and classification",
          "year": 2026,
          "authors": [
            {
              "authorId": "2322239389",
              "name": "Jyun-Ping Kao"
            },
            {
              "authorId": "2221608640",
              "name": "Jiaxin Yang"
            },
            {
              "authorId": null,
              "name": "C.-C. Jay Kuo"
            },
            {
              "authorId": "2406997775",
              "name": "Jonghye Woo"
            }
          ]
        }
      },
      {
        "contexts": [
          "However, Fourier transforms (FFT) provide global frequency representations that may not adequately capture multi-scale local structures in medical images (24)."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "f8aa2350645b9fb4a1eb464cfcececfcbd8a5512",
          "title": "Wavelet-enhanced boundary adaptation network for liver hemangioma segmentation in non-contrast CT",
          "year": 2026,
          "authors": [
            {
              "authorId": "2407304765",
              "name": "Bohao Zeng"
            },
            {
              "authorId": "2256831164",
              "name": "Lei Zhang"
            },
            {
              "authorId": null,
              "name": "Liling Peng"
            },
            {
              "authorId": null,
              "name": "Wenming Cao"
            },
            {
              "authorId": "2284288536",
              "name": "Xiaotao Fan"
            },
            {
              "authorId": "2407885603",
              "name": "Xinfeng Sun"
            },
            {
              "authorId": "2407452190",
              "name": "Xin Gao"
            }
          ]
        }
      },
      {
        "contexts": [
          "We selected UNETR++ [38], an advanced transformer-based model that builds upon UNETR (“U-Net with Transformer Encoder”) [16]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "f2fd3c057ea22f59b6b9b9e941b9477b20347e43",
          "title": "Robust Computational Extraction of Non-Enhancing Hypercellular Tumor Regions from Clinical Imaging Data",
          "year": 2026,
          "authors": [
            {
              "authorId": "2406838222",
              "name": "A. Brawanski"
            },
            {
              "authorId": "2406839200",
              "name": "Th. Schaffer"
            },
            {
              "authorId": "2406827714",
              "name": "F. Raab"
            },
            {
              "authorId": "144505077",
              "name": "K. Schebesch"
            },
            {
              "authorId": "2406838417",
              "name": "M. Schrey"
            },
            {
              "authorId": "2406837223",
              "name": "Chr. Doenitz"
            },
            {
              "authorId": "2175019778",
              "name": "A. M. Tom'e"
            },
            {
              "authorId": "2285772862",
              "name": "E. W. Lang"
            }
          ]
        }
      },
      {
        "contexts": [
          "Architectures such as UNETR and Swin UNETR show that hierarchical Transformer backbones, including the Swin Transformer, can provide powerful representations for 3D multi-modal brain tumor segmentation [8, 9, 19].",
          "However, emerging Transformer-based architectures, such as UNETR[9] and nnFormer[34], primarily focus on integrating self-attention mechanisms into the encoder while often retaining a standard, weakly supervised decoder."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "782f69342f45ca4c2c5329a5c956a2adb8917e1d",
          "title": "BMDS-Net: A Bayesian Multi-Modal Deep Supervision Network for Robust Brain Tumor Segmentation",
          "year": 2026,
          "authors": [
            {
              "authorId": "2407031873",
              "name": "Yan Zhou"
            },
            {
              "authorId": "2408183675",
              "name": "Zhen Huang"
            },
            {
              "authorId": "2406881097",
              "name": "Yingqiu Li"
            },
            {
              "authorId": "2406844230",
              "name": "Ouyang Yue"
            },
            {
              "authorId": "2406846070",
              "name": "Suncheng Xiang"
            },
            {
              "authorId": "2407516293",
              "name": "Zehua Wang"
            }
          ]
        }
      },
      {
        "contexts": [
          "To address this core limitation of locality, architectural paradigms that integrate Vision Transformers with U-shaped networks, exemplified by models such as UNETR [19] and SwinUNETR [20], have been introduced to the field [17], [18].",
          "I, on the particularly challenging CREMI-A benchmark, NeuroMamba yields 0.166 PEA [27] 0.950 1.017 1.967 0.200 LSD [25] 1.217 0.821 2.038 0.181 UNETR [19] 1.008 0.915 1.923 0.162 SwinUNETR [20] 1.014 0.901 1.915 0.168 U-Mamba [32] 1.058 0.897 1.955 0.173 SegMamba [33]"
        ],
        "intents": [],
        "isInfluential": true,
        "citingPaper": {
          "paperId": "f2595e92b085e3386424f7d9c891727db7d30e78",
          "title": "NeuroMamba: Multi-Perspective Feature Interaction with Visual Mamba for Neuron Segmentation",
          "year": 2026,
          "authors": [
            {
              "authorId": "2300180230",
              "name": "Liuyun Jiang"
            },
            {
              "authorId": "2406281664",
              "name": "Yizhuo Lu"
            },
            {
              "authorId": "2304362494",
              "name": "Yanchao Zhang"
            },
            {
              "authorId": "2108477514",
              "name": "Jiazheng Liu"
            },
            {
              "authorId": "2223984386",
              "name": "Hua Han"
            }
          ]
        }
      },
      {
        "contexts": [
          "Subsequent advances have built upon this foundation, including nnU-Net [36], which provides automated architecture design for medical imaging, and transformer-based approaches such as UNETR [3], which integrate self-attention mechanisms for improved spatial reasoning."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "27ecdf199667f83b090822fe35c8f1405234e8a3",
          "title": "Sub-Region-Aware Modality Fusion and Adaptive Prompting for Multi-Modal Brain Tumor Segmentation",
          "year": 2026,
          "authors": [
            {
              "authorId": "2149255992",
              "name": "Shadi Alijani"
            },
            {
              "authorId": "2374148739",
              "name": "Fereshteh Aghaee Meibodi"
            },
            {
              "authorId": "2239389091",
              "name": "Homayoun Najjaran"
            }
          ]
        }
      },
      {
        "contexts": [
          "In order to balance category imbalance and gradient stability, this paper designs a dynamic weight-based hybrid loss function, which is defined as follows [4] : L hybrid = α ⋅ L Dice + (1 − α ) ⋅ L CE Among them, is the cross-entropy loss, and is the Dice loss."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "f5f46b0fae5322f4169725e82ead07c85e75bd9f",
          "title": "Research on U-Net medical image segmentation algorithm based on attention mechanism",
          "year": 2026,
          "authors": [
            {
              "authorId": "2406208001",
              "name": "Xuyan Wei"
            },
            {
              "authorId": "2407158791",
              "name": "Hao Fang"
            }
          ]
        }
      },
      {
        "contexts": [
          "Three commonly used spacings in 3D patch-based segmentation were examined: (i) nnU-Net’s median spacing for BTCV (3.0, 0.75, 0.75) [1], (ii) isotropic 1 mm spacing [18], and (iii) a low-resolution spacing (2.0, 1.5, 1.5) [19]."
        ],
        "intents": [],
        "isInfluential": true,
        "citingPaper": {
          "paperId": "ce220ec16b3fcda1a4d8eac3e85b1a509d580ee6",
          "title": "LocBAM: Advancing 3D Patch-Based Image Segmentation by Integrating Location Contex",
          "year": 2026,
          "authors": [
            {
              "authorId": "2405888302",
              "name": "Donnate Hooft"
            },
            {
              "authorId": "2306660932",
              "name": "Stefan M. Fischer"
            },
            {
              "authorId": "3468980",
              "name": "Cosmin I. Bercea"
            },
            {
              "authorId": "4892738",
              "name": "J. Peeken"
            },
            {
              "authorId": "2251664322",
              "name": "J. A. Schnabel"
            }
          ]
        }
      },
      {
        "contexts": [
          "The Transformer-based methods include UNETR [31], UNETR++ [32], SwinUNETR [33], and Swin-UNETRv2 [34]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "b8067e14ddb11529c0cceedb0ba21815e8d83d2e",
          "title": "RegFreeNet: A Registration-Free Network for CBCT-based 3D Dental Implant Planning",
          "year": 2026,
          "authors": [
            {
              "authorId": "2162654179",
              "name": "Xinquan Yang"
            },
            {
              "authorId": "2208154781",
              "name": "Xuguang Li"
            },
            {
              "authorId": "2395712433",
              "name": "Mianjie Zheng"
            },
            {
              "authorId": "2395772969",
              "name": "Xuefen Liu"
            },
            {
              "authorId": "2398631289",
              "name": "Kun Tang"
            },
            {
              "authorId": "2405640357",
              "name": "Kian Ming Lim"
            },
            {
              "authorId": "2307453786",
              "name": "He Meng"
            },
            {
              "authorId": "2283413134",
              "name": "Jianfeng Ren"
            },
            {
              "authorId": "2261094842",
              "name": "Linlin Shen"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "c41f1ff816e1284347d448388976991b5770efdb",
          "title": "U-Harmony: Enhancing Joint Training for Segmentation Models with Universal Harmonization",
          "year": 2026,
          "authors": [
            {
              "authorId": "2406115466",
              "name": "Weiwei Ma"
            },
            {
              "authorId": "2321487156",
              "name": "Xiaobing Yu"
            },
            {
              "authorId": "2292019619",
              "name": "Peijie Qiu"
            },
            {
              "authorId": "2405910923",
              "name": "Jin Yang"
            },
            {
              "authorId": "2405888409",
              "name": "Pan Xiao"
            },
            {
              "authorId": "2405977611",
              "name": "Xiaoqi Zhao"
            },
            {
              "authorId": "2266804945",
              "name": "Xiaofeng Liu"
            },
            {
              "authorId": "3388392",
              "name": "T. Miyazaki"
            },
            {
              "authorId": "1740235",
              "name": "S. Omachi"
            },
            {
              "authorId": "2359560921",
              "name": "Yongsong Huang"
            }
          ]
        }
      },
      {
        "contexts": [
          "These models represent diverse architectural paradigms and complexity levels, ranging from lightweight networks to computationally intensive frameworks: 3D U-Net [41], DeepLabV3 [42], 3D UX-Net [43], RepUX-Net [44], UNETR [2], SwinUNETR [46], Slim-UNETR [45], nnFormer [47], and VSmTrans [48].",
          "Image segmentation has become the primary technique to achieve this goal, often acting as the foundational step for quantitative evaluations of anatomical structures [2], [3]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "a9a52b18472a7d527f781b3efadc7210a6f73690",
          "title": "Partial Decoder Attention Network with Contour-weighted Loss Function for Data-Imbalance Medical Image Segmentation",
          "year": 2026,
          "authors": [
            {
              "authorId": "2325195705",
              "name": "Zhengyong Huang"
            },
            {
              "authorId": null,
              "name": "Ning Jiang"
            },
            {
              "authorId": "2406067849",
              "name": "Xingwen Sun"
            },
            {
              "authorId": "2405955922",
              "name": "Lihua Zhang"
            },
            {
              "authorId": "2406108296",
              "name": "Peng Chen"
            },
            {
              "authorId": "2405887438",
              "name": "Jens Domke"
            },
            {
              "authorId": "2325156178",
              "name": "Yao Sui"
            }
          ]
        }
      },
      {
        "contexts": [
          "The introduction of transformers motivated a shift towards attention-based architectures to overcome the limited receptive fields of CNNs and capture long-range dependencies [12,11,26]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "51033ed69e7373e576af0a321eda7c4b9ad8b9d8",
          "title": "Decoder-Free Supervoxel GNN for Accurate Brain-Tumor Localization in Multi-modal MRI",
          "year": 2026,
          "authors": [
            {
              "authorId": "2325160278",
              "name": "Andrea Protani"
            },
            {
              "authorId": "2384105165",
              "name": "Marc Molina Van De Bosch"
            },
            {
              "authorId": "2325155176",
              "name": "Lorenzo Giusti"
            },
            {
              "authorId": "2406022743",
              "name": "Heloisa Barbosa Da Silva"
            },
            {
              "authorId": "2364385236",
              "name": "Paolo Cacace"
            },
            {
              "authorId": "2297185968",
              "name": "Albert Sund Aillet"
            },
            {
              "authorId": "2405754196",
              "name": "Friedhelm Hummel"
            },
            {
              "authorId": "2151202766",
              "name": "Luigi Serio"
            }
          ]
        }
      },
      {
        "contexts": [
          "Early work adapted contrastive learning and masked image modeling to medical data, and more recent efforts have scaled these ideas into \"foundation\" or \"generalist\" models spanning multiple modalities and tasks [15-17]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "bea8a3ff665e563f7726a83010c7a03793c4ea17",
          "title": "A Generalist Foundation Model for Total-body PET/CT Enables Diagnostic Reporting and System-wide Metabolic Profiling",
          "year": 2026,
          "authors": [
            {
              "authorId": "2408105279",
              "name": "Wei Chen"
            },
            {
              "authorId": "2405835370",
              "name": "Liang Wu"
            },
            {
              "authorId": "2405892043",
              "name": "Shuyi Lu"
            },
            {
              "authorId": "2296392841",
              "name": "Yuanyuan Sun"
            },
            {
              "authorId": "2370942952",
              "name": "Wenkai Bi"
            },
            {
              "authorId": "153605485",
              "name": "Zilong Yuan"
            },
            {
              "authorId": "46968461",
              "name": "Yaoyao He"
            },
            {
              "authorId": "2405825564",
              "name": "Feng Wang"
            },
            {
              "authorId": "2392860839",
              "name": "Junchi Ma"
            },
            {
              "authorId": "2405744918",
              "name": "Shuyong Liu"
            },
            {
              "authorId": "2407161717",
              "name": "Zhaoping Cheng"
            },
            {
              "authorId": "2405826877",
              "name": "Xiaoyan Hu"
            },
            {
              "authorId": null,
              "name": "Jianfeng Qiu"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "0f761d8f7f9275ded6a7b2fe8093a505df7a6820",
          "title": "UroFusion-X: a unified multimodal deep learning framework for robust diagnosis, subtyping, and prognosis of urological cancers",
          "year": 2026,
          "authors": [
            {
              "authorId": "2319026011",
              "name": "Yingming Xiao"
            },
            {
              "authorId": "2305161934",
              "name": "Shengke Yang"
            },
            {
              "authorId": "2408476857",
              "name": "Mingjing He"
            },
            {
              "authorId": "2291318977",
              "name": "Li Chen"
            },
            {
              "authorId": "2363408016",
              "name": "Yi Wu"
            },
            {
              "authorId": "2291441292",
              "name": "Lei Zhong"
            }
          ]
        }
      },
      {
        "contexts": [
          "Numerous studies [10, 68, 81, 82, 86, 94, 95] have explored the use of 3D CNNs for biomedical volumetric segmentation [88, 96, 97]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "4c7e8b6846c2f9d3dd6f70335fbaed954cf82028",
          "title": "Deep learning-based neurodevelopmental assessment in preterm infants",
          "year": 2026,
          "authors": [
            {
              "authorId": "2407625917",
              "name": "Lexin Ren"
            },
            {
              "authorId": "2405823079",
              "name": "Jiamiao Lu"
            },
            {
              "authorId": "2304710738",
              "name": "Weichuan Zhang"
            },
            {
              "authorId": "2405825813",
              "name": "Benqing Wu"
            },
            {
              "authorId": "2405827686",
              "name": "Tuo Wang"
            },
            {
              "authorId": "2407640786",
              "name": "Yi Liao"
            },
            {
              "authorId": "2369139228",
              "name": "Jiapan Guo"
            },
            {
              "authorId": "2268635619",
              "name": "Changming Sun"
            },
            {
              "authorId": "2363681010",
              "name": "Liang Guo"
            }
          ]
        }
      },
      {
        "contexts": [
          "More recently, Transformer based designs improve global context modeling for medical segmentation, including hybrid and fully Transformer architectures [8,19,18,83,90,72]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "baaa11aa20de9d6db363faa137045c12114e9faa",
          "title": "Medical SAM3: A Foundation Model for Universal Prompt-Driven Medical Image Segmentation",
          "year": 2026,
          "authors": [
            {
              "authorId": "2405660541",
              "name": "Chongcong Jiang"
            },
            {
              "authorId": "2397491809",
              "name": "Tianxingjian Ding"
            },
            {
              "authorId": "2394125323",
              "name": "Chuhan Song"
            },
            {
              "authorId": "2405432036",
              "name": "Jiachen Tu"
            },
            {
              "authorId": "2321486817",
              "name": "Ziyang Yan"
            },
            {
              "authorId": "2405850000",
              "name": "Yihua Shao"
            },
            {
              "authorId": "2405478663",
              "name": "Zhenyi Wang"
            },
            {
              "authorId": "2405888296",
              "name": "Yuzhang Shang"
            },
            {
              "authorId": "2392678661",
              "name": "Tianyu Han"
            },
            {
              "authorId": "2405717314",
              "name": "Yu Tian"
            }
          ]
        }
      },
      {
        "contexts": [
          "Hybrid designs that combine convolutional and transformer components, such as UNet Transformer (UNETR) (83) and Swin UNETR (84), have proven to be more reliable."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "bd79187679ff7ea1cb41410f89c9586de8006f62",
          "title": "Artificial intelligence in aortic CT angiography: current applications and future perspectives",
          "year": 2026,
          "authors": [
            {
              "authorId": "2382818887",
              "name": "Jingkai Xu"
            },
            {
              "authorId": "2392677540",
              "name": "Jinjin Liu"
            },
            {
              "authorId": "2382104660",
              "name": "Guoquan Cao"
            }
          ]
        }
      },
      {
        "contexts": [
          "Block 2 (UNETR Block) 57 , inspired by transformer-based designs, enables effective global context modeling and precise spatial reconstruction through transposed convolution operations to recover spatial resolution effectively, enabling accurate boundary reconstruction of small and low-contrast…",
          "Block 2 (UNETR Block) 57 , inspired by transformer-based designs, enables effective global context modeling and precise spatial reconstruction through transposed convolution operations to recover spatial resolution effectively, enabling accurate boundary reconstruction of small and low-contrast thyroid nodules from ultrasound imaging.",
          "Block 1 (Residual Block) 56 introduces lightweight skip connections that promote stable gradient flow and faster convergence with minimal computational overhead, making it well suited for ultrasound imaging where training stability is critical.",
          "Block 3 (UnetRes Block) 58 combines residual learning with convolutional feature extraction, providing a favorable trade-off between contextual awareness and computational efficiency for delineating irregular thyroid nodules in ultrasound imaging.",
          "The results reveal that the effectiveness of individual blocks varies across datasets — for instance, Block 3 achieved superior results on the DDTI dataset, whereas Block 4 performed better on the TN3K dataset.",
          "Block 4 (Modified ASPP Block) 59 employs atrous convolutions to capture multi-scale contextual information without significantly increasing the number of parameters, which is beneficial for detecting nodules of varying sizes and echogenic patterns from ultrasound imaging."
        ],
        "intents": [],
        "isInfluential": true,
        "citingPaper": {
          "paperId": "a7edda11a3f1f29dd8bf2a411f0d954722588b2e",
          "title": "EvoThy-Net: an evolutionary encoder-decoder network for thyroid nodule segmentation in ultrasound imaging",
          "year": 2026,
          "authors": [
            {
              "authorId": "2404455217",
              "name": "Naga Sujini Ganne"
            },
            {
              "authorId": "66630853",
              "name": "Sivadi Balakrishna"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "7f88e8b9e960e31aedaf6091efc3b2a244ca8e3c",
          "title": "ReCo-KD: Region- and Context-Aware Knowledge Distillation for Efficient 3D Medical Image Segmentation",
          "year": 2026,
          "authors": [
            {
              "authorId": "2404317861",
              "name": "Qizhen Lan"
            },
            {
              "authorId": "2282441599",
              "name": "Yu-Chun Hsu"
            },
            {
              "authorId": "2405051760",
              "name": "Nida Saddaf Khan"
            },
            {
              "authorId": "2370150164",
              "name": "Xiaoqian Jiang"
            }
          ]
        }
      },
      {
        "contexts": [
          "To overcome the limited receptive fields of CNNs, Transformer-based models such as UNETR [34], nnFormer [25], and hybrid architectures like 3DUX-Net [37] were introduced to capture global dependencies."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "c29c3401d9a263e7b3d9f537e21b01f7c961dd0e",
          "title": "Anatomy Aware Cascade Network: Bridging Epistemic Uncertainty and Geometric Manifold for 3D Tooth Segmentation",
          "year": 2026,
          "authors": [
            {
              "authorId": "2342072752",
              "name": "Bing Yu"
            },
            {
              "authorId": "2404035442",
              "name": "Liu Shi"
            },
            {
              "authorId": "2404023923",
              "name": "Haitao Wang"
            },
            {
              "authorId": "2403991530",
              "name": "Deran Qi"
            },
            {
              "authorId": "2407683994",
              "name": "Xiang Cai"
            },
            {
              "authorId": "2405386217",
              "name": "Wei Zhong"
            },
            {
              "authorId": "2277685588",
              "name": "Qiegen Liu"
            }
          ]
        }
      },
      {
        "contexts": [
          "MRI scans often show considerable variation in intensity across scanners and acquisition settings, so a stable normalization step—commonly z-score normalization—is usually needed to keep the data comparable [26]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "ef9428cf5007d716b82bbda4c5cbb81716312ff7",
          "title": "Multi-modal MRI-Based Alzheimer's Disease Diagnosis with Transformer-based Image Synthesis and Transfer Learning",
          "year": 2026,
          "authors": [
            {
              "authorId": null,
              "name": "Jason Qiu"
            }
          ]
        }
      },
      {
        "contexts": [
          "Meanwhile, the emergence of vision transformers (ViTs) [7], [29] has introduced token-based modeling to medical imaging [12], [13], [30], [31], enabling long-range dependency learning."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "6c9ae3753100360184571be2bf95619e22582baa",
          "title": "TokenSeg: Efficient 3D Medical Image Segmentation via Hierarchical Visual Token Compression",
          "year": 2026,
          "authors": [
            {
              "authorId": "2403575193",
              "name": "Sen Zeng"
            },
            {
              "authorId": "2353624468",
              "name": "Hongyun Zhou"
            },
            {
              "authorId": "2403648285",
              "name": "Zheng Zhu"
            },
            {
              "authorId": "2403205599",
              "name": "Yang Liu"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "7cfaa822218a798ab2934efc25e03ba0c3ec71aa",
          "title": "A Self-Supervised Foundation Model for Robust and Generalizable Representation Learning in STED Microscopy",
          "year": 2026,
          "authors": [
            {
              "authorId": "1403746389",
              "name": "Anthony Bilodeau"
            },
            {
              "authorId": "2310616589",
              "name": "Frédéric Beaupré"
            },
            {
              "authorId": "2294338047",
              "name": "Julia Chabbert"
            },
            {
              "authorId": "2403578384",
              "name": "Kamylle Thériault"
            },
            {
              "authorId": "2078543220",
              "name": "Andréa Deschênes"
            },
            {
              "authorId": "2295673049",
              "name": "Jean-Michel Bellavance"
            },
            {
              "authorId": "2365618900",
              "name": "Koraly Lessard"
            },
            {
              "authorId": "1995475499",
              "name": "Renaud Bernatchez"
            },
            {
              "authorId": "2247482148",
              "name": "Paul De Koninck"
            },
            {
              "authorId": "2080941173",
              "name": "Christian Gagné"
            },
            {
              "authorId": "1398620408",
              "name": "Flavie Lavoie-Cardinal"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "9d9952a1c76bfb237a312433049ba39f78e9279f",
          "title": "A hybrid Transformer-CNN framework for uncertainty-guided semi-supervised multiclass eye disease classification with enhanced interpretability.",
          "year": 2026,
          "authors": [
            {
              "authorId": "2343541126",
              "name": "Muhammad Hammad Malik"
            },
            {
              "authorId": "2338925124",
              "name": "Zishuo Wan"
            },
            {
              "authorId": "46300632",
              "name": "Yingying Ren"
            },
            {
              "authorId": "2338888983",
              "name": "Da-Wei Ding"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "9ec04f1c74e7e7541b1bd10b4a3502f59b5eb3cb",
          "title": "From Preoperative CT to Postmastoidectomy Mesh Construction: Mastoidectomy Shape Prediction for Cochlear Implant Surgery",
          "year": 2026,
          "authors": [
            {
              "authorId": "2314883232",
              "name": "Yike Zhang"
            },
            {
              "authorId": "2178393581",
              "name": "Eduardo Davalos"
            },
            {
              "authorId": "2290917963",
              "name": "Dingjie Su"
            },
            {
              "authorId": "1734758447",
              "name": "Ange Lou"
            },
            {
              "authorId": "2290018526",
              "name": "Jack H. Noble"
            }
          ]
        }
      },
      {
        "contexts": [
          "…side, language technologies provide transferable evidence that bottlenecked interlingua encodings and attention-guided summarization can capture long-range dependencies without sacrificing interpretability—an idea parallel to transformer blocks that model global CT context over 3D patches 21–23 ."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "08eb03356d89e5383735320763d59f87a5021d1f",
          "title": "AI driven hybrid convolutional and transformer based deep learning architecture for precise lung nodule classification",
          "year": 2026,
          "authors": [
            {
              "authorId": "2403171700",
              "name": "R. Y. Abdullah"
            },
            {
              "authorId": "2403165463",
              "name": "C. Venkatesan"
            },
            {
              "authorId": "2384343992",
              "name": "E. Naresh"
            },
            {
              "authorId": "2403571463",
              "name": "B. P. P. Kumar"
            }
          ]
        }
      },
      {
        "contexts": [
          "Transformer-based methods (TransUNet [10], UN-ETR [11], Swin-UNETR [17]) achieve global context through self-attention but require 60-105M parameters and O ( n 2 ) complexity, making them impractical for high-resolution images and small datasets (N   1000).",
          "Transformers provide global interactions via self-attention [10, 11] but incur O ( n 2 ) complexity, impractical for high-resolution clinical images and small datasets."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "92ff85514c89fa06de96ababb64cca2125faaea7",
          "title": "S2M-Net: Spectral-Spatial Mixing for Medical Image Segmentation with Morphology-Aware Adaptive Loss",
          "year": 2026,
          "authors": [
            {
              "authorId": "2355864816",
              "name": "Md. Sanaullah Chowdhury Lameya Sabrin"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "a15eb582cde79ac24e21f60605533d6bb933dd71",
          "title": "KANSeg: An efficient medical image segmentation model based on Kolmogorov-Arnold networks for multi-organ segmentation",
          "year": 2026,
          "authors": [
            {
              "authorId": "2262454034",
              "name": "Junan Zhu"
            },
            {
              "authorId": "2342106945",
              "name": "Zhizhe Tang"
            },
            {
              "authorId": "2374304576",
              "name": "Zheng Liang"
            },
            {
              "authorId": "2262480050",
              "name": "Ping Ma"
            },
            {
              "authorId": "2308680687",
              "name": "Chuanjian Wang"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "0a2c10fc5ea92ea7d2b869969e40f2a7f7ea38c5",
          "title": "Advancing brain tumor segmentation in smart healthcare via T1-generated virtual multimodal MRI fusion",
          "year": 2026,
          "authors": [
            {
              "authorId": "2256921061",
              "name": "Youjian Zhang"
            },
            {
              "authorId": "2292996313",
              "name": "Jie Wang"
            },
            {
              "authorId": "2325523795",
              "name": "Xinquan Yang"
            },
            {
              "authorId": "2325569751",
              "name": "Xinyuan Zhang"
            },
            {
              "authorId": "2117597940",
              "name": "Abudoukeyoumujiang Abulizi"
            },
            {
              "authorId": "2393492982",
              "name": "Hong Jiang"
            },
            {
              "authorId": "2257386975",
              "name": "Guanqun Zhou"
            },
            {
              "authorId": "2393873345",
              "name": "Haiming Liao"
            },
            {
              "authorId": "2316905774",
              "name": "Gang Yu"
            },
            {
              "authorId": "2256939091",
              "name": "Zhicheng Zhang"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "6faf976302d26764d026b67cd91ea587d25a92a2",
          "title": "SAM-guided prompt learning for Multiple Sclerosis lesion segmentation",
          "year": 2026,
          "authors": [
            {
              "authorId": "1985894550",
              "name": "Federica Proietto Salanitri"
            },
            {
              "authorId": "1986291097",
              "name": "Giovanni Bellitto"
            },
            {
              "authorId": "2279911935",
              "name": "Salvatore Calcagno"
            },
            {
              "authorId": "2066164432",
              "name": "Ulas Bagci"
            },
            {
              "authorId": "2120814918",
              "name": "C. Spampinato"
            },
            {
              "authorId": "2335898946",
              "name": "Manuela Pennisi"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "ed3ddfdc966160a2b02cc570ac225b569461c7f3",
          "title": "A multi-scale and multi-transform-based lightweight U-Net model for Brain Tumor Segmentation with enhanced spatial information",
          "year": 2026,
          "authors": [
            {
              "authorId": "2346530712",
              "name": "Rama Rani"
            },
            {
              "authorId": "2238606810",
              "name": "Chandan Singh"
            },
            {
              "authorId": "2015697",
              "name": "S. K. Ranade"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "5d165a6ea5c70baf2fd50a6ff6f90a918f6137f6",
          "title": "UDV-Net: A hybrid CNN and transformer vein segmentation network with vascular prior and spatial awareness",
          "year": 2026,
          "authors": [
            {
              "authorId": "2316752579",
              "name": "Bowei Shen"
            },
            {
              "authorId": "87072105",
              "name": "Xiaoquan Huang"
            },
            {
              "authorId": "2402448576",
              "name": "Yuli Li"
            },
            {
              "authorId": "2402423698",
              "name": "Xinghuan Li"
            },
            {
              "authorId": "2279959039",
              "name": "Lili Ma"
            },
            {
              "authorId": "2316784413",
              "name": "Yonghong Shi"
            },
            {
              "authorId": "2305585625",
              "name": "Shiyao Chen"
            }
          ]
        }
      },
      {
        "contexts": [
          "UNETR [23] employs the Vision Transformer (ViT) [11] as its encoder to learn global information in a single-scale sequence.",
          "To reduce the sequence length, meth-ods based on Transformer architectures, such as UNETR [23], directly down-sample the 3D input with a resolution of However, this approach limits the ability to encode multi-scale features, which are essential for predicting segmentation results via the decoder.",
          "…also compare our SegMamba-V2 against four most famous and state-of-the-art transformer-based 3D medical image segmentation methods, which are UNETR [23], SwinUNETR [25], SwinUNETR-V2 [34], and nnFormer [24] These methods utilize Vision Transformer [11] or SwinTransformer [26] as encoders to learn…"
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "224b0ee9e7b947d2d8b1cf6cf9f086291ddfe803",
          "title": "SegMamba-V2: Long-Range Sequential Modeling Mamba for General 3-D Medical Image Segmentation",
          "year": 2026,
          "authors": [
            {
              "authorId": "153107262",
              "name": "Zhaohu Xing"
            },
            {
              "authorId": "2280882260",
              "name": "Tian Ye"
            },
            {
              "authorId": "2237943083",
              "name": "Yijun Yang"
            },
            {
              "authorId": "2052543334",
              "name": "D. Cai"
            },
            {
              "authorId": "2222027370",
              "name": "Baowen Gai"
            },
            {
              "authorId": "2348959539",
              "name": "Xiao-Jian Wu"
            },
            {
              "authorId": "2266219248",
              "name": "Feng Gao"
            },
            {
              "authorId": "2348189926",
              "name": "Lei Zhu"
            }
          ]
        }
      },
      {
        "contexts": [
          "To validate the effectiveness of our proposed model, we compared it with several state-of-the-art (SOTA) methods, including CNN-based [53], [59], [62], Transformer-based [54], [55], [56], [57], Mamba-based [61] architectures, as well as PET/CT multi-modal segmentation approaches [63].",
          "It can be observed that models such as 3D U-Net [53], Swin UNet [54], UNETR [55], and Swin UNETR [57] show suboptimal performance, often exhibiting signiﬁcant over-segmentation or under-segmentation across multiple datasets."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "1dd87c8e0bf6b8f497ad7847284d2ed2e9acdad7",
          "title": "An Automatic 3D PET Tumor Segmentation Framework Assisted by Geodesic Sequences",
          "year": 2026,
          "authors": [
            {
              "authorId": "2238107545",
              "name": "Lin Yang"
            },
            {
              "authorId": "2273970810",
              "name": "D. Shao"
            },
            {
              "authorId": "10672282",
              "name": "Chuanli Cheng"
            },
            {
              "authorId": "2307239713",
              "name": "Chao Zou"
            },
            {
              "authorId": "2024788161",
              "name": "Zhenxing Huang"
            },
            {
              "authorId": "2257132069",
              "name": "Hairong Zheng"
            },
            {
              "authorId": "2285364651",
              "name": "Dong Liang"
            },
            {
              "authorId": "2259980021",
              "name": "Zhi-Feng Pang"
            },
            {
              "authorId": "2303888365",
              "name": "Xue-Cheng Tai"
            },
            {
              "authorId": "2310391020",
              "name": "Zhanli Hu"
            }
          ]
        }
      },
      {
        "contexts": [
          "CNN-based models such as U-Net are good at extraction local features and poor at long-range dependencies, whereas Transformer-based models such as ViT and UNETR [39] are good at long-range dependencies and poor at spatial details and require large computational resources."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "f97224e600d7f2f1ae3c40001ed7c635831dfbfd",
          "title": "Global-local feature fusion in MRI brain tumor segmentation via enhanced U-Net-ViT architecture and adaptive contrast preprocessing",
          "year": 2026,
          "authors": [
            {
              "authorId": "2405469710",
              "name": "Xinxin Sun"
            },
            {
              "authorId": "11016472",
              "name": "U. Bhatti"
            },
            {
              "authorId": "2403922201",
              "name": "Junfeng Zhang"
            },
            {
              "authorId": "2407518945",
              "name": "Yu Zhang"
            },
            {
              "authorId": "2278894589",
              "name": "Yonis Gulzar"
            },
            {
              "authorId": "2275317655",
              "name": "Muhammad Aamir"
            },
            {
              "authorId": "2327234605",
              "name": "Hayitov Abdulla Nurmatovich"
            },
            {
              "authorId": "2372733491",
              "name": "Khudoynazarov Egambergan Madraximovich"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "c9ea67e5651daad4e7dcceeab6f9205c5371ad03",
          "title": "AttCo: Attention-based co-Learning fusion of deep feature representation for medical image segmentation using multimodality",
          "year": 2026,
          "authors": [
            {
              "authorId": "2005136354",
              "name": "Duy Dao"
            },
            {
              "authorId": "2300411671",
              "name": "Hyung-Jeong Yang"
            },
            {
              "authorId": "2189509689",
              "name": "Sooyoung Kim"
            },
            {
              "authorId": "2289784328",
              "name": "Sae-Ryung Kang"
            }
          ]
        }
      }
    ]
  },
  {
    "paperId": "8e33914d6051dd031a5e096962b9398fc1d16067",
    "title": "Vision Transformers for Dense Prediction",
    "year": 2021,
    "abstract": "We introduce dense prediction transformers, an architecture that leverages vision transformers in place of convolutional networks as a backbone for dense prediction tasks. We assemble tokens from various stages of the vision transformer into image-like representations at various resolutions and progressively combine them into full-resolution predictions using a convolutional decoder. The transformer backbone processes representations at a constant and relatively high resolution and has a global receptive field at every stage. These properties allow the dense prediction transformer to provide finer-grained and more globally coherent predictions when compared to fully-convolutional networks. Our experiments show that this architecture yields substantial improvements on dense prediction tasks, especially when a large amount of training data is available. For monocular depth estimation, we observe an improvement of up to 28% in relative performance when compared to a state-of-the-art fully-convolutional network. When applied to semantic segmentation, dense prediction transformers set a new state of the art on ADE20K with 49.02% mIoU. We further show that the architecture can be fine-tuned on smaller datasets such as NYUv2, KITTI, and Pascal Context where it also sets the new state of the art. Our models are available at https://github.com/intel-isl/DPT.",
    "url": "https://www.semanticscholar.org/paper/8e33914d6051dd031a5e096962b9398fc1d16067",
    "venue": "IEEE International Conference on Computer Vision",
    "publicationDate": "2021-03-24",
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2103.13413",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2103.13413, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "2774325",
        "name": "René Ranftl",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1651204675",
        "name": "Alexey Bochkovskiy",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "145231047",
        "name": "V. Koltun",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 2377,
    "referenceCount": 60,
    "influentialCitationCount": 310,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "externalIds": {
      "DBLP": "journals/corr/abs-2103-13413",
      "ArXiv": "2103.13413",
      "DOI": "10.1109/ICCV48922.2021.01196",
      "CorpusId": 232352612
    },
    "journal": {
      "name": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)",
      "volume": null,
      "pages": "12159-12168"
    },
    "tldr": "D dense prediction transformers, an architecture that leverages vision transformers in place of convolutional networks as a backbone for dense prediction tasks, can be fine-tuned on smaller datasets such as NYUv2, KITTI, and Pascal Context where it also sets the new state of the art.",
    "citations": [
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "dee6194dd40594ad1f36f094246ddb6f7489d4d6",
          "title": "BioSalNet: Biologically inspired saliency prediction",
          "year": 2026,
          "authors": [
            {
              "authorId": "2400059928",
              "name": "Fazhan Yang"
            },
            {
              "authorId": "2401922634",
              "name": "Jiansheng Qian"
            },
            {
              "authorId": "9238030",
              "name": "Xingge Guo"
            },
            {
              "authorId": "2399934315",
              "name": "Song Liang"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "ad63337011bdf3d1260b118cd649f4a3a6985872",
          "title": "Multi-stream interaction network with cross-modal contrast distillation for co-salient object detection",
          "year": 2026,
          "authors": [
            {
              "authorId": "2328615308",
              "name": "Wujie Zhou"
            },
            {
              "authorId": "2266709292",
              "name": "Bingying Wang"
            },
            {
              "authorId": "2289817774",
              "name": "Xiena Dong"
            },
            {
              "authorId": "2237076234",
              "name": "Caie Xu"
            },
            {
              "authorId": "2233609840",
              "name": "Fangfang Qiang"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "8f6e5f0f4da00a92e736e516477da4c4c67f33b3",
          "title": "Cross-backbone pixel consistency and layer-wise attention fusion for weakly supervised semantic segmentation",
          "year": 2026,
          "authors": [
            {
              "authorId": "2403123361",
              "name": "Mengya Liu"
            },
            {
              "authorId": "2296233296",
              "name": "Lei Zhu"
            },
            {
              "authorId": "2403463691",
              "name": "Jiahui Cheng"
            }
          ]
        }
      },
      {
        "contexts": [
          "Monocular Depth Estimation: Recent monocular depth meth-ods increasingly adopt transformer backbones and large-scale training to produce more coherent predictions, e.g., DPT [14] and Depth Anything [15]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "3e8f5882d371bdb06d4d078ee1113c51f69dd3a3",
          "title": "JustDepth: Real-Time Radar-Camera Depth Estimation With Single-Scan LiDAR Supervision",
          "year": 2026,
          "authors": [
            {
              "authorId": "2351811949",
              "name": "Wooyung Yun"
            },
            {
              "authorId": "2405860703",
              "name": "Dongwook Kim"
            },
            {
              "authorId": "2237948662",
              "name": "Soomok Lee"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "9bfd73fc9bfdfc1dd37c38e3dab582fd3632fe14",
          "title": "Object depth measurement based on monocular vision and point transformation for unmanned aerial vehicles",
          "year": 2026,
          "authors": [
            {
              "authorId": null,
              "name": "Peiran Zhang"
            },
            {
              "authorId": "2258544596",
              "name": "Fuqiang Zhou"
            },
            {
              "authorId": "2278375577",
              "name": "Zhipeng Song"
            },
            {
              "authorId": null,
              "name": "Wentao Guo"
            },
            {
              "authorId": "2394029725",
              "name": "Donghang Xie"
            }
          ]
        }
      },
      {
        "contexts": [
          "While monocular depth estimation has achieved strong accuracy and robustness through multi-scale feature aggregation and geometric priors [ 14 , 15], its integration into NeRF-based rendering pipelines has been relatively limited [16, 17 ]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "d38ddc7916a93f45a0f92b13546cb6e9765c891b",
          "title": "Enhancing neural radiance fields with geometry-aware transformers and depth fusion",
          "year": 2026,
          "authors": [
            {
              "authorId": null,
              "name": "Mingqiang Xu"
            },
            {
              "authorId": null,
              "name": "Zhengyao Bai"
            },
            {
              "authorId": null,
              "name": "Chenghao Cao"
            },
            {
              "authorId": "2354064337",
              "name": "Zenan Xu"
            },
            {
              "authorId": null,
              "name": "Han Ma"
            },
            {
              "authorId": "2353995581",
              "name": "Tao Lin"
            },
            {
              "authorId": "2354065492",
              "name": "Qiqin Huang"
            }
          ]
        }
      },
      {
        "contexts": [
          "The DPT [44] decoder is also trained with a high learning rate, while the ViT encoder is trained with a low learning rate η lo ."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "50d89317e91ee03063d0485ff53731ef988ddb5f",
          "title": "Depth Completion in Unseen Field Robotics Environments Using Extremely Sparse Depth Measurements",
          "year": 2026,
          "authors": [
            {
              "authorId": "2323735394",
              "name": "Marco Job"
            },
            {
              "authorId": "2709732",
              "name": "Thomas Stastny"
            },
            {
              "authorId": "2322501765",
              "name": "Eleni Kelasidi"
            },
            {
              "authorId": "2258712869",
              "name": "R. Siegwart"
            },
            {
              "authorId": "32545941",
              "name": "Michael Pantic"
            }
          ]
        }
      },
      {
        "contexts": [
          "DPT [53] shows that a ViT encoder [54] with a lightweight convolutional decoder yields fine-grained and globally consistent outputs for depth and segmentation, and it generalizes well across datasets.",
          "The DPT [53] decoder D DPT transforms this representation into multi-scale features F DPT = D DPT ( F enc ) ."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "e58a85cd9a38a870e3726c205fbea4c6d6ab3500",
          "title": "From Frames to Sequences: Temporally Consistent Human-Centric Dense Prediction",
          "year": 2026,
          "authors": [
            {
              "authorId": "40129763",
              "name": "Xingyu Miao"
            },
            {
              "authorId": "2354385245",
              "name": "Junting Dong"
            },
            {
              "authorId": "2408460392",
              "name": "Qin Zhao"
            },
            {
              "authorId": "2408442259",
              "name": "Yuhang Yang"
            },
            {
              "authorId": null,
              "name": "Junhao Chen"
            },
            {
              "authorId": "2408499438",
              "name": "Yang Long"
            }
          ]
        }
      },
      {
        "contexts": [
          "These outputs are upsampled to full resolution using a DPT head Ranftl et al. (2021) and further processed with our surface continuity prior and forced alpha-blending techniques to produce the final standard Gaussian attributes.",
          "The feasibility of such models arises from the observation that, even with sparse-view conditions, image features extracted by modern backbones (e.g., ViTs Ranftl et al. (2021); Zhang et al. (2022); Wang et al. (2024a)) retain sufficient local geometric cues for direct 3D reasoning.",
          "To enhance depth quality, we employ a hierarchical cascade structure Gu et al. (2020), refining the predicted depth to Ws , which is subsequently upsampled to full resolution using a DPT head Ranftl et al. (2021)."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "a323c5c91a2d865dbc67ba4333f4a8876a6c765b",
          "title": "SurfSplat: Conquering Feedforward 2D Gaussian Splatting with Surface Continuity Priors",
          "year": 2026,
          "authors": [
            {
              "authorId": null,
              "name": "Bing He"
            },
            {
              "authorId": "2408509175",
              "name": "Jingnan Gao"
            },
            {
              "authorId": "2293940941",
              "name": "Yunuo Chen"
            },
            {
              "authorId": "2372553120",
              "name": "Ning Cao"
            },
            {
              "authorId": "2373991449",
              "name": "Gang Chen"
            },
            {
              "authorId": null,
              "name": "Zhengxue Cheng"
            },
            {
              "authorId": null,
              "name": "Li Song"
            },
            {
              "authorId": "2247834984",
              "name": "Wenjun Zhang"
            }
          ]
        }
      },
      {
        "contexts": [
          "This fused representation is then decoded by two lightweight DPT heads [84] to predict global ( P s,g ) and local ( P s,ℓ ) coordinates along with their confidences."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "87e1debf8d24f63c6381526ec2d8cbeba83b06cc",
          "title": "Distill3R: A Pipeline for Democratizing 3D Foundation Models on Commodity Hardware",
          "year": 2026,
          "authors": [
            {
              "authorId": "2408457705",
              "name": "Brandon Leblanc"
            },
            {
              "authorId": "2268759976",
              "name": "Charalambos Poullis"
            }
          ]
        }
      },
      {
        "contexts": [
          "Specifically, we employ the DPT 2D model [14] for high-quality depth estimation and DINOv2 [15] for semantic feature extraction, enabling robust cross-view consistency."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "196fff42103b043d52bded02e47adeb428f38ef5",
          "title": "PSGS: Text-driven Panorama Sliding Scene Generation via Gaussian Splatting",
          "year": 2026,
          "authors": [
            {
              "authorId": null,
              "name": "Xin Zhang"
            },
            {
              "authorId": null,
              "name": "Shen Chen"
            },
            {
              "authorId": "2313686121",
              "name": "Jiale Zhou"
            },
            {
              "authorId": null,
              "name": "Lei Li"
            }
          ]
        }
      },
      {
        "contexts": [
          "Monocular depth was then predicted per animal using DPT [22] or Depth Anything [27], and relative depth was converted to metric distance using camera-specific reference frames containing a human at known distances up to 15 m."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "6aa4bbf0242cd6ca0124975d4064991e69c0f6f8",
          "title": "Deep in the Jungle: Towards Automating Chimpanzee Population Estimation",
          "year": 2026,
          "authors": [
            {
              "authorId": "2408225302",
              "name": "Tom Raynes"
            },
            {
              "authorId": "2034020316",
              "name": "Otto Brookes"
            },
            {
              "authorId": "2048073762",
              "name": "T. Haucke"
            },
            {
              "authorId": "2408225036",
              "name": "Lukas Bosch"
            },
            {
              "authorId": "12352595",
              "name": "A. Crunchant"
            },
            {
              "authorId": "2408224932",
              "name": "Hjalmar Kuhl"
            },
            {
              "authorId": "2352947052",
              "name": "Sara Beery"
            },
            {
              "authorId": "2280906446",
              "name": "M. Mirmehdi"
            },
            {
              "authorId": "1717278",
              "name": "T. Burghardt"
            }
          ]
        }
      },
      {
        "contexts": [
          "To balance efficiency and effectiveness, we adopt smaller 𝑟 𝑘 values in deeper layers, aligning with standard practices in PEFT[23][24][25].",
          "Techniques like Side Tuning[23, 24, 25], Prompt(P)-Tuning[26, 27, 28] and LoRA[29, 30] are representative."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "1eaa69adbb2da54627d5b37f48dff503be75d1d7",
          "title": "PEFT-MuTS: A Multivariate Parameter-Efficient Fine-Tuning Framework for Remaining Useful Life Prediction based on Cross-domain Time Series Representation Model",
          "year": 2026,
          "authors": [
            {
              "authorId": "2280498165",
              "name": "En Fu"
            },
            {
              "authorId": "2338278626",
              "name": "Yanyan Hu"
            },
            {
              "authorId": "2408347693",
              "name": "Changhua Hu"
            },
            {
              "authorId": "31020918",
              "name": "Zengwang Jin"
            },
            {
              "authorId": "2280449814",
              "name": "Kaixiang Peng"
            }
          ]
        }
      },
      {
        "contexts": [
          "…scale-and-shift-invariant objectives: MegaDepth [57] highlighted the benefits of diverse training data, MiDaS [82] established mixing heterogeneous datasets with scale-and-shift-invariant losses, and transformer-based architectures brought global context beyond CNN locality constraints [23, 81].",
          "Excluding the benefits of data scaling, we evaluate the effectiveness of our ViT-encoder–DPT-head skip-connection design by comparing it with a classic U-Net–style skip-connection architecture [81]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "2f54188660704785f0fa68d5af6169190b7a5181",
          "title": "MetricAnything: Scaling Metric Depth Pretraining with Noisy Heterogeneous Sources",
          "year": 2026,
          "authors": [
            {
              "authorId": null,
              "name": "Baorui Ma"
            },
            {
              "authorId": "2308734940",
              "name": "Jiahui Yang"
            },
            {
              "authorId": "2291139460",
              "name": "Donglin Di"
            },
            {
              "authorId": "2407525032",
              "name": "Xuancheng Zhang"
            },
            {
              "authorId": "2330273901",
              "name": "Jianxun Cui"
            },
            {
              "authorId": "2319131793",
              "name": "Hao Li"
            },
            {
              "authorId": "2377774457",
              "name": "Yan Xie"
            },
            {
              "authorId": "2311343221",
              "name": "Wei Chen"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "d8a1a715a375700447b469daae669a38c7609569",
          "title": "SCFI-ESeg: spatial and content feature integration for efficient semantic segmentation",
          "year": 2026,
          "authors": [
            {
              "authorId": "2361698323",
              "name": "Ning Li"
            },
            {
              "authorId": "2243369567",
              "name": "Xudong Zhang"
            },
            {
              "authorId": "2269698303",
              "name": "Gaochao Yang"
            },
            {
              "authorId": "2244167190",
              "name": "Bo Li"
            },
            {
              "authorId": "2346533760",
              "name": "Baohua Yuan"
            }
          ]
        }
      },
      {
        "contexts": [
          "Li et al. [19] utilize the CLIP text encoder to produce embeddings of descriptive labels, in parallel with a DPT-based image encoder [29] to generate dense per-pixel visual features."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "a10136103c0c371aad27ab7fab3a5aefc37a3b34",
          "title": "AerOSeg++: Scale-Aware and Texture-Guided Open-Vocabulary Segmentation with SAM Features for Remote Sensing Images",
          "year": 2026,
          "authors": [
            {
              "authorId": "2299337663",
              "name": "Saikat Dutta"
            },
            {
              "authorId": "2349847062",
              "name": "Akhil Vasim"
            },
            {
              "authorId": "2231657706",
              "name": "H. Rezatofighi"
            },
            {
              "authorId": "2322968474",
              "name": "Biplab Banerjee"
            }
          ]
        }
      },
      {
        "contexts": [
          "By fusing Radar and image features through a transformer-based cross-modal reassembly mechanism, 15 RCDPT explicitly supervises depth prediction and improves the quality of intermediate depth features."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "c445aee0c98276b114a40db8300c760f68d77718",
          "title": "Instance-Guided Radar Depth Estimation for 3D Object Detection",
          "year": 2026,
          "authors": [
            {
              "authorId": "5042019",
              "name": "Chen-Chou Lo"
            },
            {
              "authorId": "2280572511",
              "name": "Patrick Vandewalle"
            }
          ]
        }
      },
      {
        "contexts": [
          "We compare the heads we used previously with other common heads in the literature: Mask-RCNN (He et al., 2017) vs. Faster-RCNN (Ren et al., 2016) for object detection, ViTPose (Xu et al., 2022) vs. RTMPose (Jiang et al., 2023) for pose estimation, Linear vs. Seg-menter (Strudel et al., 2021) for segmentation, and Linear vs. DPT (Ranftl et al., 2021) for depth estimation.",
          "…Mask-RCNN (He et al., 2017) vs. Faster-RCNN (Ren et al., 2016) for object detection, ViTPose (Xu et al., 2022) vs. RTMPose (Jiang et al., 2023) for pose estimation, Linear vs. Seg-menter (Strudel et al., 2021) for segmentation, and Linear vs. DPT (Ranftl et al., 2021) for depth estimation.",
          "For semantic segmentation and depth estimation, we replace the linear head with a DPT-based decoder (Ranftl et al., 2021) to better exploit the additional modeling capacity afforded by end-to-end optimization."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "4599628118c0c484c7b2e45a95e561d205d64e45",
          "title": "On the Role of Depth in Surgical Vision Foundation Models: An Empirical Study of RGB-D Pre-training",
          "year": 2026,
          "authors": [
            {
              "authorId": "2281684900",
              "name": "John J. Han"
            },
            {
              "authorId": null,
              "name": "Adam Schmidt"
            },
            {
              "authorId": "34902211",
              "name": "Muhammad Abdullah Jamal"
            },
            {
              "authorId": "40992763",
              "name": "C. Nwoye"
            },
            {
              "authorId": "2406994432",
              "name": "Anita Rau"
            },
            {
              "authorId": null,
              "name": "Jie Ying Wu"
            },
            {
              "authorId": "2146343",
              "name": "O. Mohareri"
            }
          ]
        }
      },
      {
        "contexts": [
          "DPT [8] was a landmark study, proving that a Vision Transformer (ViT) backbone, with its inherent ability to capture global context, could outperform state-of-the-art CNNs."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "23091ee070d729fae62b6380b0c8f5f3c2cb04cb",
          "title": "SPACE-CLIP: Spatial Perception via Adaptive CLIP Embeddings for Monocular Depth Estimation",
          "year": 2026,
          "authors": [
            {
              "authorId": "2373270934",
              "name": "Taewan Cho"
            },
            {
              "authorId": "2382321313",
              "name": "Taeryang Kim"
            },
            {
              "authorId": "2406837898",
              "name": "Andrew Jaeyong Choi"
            }
          ]
        }
      },
      {
        "contexts": [
          "A lightweight camera head outputs camera parameters, while a DPT head [14] produces dense per-frame predictions (depth/point maps and tracking features)."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "3641629c4e8255991073dae24289645bc0476850",
          "title": "Learning with Geometric Priors in U-Net Variants for Polyp Segmentation",
          "year": 2026,
          "authors": [
            {
              "authorId": "2343636728",
              "name": "Fabian Vazquez"
            },
            {
              "authorId": "2343637746",
              "name": "J. A. Nuñez"
            },
            {
              "authorId": "2359630998",
              "name": "Diego Adame"
            },
            {
              "authorId": "2391759406",
              "name": "Alissen Moreno"
            },
            {
              "authorId": "2406844005",
              "name": "Augustin Zhan"
            },
            {
              "authorId": "2307026237",
              "name": "Huimin Li"
            },
            {
              "authorId": "2380146395",
              "name": "Jinghao Yang"
            },
            {
              "authorId": "2360408460",
              "name": "Haoteng Tang"
            },
            {
              "authorId": "2286841824",
              "name": "Bin Fu"
            },
            {
              "authorId": "2336301511",
              "name": "Pengfei Gu"
            }
          ]
        }
      },
      {
        "contexts": [
          "The depth head follows the DPT [34] decoder, a convolutional module that fuses features from four intermediate layers of the visual encoder: { 3 , 6 , 9 , 12 } for ViT-B/16 and { 6 , 12 , 18 , 24 } for ViT-L/16 .",
          "We attach a lightweight DPT head [34] to the shared visual encoder, which aggregates multi-layer features { z ( l ) i } l ∈S from selected ViT layers to predict a dense depth map ˆ d i ∈ R H × W ."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "748ec65e07b2cb1004f36ff1fa15ffc4953aca50",
          "title": "Revisiting Multi-Task Visual Representation Learning",
          "year": 2026,
          "authors": [
            {
              "authorId": "2273356419",
              "name": "Shangzhe Di"
            },
            {
              "authorId": "2405741143",
              "name": "Zhonghua Zhai"
            },
            {
              "authorId": null,
              "name": "Weidi Xie"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "ce4f8e897bc96fd9bdf1f6dd3f9fff83471708e5",
          "title": "Equivariant Learning for Unsupervised Image Dehazing",
          "year": 2026,
          "authors": [
            {
              "authorId": "2408391777",
              "name": "Zhang Wen"
            },
            {
              "authorId": "2405877412",
              "name": "Jiangwei Xie"
            },
            {
              "authorId": "2405751462",
              "name": "Dongdong Chen"
            }
          ]
        }
      },
      {
        "contexts": [
          "We design heads based on the DPT decoder (Ranftl, Bochkovskiy, and Koltun 2021) to predict Gaussian parameters."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "8da5729aae1a43a6d06539766a34cfff29a758e3",
          "title": "One-Shot Refiner: Boosting Feed-forward Novel View Synthesis via One-Step Diffusion",
          "year": 2026,
          "authors": [
            {
              "authorId": "2288392123",
              "name": "Yitong Dong"
            },
            {
              "authorId": "2332946984",
              "name": "Qi Zhang"
            },
            {
              "authorId": "2407417799",
              "name": "Minchao Jiang"
            },
            {
              "authorId": "2407457640",
              "name": "Zhiqiang Wu"
            },
            {
              "authorId": "2332538716",
              "name": "Qingnan Fan"
            },
            {
              "authorId": "2347732548",
              "name": "Ying Feng"
            },
            {
              "authorId": "2364084623",
              "name": "Huaqi Zhang"
            },
            {
              "authorId": "2238044333",
              "name": "Hujun Bao"
            },
            {
              "authorId": "2405761432",
              "name": "Guofeng Zhang"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "d30d410047711c28f296d9adcd38e448ee53198a",
          "title": "Enhancing dietary management and nutritional analysis through deep learning: a multi-model approach for food classification and volume estimation",
          "year": 2026,
          "authors": [
            {
              "authorId": "2366973595",
              "name": "N. Karthikeyan"
            },
            {
              "authorId": "2376981247",
              "name": "B. Natarajan"
            },
            {
              "authorId": "31177982",
              "name": "Soumya Ranjan Mahapatro"
            },
            {
              "authorId": "2405873653",
              "name": "D. M. Hussain"
            }
          ]
        }
      },
      {
        "contexts": [
          "Metric UNet++ [15] MAnet [16] PAN [17] SegFormer [18] FPN [19] DPT [20] UPerNet [21] MVI-based [ IV."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "f33f9ac3f691137398fbc878699deb7c9484c3b4",
          "title": "MANGO: A Global Single-Date Paired Dataset for Mangrove Segmentation",
          "year": 2026,
          "authors": [
            {
              "authorId": "2348739803",
              "name": "Junhyuk Heo"
            },
            {
              "authorId": "2407403454",
              "name": "Beomkyu Choi"
            },
            {
              "authorId": "2406856148",
              "name": "Hyunjin Shin"
            },
            {
              "authorId": "2348739774",
              "name": "Darongsae Kwon"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "87b99317616852b0beac623065d024efe6d6c063",
          "title": "VIRO: Robust and Efficient Neuro-Symbolic Reasoning with Verification for Referring Expression Comprehension",
          "year": 2026,
          "authors": [
            {
              "authorId": "2110832024",
              "name": "Hyejin Park"
            },
            {
              "authorId": "2406788626",
              "name": "Junhyuk Kwon"
            },
            {
              "authorId": "2238955191",
              "name": "Suha Kwak"
            },
            {
              "authorId": "2525635",
              "name": "Jungseul Ok"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "e31467730a9f27e7505506dfde48c18d1eec6a08",
          "title": "Toward Real-World High-Precision Image Matting and Segmentation",
          "year": 2026,
          "authors": [
            {
              "authorId": "2316591386",
              "name": "Haipeng Zhou"
            },
            {
              "authorId": "153107262",
              "name": "Zhaohu Xing"
            },
            {
              "authorId": "2274928570",
              "name": "Hongqiu Wang"
            },
            {
              "authorId": "2316590562",
              "name": "Jun Ma"
            },
            {
              "authorId": "2316588833",
              "name": "Ping Li"
            },
            {
              "authorId": "2367751280",
              "name": "Lei Zhu"
            }
          ]
        }
      },
      {
        "contexts": [
          "For depth estimation, we attach a dense prediction transformer (DPT) head [64] as in [15], and for camera pose estimation, we use a 3-layer MLP regressor as in [19]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "086fd2b59751899ce487348e1cbd6989cdce5207",
          "title": "SpaRRTa: A Synthetic Benchmark for Evaluating Spatial Intelligence in Visual Foundation Models",
          "year": 2026,
          "authors": [
            {
              "authorId": "2220753465",
              "name": "Turhan Can Kargin"
            },
            {
              "authorId": "2405809439",
              "name": "Wojciech Jasi'nski"
            },
            {
              "authorId": "2124213120",
              "name": "Adam Pardyl"
            },
            {
              "authorId": "2365037832",
              "name": "Bartosz Zieli'nski"
            },
            {
              "authorId": "2405807608",
              "name": "Marcin Przewike'zlikowski"
            }
          ]
        }
      },
      {
        "contexts": [
          "For the student model, we combine the DI-NOv2 (Oquab et al., 2023) pre-trained ViT-S backbone with the DPT (Ranftl et al., 2021) head as the architecture, as it has been shown to perform well in single-view depth estimation."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "f646e71da00969f3962feea89b4b61d821adfbde",
          "title": "studentSplat: Your Student Model Learns Single-view 3D Gaussian Splatting",
          "year": 2026,
          "authors": [
            {
              "authorId": "2406014680",
              "name": "Yimu Pan"
            },
            {
              "authorId": "2402726423",
              "name": "Hongda Mao"
            },
            {
              "authorId": "2402732904",
              "name": "Qingshuang Chen"
            },
            {
              "authorId": "2402879028",
              "name": "Yelin Kim"
            }
          ]
        }
      },
      {
        "contexts": [
          "Ranftl et al. 8 design a lightweight decoder with multi-level feature fusion to effectively adapt the representations from the DINO family for segmentation.",
          "To effectively use knowledge learned by DINOv3, we fuse multi-layer features from the DINOv3 backbone and pass them to a lightweight decoder to produce the final segmentation mask, inspired by the light-decoder design in Ranftl et al. 8 We use a pretrained DINOv3 Vision Transformer (ViT) as the encoder and keep all its weights frozen during training.",
          "…multi-layer features from the DINOv3 backbone and pass them to a lightweight decoder to produce the final segmentation mask, inspired by the light-decoder design in Ranftl et al. 8 We use a pretrained DINOv3 Vision Transformer (ViT) as the encoder and keep all its weights frozen during training."
        ],
        "intents": [],
        "isInfluential": true,
        "citingPaper": {
          "paperId": "995a33cb25e6864c5dc8de98860fac3eef9574fa",
          "title": "An effective interactive brain cytoarchitectonic parcellation framework using pretrained foundation model",
          "year": 2026,
          "authors": [
            {
              "authorId": "2329900019",
              "name": "Shiqi Zhang"
            },
            {
              "authorId": "2330168768",
              "name": "Fang Xu"
            },
            {
              "authorId": "2405993279",
              "name": "Pengcheng Zhou"
            }
          ]
        }
      },
      {
        "contexts": [
          "We address this by incorporating a geometric prior from a pretrained monocular depth estimation network, MiDaS [10], [11].",
          "To address this, we incorporate a geometric prior derived from pre-trained monocular depth estimation networks, such as MiDaS-DPT [10], [11] or Depth Anything [12].",
          "When ground truth depth is not available, monocular depth estimators such as MiDaS [10], [11] and Depth Anything [12], [13] can provide valuable, although scale and shift ambiguous, geometric cues.",
          "For the geometric regularization component, monocular depth priors were generated by fusing the outputs of MiDaS-DPT [10], [11] and Depth-Anything V2 [12], [13]."
        ],
        "intents": [],
        "isInfluential": true,
        "citingPaper": {
          "paperId": "43ef1611e648665cbf9c1e427f5446cd543071e1",
          "title": "TIDI-GS: Floater Suppression in 3D Gaussian Splatting for Enhanced Indoor Scene Fidelity",
          "year": 2026,
          "authors": [
            {
              "authorId": "2352074766",
              "name": "SooYeun Yang"
            },
            {
              "authorId": "2404470755",
              "name": "Cheyul Im"
            },
            {
              "authorId": "2332041549",
              "name": "Jee Won Lee"
            },
            {
              "authorId": "2322844765",
              "name": "J. Choi"
            }
          ]
        }
      },
      {
        "contexts": [
          "We apply two widely recognized metrics Absolute Mean Relative Error (AbsRel) [31] and δ 1 accuracy [32] for assessing quality of depth estimation."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "39fd59ddfe15fe7c994eeeb87fc356b172ca8807",
          "title": "NanoSD: Edge Efficient Foundation Model for Real Time Image Restoration",
          "year": 2026,
          "authors": [
            {
              "authorId": "3314220",
              "name": "Subhajit Sanyal"
            },
            {
              "authorId": "2292187468",
              "name": "S. Miriyala"
            },
            {
              "authorId": "2404630491",
              "name": "Akshay Janardan Bankar"
            },
            {
              "authorId": "2349873639",
              "name": "Sravanth Kodavanti"
            },
            {
              "authorId": "2404634656",
              "name": "Manjunath Arveti"
            },
            {
              "authorId": "1961219874",
              "name": "Abhishek Ameta"
            },
            {
              "authorId": "2404631782",
              "name": "Shreyas Pandith"
            },
            {
              "authorId": "9301610",
              "name": "Amit Satish Unde"
            }
          ]
        }
      },
      {
        "contexts": [
          "Monocular depth estimation (MDE) (Bian et al., 2021; Ranftl et al., 2021; Yin et al., 2022; Bhat et al., 2023; Godard et al., 2019; Yang et al., 2024; Li & Snavely, 2018) has demonstrated robust generalization across diverse scenes but lacks camera pose information and temporal consistency in…",
          "Monocular depth estimation (MDE) works (Bian et al., 2021; Ranftl et al., 2021; Yin et al., 2022; Bhat et al., 2023; Godard et al., 2019; Yang et al., 2024; Li & Snavely, 2018) estimate precise 3D information but fail to localize camera poses."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "1ced72584d5a25a6c7e71ca6777f7bfea15489f7",
          "title": "CogniMap3D: Cognitive 3D Mapping and Rapid Retrieval",
          "year": 2026,
          "authors": [
            {
              "authorId": "2349641513",
              "name": "Feiran Wang"
            },
            {
              "authorId": "2283269572",
              "name": "Junyi Wu"
            },
            {
              "authorId": "2194185492",
              "name": "Dawen Cai"
            },
            {
              "authorId": "2294676855",
              "name": "Yuan Hong"
            },
            {
              "authorId": "2404508293",
              "name": "Yan Yan"
            }
          ]
        }
      },
      {
        "contexts": [
          "These UV tokens are jointly processed with the UV aggregate map via a DPT-based [59, 60] decoder, producing a UV space feature map of size 384 × 384 × 256 .",
          "These features are subsequently decoded into dense UV coordinate maps through a trainable DPT head [59, 60]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "42351d7c328b36b28f7ef908fc9af9cddaa659c5",
          "title": "UIKA: Fast Universal Head Avatar from Pose-Free Images",
          "year": 2026,
          "authors": [
            {
              "authorId": "2332105414",
              "name": "Zijian Wu"
            },
            {
              "authorId": "2404109319",
              "name": "Boyao Zhou"
            },
            {
              "authorId": "2404402883",
              "name": "Liangxiao Hu"
            },
            {
              "authorId": "2273469505",
              "name": "Hongyu Liu"
            },
            {
              "authorId": "2402750448",
              "name": "Yuan Sun"
            },
            {
              "authorId": "2403522919",
              "name": "Xuan Wang"
            },
            {
              "authorId": "2305233872",
              "name": "Xun Cao"
            },
            {
              "authorId": "2405725400",
              "name": "Yujun Shen"
            },
            {
              "authorId": "2293271316",
              "name": "Hao Zhu"
            }
          ]
        }
      },
      {
        "contexts": [
          "Following the methodology proposed in DPT [20], this module establishes task-specific multi-scale feature pyramids for both depth estimation and semantic segmentation."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "fec1ceb98a49e5aef484ba5d6e75e274bc9b4749",
          "title": "Research on a Curvature-Enhanced and Synergistic Attention-Based Multi-Task Perception Method for Transparent Objects",
          "year": 2026,
          "authors": [
            {
              "authorId": "2406876416",
              "name": "Jiajin Han"
            },
            {
              "authorId": "2356487204",
              "name": "Sanpeng Deng"
            },
            {
              "authorId": "2394994997",
              "name": "Yuming Qi"
            },
            {
              "authorId": "2395367827",
              "name": "Xiumin Shi"
            }
          ]
        }
      },
      {
        "contexts": [
          "During training, we incorporate an auxiliary pseudo-depth prediction task supervised by DPT-Hybrid [35], improving cross-domain generalisation.",
          "The prediction of a pseudo-depth map is supervised using the prediction by the DPT-Hybrid [35].",
          "Qualitative analysis of pseudo-depth estimation models In the paper, we compare two pseudo-depth estimation models: DPT-Hybrid [35] and DepthAnything [43]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "02bb69a3d4f3a2cdf5504b202006cc65f43a3f56",
          "title": "Towards Egocentric 3D Hand Pose Estimation in Unseen Domains",
          "year": 2026,
          "authors": [
            {
              "authorId": "2164040130",
              "name": "Wiktor Mucha"
            },
            {
              "authorId": "2316428659",
              "name": "Michael Wray"
            },
            {
              "authorId": "2273828909",
              "name": "Martin Kampel"
            }
          ]
        }
      },
      {
        "contexts": [
          "These methods converge towards using transformer-based architectures [60]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "7bf43ce5146fd6664fcbc7ad51d838fd1f851c73",
          "title": "Pixel-Perfect Visual Geometry Estimation",
          "year": 2026,
          "authors": [
            {
              "authorId": "2158317969",
              "name": "Gangwei Xu"
            },
            {
              "authorId": "2348809613",
              "name": "Haotong Lin"
            },
            {
              "authorId": "2349213656",
              "name": "Hongcheng Luo"
            },
            {
              "authorId": "2348805049",
              "name": "Haiyang Sun"
            },
            {
              "authorId": "2352907431",
              "name": "Bing Wang"
            },
            {
              "authorId": "2366092163",
              "name": "Guang Chen"
            },
            {
              "authorId": "2353991700",
              "name": "Sida Peng"
            },
            {
              "authorId": "2367554550",
              "name": "Hangjun Ye"
            },
            {
              "authorId": "2403733117",
              "name": "Xin Yang"
            }
          ]
        }
      },
      {
        "contexts": [
          "Conversely image-based depth foundation models [4–6, 34, 45, 46, 48, 49, 63, 64] have demonstrated significant potential for generating dense depth predictions from single images but still lack behind when delivering metric depth accuracy."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "6433070292f7f21d722fd5dc3900fcabe524fdc0",
          "title": "UniLiPs: Unified LiDAR Pseudo-Labeling with Geometry-Grounded Dynamic Scene Decomposition",
          "year": 2026,
          "authors": [
            {
              "authorId": "2376266226",
              "name": "Filippo Ghilotti"
            },
            {
              "authorId": "2302400989",
              "name": "Samuel Brucker"
            },
            {
              "authorId": "2042764014",
              "name": "Nahku Saidy"
            },
            {
              "authorId": "2402084204",
              "name": "Matteo Matteucci"
            },
            {
              "authorId": "51506568",
              "name": "Mario Bijelic"
            },
            {
              "authorId": "2297188932",
              "name": "Felix Heide"
            }
          ]
        }
      },
      {
        "contexts": [
          "Discriminative methods [23, 34, 35, 39, 40] achieve strong zero-shot generalization through large-scale multi-dataset pretraining, while generative approaches [13, 15] finetune pretrained diffusion models to leverage rich visual priors to synthesize depth maps.",
          "DPT Head The standard DPT head [23] is a lightweight decoder that converts multi-scale transformer tokens into dense predictions through a series of reassembling, upsam-pling, and fusion stages.",
          "Single-view depth estimation has been explored through both discriminative and generative approaches [15, 23, 39]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "4e0f255e00c9710cb693f80a09c07f50323f491c",
          "title": "MoE3D: A Mixture-of-Experts Module for 3D Reconstruction",
          "year": 2026,
          "authors": [
            {
              "authorId": "2403306031",
              "name": "Zichen Wang"
            },
            {
              "authorId": "2403192109",
              "name": "Ang Cao"
            },
            {
              "authorId": "2403916611",
              "name": "Liam J. Wang"
            },
            {
              "authorId": "2338670884",
              "name": "Jeong Joon Park"
            }
          ]
        }
      },
      {
        "contexts": [
          "Representative networks such as DPT [22] and AdaBins [23] utilize global attention mechanisms to extract semantic context, achieving impressive results on standard benchmarks like KITTI and NYU Depth V2."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "e838f5809e6405caff29fb1c2f4cd721b3aea678",
          "title": "Vision-based feature detection and geometric modeling for precise inter-UAV distance measurement",
          "year": 2026,
          "authors": [
            {
              "authorId": "2303489544",
              "name": "Guixin Yuan"
            },
            {
              "authorId": "2404622995",
              "name": "Xiaohu Zhang"
            },
            {
              "authorId": "2309299841",
              "name": "Zhenyao Zhao"
            },
            {
              "authorId": "2405069096",
              "name": "Jiannan Cui"
            },
            {
              "authorId": "2258292663",
              "name": "Chujun Li"
            },
            {
              "authorId": "2155641998",
              "name": "Xiangpeng Xu"
            }
          ]
        }
      },
      {
        "contexts": [
          "The geometry tokens V are then decoded by several individual DPT heads [42] ( Token-to-Latent Adapter."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "e865bfb0d0d7bb8a9a3471581ee75a84d23c4ad0",
          "title": "Gen3R: 3D Scene Generation Meets Feed-Forward Reconstruction",
          "year": 2026,
          "authors": [
            {
              "authorId": "2239058733",
              "name": "Jiaxin Huang"
            },
            {
              "authorId": "2295681231",
              "name": "Yuanbo Yang"
            },
            {
              "authorId": "2307555993",
              "name": "Bangbang Yang"
            },
            {
              "authorId": "2261359525",
              "name": "Lin Ma"
            },
            {
              "authorId": "2261275961",
              "name": "Yuewen Ma"
            },
            {
              "authorId": "2371017317",
              "name": "Yiyi Liao"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "83f1263bc767f515b1581361387ddb6613e60d92",
          "title": "Finer monocular depth estimation with long range in various driving lighting environments",
          "year": 2026,
          "authors": [
            {
              "authorId": "2262086356",
              "name": "Yan Liu"
            },
            {
              "authorId": "2408345438",
              "name": "Mingyu Yan"
            },
            {
              "authorId": "2262085599",
              "name": "Yanqiu Xiao"
            },
            {
              "authorId": "2059217740",
              "name": "Guangzhen Cui"
            },
            {
              "authorId": "2258674819",
              "name": "Li Han"
            }
          ]
        }
      },
      {
        "contexts": [
          "…the inherent problems of the CNN backbone, the performance of Zero-Shot Monocular Depth Estimation was further improved by using the vision transformer architecture, such as DPT (Ranftl et al., 2021), Omnidata (Eftekhar et al., 2021), Depthformer (Li et al., 2023) and Zoepdeth (Bhat et al., 2023).",
          "For the class token, we keep the same processing as DPT (Ranftl et al., 2021), concatenate it with the spatial token, and then fuse it through the learnable projection.",
          "DPT (Ranftl et al., 2021) utilizes the ViT (Dosovitskiy et al., 2020) backbone network to generate high-resolution features, thereby achieving finer-grained representation and improving prediction accuracy.",
          "As shown in Table 6, we compared the inference latency and throughput of the SDT and DPT decoders on the Jet-son Orin Nano (4GB) at two input resolutions.",
          "DPT (Ranftl et al., 2021) has demonstrated impressive performance in various dense prediction tasks and is currently used as the decoder in mainstream models.",
          "Furthermore, we compared the efficiency performance of SDT and DPT on edge devices.",
          "A key difference between SDT and DPT (Ranftl et al., 2021) is the order of feature reassembly."
        ],
        "intents": [],
        "isInfluential": true,
        "citingPaper": {
          "paperId": "d54a5c5eff2fb9e8e3519a9360db018efd0600d4",
          "title": "AnyDepth: Depth Estimation Made Easy",
          "year": 2026,
          "authors": [
            {
              "authorId": "2391769570",
              "name": "Zeyu Ren"
            },
            {
              "authorId": "2396902821",
              "name": "Zeyu Zhang"
            },
            {
              "authorId": "2389489651",
              "name": "Wukai Li"
            },
            {
              "authorId": "2381200388",
              "name": "Qingxiang Liu"
            },
            {
              "authorId": "2291035187",
              "name": "Hao Tang"
            }
          ]
        }
      },
      {
        "contexts": [
          "VideoDepthAnything (VDA) builds upon DepthAny-thingV2 and the Dense Prediction Transformers (DPT) head [29], introducing temporal layers from AnimateD-iff [15].",
          "Similar to the the seminal MiDaS [30] with its Dense Prediction Transformers (DPT) head [29] for single image estimation, and Video Depth Anything (VDA) with its Spatial-Temporal Head (STH) for video estimation, we propose a novel efficient architecture that works for both worlds.",
          "We evaluate our proposed method on multiple benchmark datasets for video depth estimation, following the protocols used in previous works [6, 7, 29, 30].",
          "As with MiDaS [30] and DPT [29], we use an architecture similar to ViT [8] as the encoder, which consists of a series of transformer layers that process the input image as a sequence of patches.",
          "Both Reassemble and Fusion modules are similar to the corresponding modules from the original DPT paper [29]."
        ],
        "intents": [],
        "isInfluential": true,
        "citingPaper": {
          "paperId": "3bb091e7f830a4389cca1e9c04a3f06c66fc631c",
          "title": "StableDPT: Temporal Stable Monocular Video Depth Estimation",
          "year": 2026,
          "authors": [
            {
              "authorId": "2402893795",
              "name": "Ivan Sobko"
            },
            {
              "authorId": "1848930",
              "name": "Hayko Riemenschneider"
            },
            {
              "authorId": "2284792844",
              "name": "Markus Gross"
            },
            {
              "authorId": "2604867",
              "name": "Christopher Schroers"
            }
          ]
        }
      },
      {
        "contexts": [
          "To verify the effectiveness of our depth representation, we compare it with a baseline that predicts depth on discrete grids using a DPT decoder [29].",
          "With the development of deep learning, mainstream depth estimation methods [3, 18, 29, 41, 46, 48, 49] adopt regular 2D grids to represent depth maps, as this representation is naturally compatible with modern neural network architectures.",
          "Recent works [29, 41, 48, 49] adopt Vision Trans-former (ViT) backbones [7] with convolutional decoders to regress 2D discretized depth maps.",
          "Following [29], we design a reassemble block which extracts feature tokens from multiple ViT layers and projects them to different hidden dimensions."
        ],
        "intents": [],
        "isInfluential": true,
        "citingPaper": {
          "paperId": "9973870aae7767ae46fb7dd45f9c3304a5634f22",
          "title": "InfiniDepth: Arbitrary-Resolution and Fine-Grained Depth Estimation with Neural Implicit Fields",
          "year": 2026,
          "authors": [
            {
              "authorId": "2339988705",
              "name": "Hao Yu"
            },
            {
              "authorId": "2348809613",
              "name": "Haotong Lin"
            },
            {
              "authorId": "2402937070",
              "name": "Jiawei Wang"
            },
            {
              "authorId": "2334911417",
              "name": "Jiaxin Li"
            },
            {
              "authorId": "2326986535",
              "name": "Yida Wang"
            },
            {
              "authorId": "2403006223",
              "name": "Xueyang Zhang"
            },
            {
              "authorId": "2403210430",
              "name": "Yue Wang"
            },
            {
              "authorId": "2238207469",
              "name": "Xiaowei Zhou"
            },
            {
              "authorId": "2274775115",
              "name": "Ruizhen Hu"
            },
            {
              "authorId": "2353991700",
              "name": "Sida Peng"
            }
          ]
        }
      },
      {
        "contexts": [
          "Depth Anything is a foundation model for natural image depth estimation that employs DINOv2 [26] as the encoder and a Dense Prediction Transformer (DPT) [27] as the decoder."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "46dcf3c39fa48fd8feeda132b3925fcfb9bb04b8",
          "title": "Reinforcement Learning for Follow-the-Leader Robotic Endoscopic Navigation via Synthetic Data",
          "year": 2026,
          "authors": [
            {
              "authorId": "2361003121",
              "name": "Sicong Gao"
            },
            {
              "authorId": "2316982800",
              "name": "Chen Qian"
            },
            {
              "authorId": "2402898455",
              "name": "Laurence Xian"
            },
            {
              "authorId": "2281206132",
              "name": "Liao Wu"
            },
            {
              "authorId": "1783801",
              "name": "M. Pagnucco"
            },
            {
              "authorId": "2157995570",
              "name": "Yang Song"
            }
          ]
        }
      },
      {
        "contexts": [
          "To achieve zero-shot depth estimation, early attempts employ mixed training datasets to obtain a strong geometric prior of the scene [45, 46, 69].",
          "4b, our depth fixer has two main branches: a feature branch built upon DI-NOv2 [39] and DPT [46] to extract deep features and image semantics, and a pixel branch based on U-Net [48] to where L 1 denotes the ℓ 1 loss, and L α is an image matting loss from ViTMatte [71] to facilitate detail…",
          "Monocular depth estimation aims to infer scene geometry from a single image [7, 8, 20, 40, 46, 72, 73, 78], a fundamentally ill-posed problem due to the loss of depth cues during projection."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "14d7d39b7b48e418226410b8e9b1ab8f4e338ab4",
          "title": "Guardians of the Hair: Rescuing Soft Boundaries in Depth, Stereo, and Novel Views",
          "year": 2026,
          "authors": [
            {
              "authorId": "2313026102",
              "name": "Xiang Zhang"
            },
            {
              "authorId": "2223929962",
              "name": "Yang Zhang"
            },
            {
              "authorId": "2345924122",
              "name": "Lukas Mehl"
            },
            {
              "authorId": "2284792844",
              "name": "Markus Gross"
            },
            {
              "authorId": "2604867",
              "name": "Christopher Schroers"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "854ff35a36be1bffee6e2032f2bd9b6cfb3921ad",
          "title": "DeepFusion encoder for unsupervised monocular metric depth estimation",
          "year": 2026,
          "authors": [
            {
              "authorId": "2391923922",
              "name": "Zhiwei Huang"
            },
            {
              "authorId": "2294947677",
              "name": "Mohammed A. H. Ali"
            },
            {
              "authorId": "2331909455",
              "name": "Y. Nukman"
            },
            {
              "authorId": "2334588289",
              "name": "Shikai Zhang"
            },
            {
              "authorId": "2334508727",
              "name": "Hui Chen"
            },
            {
              "authorId": "2316309966",
              "name": "Mohammad Alkhedher"
            },
            {
              "authorId": "2364400982",
              "name": "Hai lu Xu"
            }
          ]
        }
      },
      {
        "contexts": [
          "In deep learning, prior work has investigated auxiliary sources such as saliency [37], [38], [39] and depth maps [40], [41], with saliency shown to correlate more strongly with detection performance [42]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "17bc33d8c023c25fb07fe6ee45eef9b041d9b28a",
          "title": "Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach",
          "year": 2026,
          "authors": [
            {
              "authorId": "2292332992",
              "name": "Matthias Bartolo"
            },
            {
              "authorId": "2389601261",
              "name": "Dylan Seychell"
            },
            {
              "authorId": "2310286022",
              "name": "Gabriel Hili"
            },
            {
              "authorId": "2285418191",
              "name": "Matthew Montebello"
            },
            {
              "authorId": "2292330759",
              "name": "C. J. Debono"
            },
            {
              "authorId": "1883584",
              "name": "Saviour Formosa"
            },
            {
              "authorId": "2402728730",
              "name": "Konstantinos Makantasis"
            }
          ]
        }
      },
      {
        "contexts": [
          "We evaluate two representative depth foundation models: DPT [16] and DepthAnything [3]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "457966bd012a0d2110a595744bcce3ebf8e81f9d",
          "title": "Language as Prior, Vision as Calibration: Metric Scale Recovery for Monocular Depth Estimation",
          "year": 2026,
          "authors": [
            {
              "authorId": "2402727715",
              "name": "Mingxing Zhan"
            },
            {
              "authorId": "2356886682",
              "name": "Li Zhang"
            },
            {
              "authorId": "2356573643",
              "name": "Beibei Wang"
            },
            {
              "authorId": "2259857569",
              "name": "Yingjie Wang"
            },
            {
              "authorId": "2402772366",
              "name": "Zenglin Shi"
            }
          ]
        }
      }
    ]
  },
  {
    "paperId": "739ceacfafb1c4eaa17509351b647c773270b3ae",
    "title": "An Empirical Study of Training Self-Supervised Vision Transformers",
    "year": 2021,
    "abstract": "This paper does not describe a novel method. Instead, it studies a straightforward, incremental, yet must-know baseline given the recent progress in computer vision: self-supervised learning for Vision Transformers (ViT). While the training recipes for standard convolutional networks have been highly mature and robust, the recipes for ViT are yet to be built, especially in the self-supervised scenarios where training becomes more challenging. In this work, we go back to basics and investigate the effects of several fundamental components for training self-supervised ViT. We observe that instability is a major issue that degrades accuracy, and it can be hidden by apparently good results. We reveal that these results are indeed partial failure, and they can be improved when training is made more stable. We benchmark ViT results in MoCo v3 and several other self-supervised frameworks, with ablations in various aspects. We discuss the currently positive evidence as well as challenges and open questions. We hope that this work will provide useful data points and experience for future research.",
    "url": "https://www.semanticscholar.org/paper/739ceacfafb1c4eaa17509351b647c773270b3ae",
    "venue": "IEEE International Conference on Computer Vision",
    "publicationDate": "2021-04-05",
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2104.02057",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2104.02057, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "39717886",
        "name": "Xinlei Chen",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1817030",
        "name": "Saining Xie",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2058350112",
        "name": "Kaiming He",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 2216,
    "referenceCount": 50,
    "influentialCitationCount": 296,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "externalIds": {
      "DBLP": "journals/corr/abs-2104-02057",
      "ArXiv": "2104.02057",
      "MAG": "3145450063",
      "DOI": "10.1109/ICCV48922.2021.00950",
      "CorpusId": 233024948
    },
    "journal": {
      "name": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)",
      "volume": null,
      "pages": "9620-9629"
    },
    "tldr": "This work investigates the effects of several fundamental components for training self-supervised ViT, and reveals that these results are indeed partial failure, and they can be improved when training is made more stable.",
    "citations": [
      {
        "contexts": [
          "We further complement Table 2 by providing alignment scores within the MAE (He et al., 2022) and MoCov3 (Chen et al., 2021) feature spaces."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "ce5f767274c577b30bc1b1461a9433dcd85b7a32",
          "title": "Test-Time Conditioning with Representation-Aligned Visual Features",
          "year": 2026,
          "authors": [
            {
              "authorId": "2323375448",
              "name": "Nicolas Sereyjol-Garros"
            },
            {
              "authorId": "2334568816",
              "name": "Ellington Kirby"
            },
            {
              "authorId": "2264977664",
              "name": "Victor Letzelter"
            },
            {
              "authorId": "2403986942",
              "name": "Victor Besnier"
            },
            {
              "authorId": "40313071",
              "name": "Nermin Samet"
            }
          ]
        }
      },
      {
        "contexts": [
          "First, we leverage language supervision as the primary recognition objective, whereas CAST relies on uni-modal discrimination (Chen et al., 2021) or fixed label supervision."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "634b0130356bf6854b96ba7c1953b2f6451657c7",
          "title": "Aligning Forest and Trees in Images and Long Captions for Visually Grounded Understanding",
          "year": 2026,
          "authors": [
            {
              "authorId": "2311114040",
              "name": "Byeongju Woo"
            },
            {
              "authorId": null,
              "name": "Zilin Wang"
            },
            {
              "authorId": "2076977818",
              "name": "Byeonghyun Pak"
            },
            {
              "authorId": "2299940987",
              "name": "Sangwoo Mo"
            },
            {
              "authorId": null,
              "name": "Stella X. Yu"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "adb186d899a0c8b250ff5bad5daf52a91ba3c9cd",
          "title": "Learning Sparse Visual Representations via Spatial-Semantic Factorization",
          "year": 2026,
          "authors": [
            {
              "authorId": null,
              "name": "Theodore Zhengde Zhao"
            },
            {
              "authorId": "39620434",
              "name": "Sid Kiblawi"
            },
            {
              "authorId": "2408440243",
              "name": "Jianwei Yang"
            },
            {
              "authorId": "2637252",
              "name": "N. Usuyama"
            },
            {
              "authorId": "73441526",
              "name": "Reuben Tan"
            },
            {
              "authorId": "2268318452",
              "name": "Noel C. F. Codella"
            },
            {
              "authorId": "2264107059",
              "name": "Tristan Naumann"
            },
            {
              "authorId": "2277607019",
              "name": "H. Poon"
            },
            {
              "authorId": null,
              "name": "Mu Wei"
            }
          ]
        }
      },
      {
        "contexts": [
          "…and ImageNet-1K, including strong supervised paradigms Sup-21K, Sup-21K/1K (Sup-21K fine-tuned on ImageNet-1K) (Ridnik et al., 2021; Dosovitskiy et al., 2020), and self-supervised paradigms iBOT-21K, iBOT-1K (Zhou et al., 2021), DINO-1K (Caron et al., 2021), and MoCo v3-1K (Chen et al., 2021)."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "33d59d9f16beb35bcf72de0b441a208cf0e2a50e",
          "title": "FlyPrompt: Brain-Inspired Random-Expanded Routing with Temporal-Ensemble Experts for General Continual Learning",
          "year": 2026,
          "authors": [
            {
              "authorId": "2282118599",
              "name": "Hongwei Yan"
            },
            {
              "authorId": "2345154312",
              "name": "Guanglong Sun"
            },
            {
              "authorId": null,
              "name": "Kanglei Zhou"
            },
            {
              "authorId": "2386910265",
              "name": "Qian Li"
            },
            {
              "authorId": "2344961589",
              "name": "Liyuan Wang"
            },
            {
              "authorId": "2294823338",
              "name": "Yi Zhong"
            }
          ]
        }
      },
      {
        "contexts": [
          "(2) SSL by instance-contrastive learning (e.g. MoCoV3 (Chen et al., 2021) or SimCLR (Chen et al., 2020)) pre-trains models to produce similar embeddings derived from positive pairs of inputs (e.g. different augmentations of the same sample) and dissimilar embed-dings for negatives (e.g. different…"
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "b02376ee2e7d3df8814fd57ff40307e868ef2c57",
          "title": "Self-Soupervision: Cooking Model Soups without Labels",
          "year": 2026,
          "authors": [
            {
              "authorId": "2063969144",
              "name": "A. Fuller"
            },
            {
              "authorId": null,
              "name": "James R. Green"
            },
            {
              "authorId": "2345183553",
              "name": "Evan Shelhamer"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "6e47e13f59a51219f783b8428424002a9598d1ea",
          "title": "Few-shot transformers for image classification using masked self-distillation and optimal tokens based global-local feature interactions",
          "year": 2026,
          "authors": [
            {
              "authorId": "2274598546",
              "name": "Binquan Li"
            },
            {
              "authorId": "2335163880",
              "name": "Xin Guo"
            },
            {
              "authorId": "2291532951",
              "name": "Lishuang Gong"
            }
          ]
        }
      },
      {
        "contexts": [
          "Other approaches include selectively freezing parameters in ViT models [7] and using spectral normalization to prevent attention entropy collapse [42], further stabilizing training."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "687e684c1b8cb8ffb03e2b3e0cb99eb5280ae54d",
          "title": "Dispelling the Curse of Singularities in Neural Network Optimizations",
          "year": 2026,
          "authors": [
            {
              "authorId": null,
              "name": "Hengjie Cao"
            },
            {
              "authorId": null,
              "name": "Mengyi Chen"
            },
            {
              "authorId": "2378723791",
              "name": "Yifeng Yang"
            },
            {
              "authorId": "2344865903",
              "name": "Fang Dong"
            },
            {
              "authorId": null,
              "name": "Ruijun Huang"
            },
            {
              "authorId": null,
              "name": "Anrui Chen"
            },
            {
              "authorId": "2344969660",
              "name": "Jixian Zhou"
            },
            {
              "authorId": null,
              "name": "Mingzhi Dong"
            },
            {
              "authorId": "2268830944",
              "name": "Yujiang Wang"
            },
            {
              "authorId": "2396534588",
              "name": "Dongsheng Li"
            },
            {
              "authorId": null,
              "name": "Wenyi Fang"
            },
            {
              "authorId": "2407527836",
              "name": "Yuanyi Lin"
            },
            {
              "authorId": null,
              "name": "Fan Wu"
            },
            {
              "authorId": "2283592632",
              "name": "Lili Shang"
            }
          ]
        }
      },
      {
        "contexts": [
          "Specifically, we use CAST, trained on ImageNet (Deng et al., 2009) with the MoCo-v3 ob-jective (Chen et al., 2021), a self-supervised learning by instance discrimination (Wu et al., 2018) that clusters visually similar images."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "44739f32da99f932f58e4d6a29db2e9ce32bac2a",
          "title": "SHED Light on Segmentation for Dense Prediction",
          "year": 2026,
          "authors": [
            {
              "authorId": "2392715699",
              "name": "Seung Hyun Lee"
            },
            {
              "authorId": "2299940987",
              "name": "Sangwoo Mo"
            },
            {
              "authorId": "2391343360",
              "name": "Stella X. Yu"
            }
          ]
        }
      },
      {
        "contexts": [
          "For alignment, we experimented with different pretrained models, including contrastive self-supervised approaches (MoCo V3 (Chen et al., 2021) and DINO (Caron et al., 2021)), a supervised method (ViT (Dosovitskiy et al., 2020)), and a copy detection model (SSCD (Pizzi et al., 2022))."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "87580fe3c4e25150f23229f12f98d248cf7b4157",
          "title": "Unsupervised Synthetic Image Attribution: Alignment and Disentanglement",
          "year": 2026,
          "authors": [
            {
              "authorId": "2343934388",
              "name": "Zongfang Liu"
            },
            {
              "authorId": "2155315836",
              "name": "Guan-Hong Chen"
            },
            {
              "authorId": null,
              "name": "Boyang Sun"
            },
            {
              "authorId": null,
              "name": "Tongliang Liu"
            },
            {
              "authorId": "2334876728",
              "name": "Kun Zhang"
            }
          ]
        }
      },
      {
        "contexts": [
          "DINOv3 (Sim´eoni et al., 2025) yields the highest P-AUPRO among all foundations, followed by other contrastive and hybrid encoders such as DINOv2 (Oquab et al., 2023), iBOT (Zhou et al., 2021), DINO (Caron et al., 2021), D-iGPT (Ren et al., 2023), and MoCov3 (Chen et al., 2021), and then by supervised DeiT (Touvron et al., 2021).",
          "…the highest P-AUPRO among all foundations, followed by other contrastive and hybrid encoders such as DINOv2 (Oquab et al., 2023), iBOT (Zhou et al., 2021), DINO (Caron et al., 2021), D-iGPT (Ren et al., 2023), and MoCov3 (Chen et al., 2021), and then by supervised DeiT (Touvron et al., 2021)."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "60a9150784bb7320d6dcb7a17c8a08661cd643ac",
          "title": "Is Training Necessary for Anomaly Detection?",
          "year": 2026,
          "authors": [
            {
              "authorId": null,
              "name": "Xingwu Zhang"
            },
            {
              "authorId": "2381361110",
              "name": "Guanxuan Li"
            },
            {
              "authorId": "2268495645",
              "name": "Paul Henderson"
            },
            {
              "authorId": "2240297844",
              "name": "Gerardo Aragon-Camarasa"
            },
            {
              "authorId": "2381048781",
              "name": "Zijun Long"
            }
          ]
        }
      },
      {
        "contexts": [
          "Table 6 presents the comprehensive CIL performance on the ViT/B-MoCoV3 backbone, which is pre-trained via self-supervised contrastive learning.",
          "This trend indicates a dimensionality expansion of the effective feature space, which is most dramatic in ViT/B-MoCoV3 .",
          "The average results on ViT/B-MoCoV3 (Chen et al., 2021) To evaluate robustness in a large-scale CIL scenario, we construct a 14-task cross-domain benchmark spanning seven diverse datasets: CIFAR-100, ImageNet-R, Cars-196, CUB-200, Caltech-101 (Fei-Fei et al., 2004), Flower-102 (Nils-back & Zisserman, 2008), and Food-101 (Bossard et al., 2014).",
          "The average results on ViT/B-MoCoV3 (Chen et al., 2021) To evaluate robustness in a large-scale CIL scenario, we construct a 14-task cross-domain benchmark spanning seven diverse datasets: CIFAR-100, ImageNet-R, Cars-196, CUB-200, Caltech-101 (Fei-Fei et al., 2004), Flower-102 (Nils-back &…",
          "It is worth noting that ViT/B-MoCoV3 has distinct behavior s: it suffers from severe performance collapse at high ranks, and the sharp “inverted-V” trends on fine-grained datasets ( Figure 12."
        ],
        "intents": [],
        "isInfluential": true,
        "citingPaper": {
          "paperId": "56c0130952fd45da35b182f1ed1962370b447734",
          "title": "Scalable Analytic Classifiers with Associative Drift Compensation for Class-Incremental Learning of Vision Transformers",
          "year": 2026,
          "authors": [
            {
              "authorId": "2281886437",
              "name": "Xuan Rao"
            },
            {
              "authorId": "2392263255",
              "name": "Mingming Ha"
            },
            {
              "authorId": null,
              "name": "Bo Zhao"
            },
            {
              "authorId": "2307019174",
              "name": "Derong Liu"
            },
            {
              "authorId": "1785004",
              "name": "C. Alippi"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "f8d3465b766269960925de0e160befa648b0098a",
          "title": "STORM: Slot-based Task-aware Object-centric Representation for robotic Manipulation",
          "year": 2026,
          "authors": [
            {
              "authorId": "2238243296",
              "name": "Alexandre Chapin"
            },
            {
              "authorId": "2183481965",
              "name": "Emmanuel Dellandr'ea"
            },
            {
              "authorId": "2407250707",
              "name": "Liming Chen"
            }
          ]
        }
      },
      {
        "contexts": [
          "The main paper reports results under the supervised ImageNet-21K (Sup-21K) [31] pre-training, while results under additional pre-training paradigms (iBOT [50], DINO [4], MoCo [5]) are deferred to the Appendix for completeness.",
          "These results highlight the effectiveness of continual prompt adaptation in overcoming the rigidity of static backbone tuning. methods under alternative pre-training paradigms, including iBOT [50], DINO [4], and MoCo [5]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "cbbbd45912a6ab8390eb511112d585744e013672",
          "title": "Is Parameter Isolation Better for Prompt-Based Continual Learning?",
          "year": 2026,
          "authors": [
            {
              "authorId": "2361615877",
              "name": "Jiangyang Li"
            },
            {
              "authorId": null,
              "name": "Chenhao Ding"
            },
            {
              "authorId": null,
              "name": "Songlin Dong"
            },
            {
              "authorId": "2311426698",
              "name": "Qiang Wang"
            },
            {
              "authorId": null,
              "name": "Jianchao Zhao"
            },
            {
              "authorId": "2261929724",
              "name": "Yuhang He"
            },
            {
              "authorId": null,
              "name": "Yihong Gong"
            }
          ]
        }
      },
      {
        "contexts": [
          "Early methods relied on hand-crafted heuristics for random initialization (Glorot et al., 2010; Chen et al., 2021)."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "81b99ca2344ae361518231474aee64d137f9ae5c",
          "title": "Self-Supervised Weight Templates for Scalable Vision Model Initialization",
          "year": 2026,
          "authors": [
            {
              "authorId": "2308169564",
              "name": "Yucheng Xie"
            },
            {
              "authorId": "2220294125",
              "name": "Fu Feng"
            },
            {
              "authorId": null,
              "name": "Ruixiao Shi"
            },
            {
              "authorId": "2382932866",
              "name": "Jing Wang"
            },
            {
              "authorId": "2240856453",
              "name": "Yong Rui"
            },
            {
              "authorId": "2273326407",
              "name": "Xin Geng"
            }
          ]
        }
      },
      {
        "contexts": [
          "Modern deep learning models increasingly rely on depth to improve performance (Kaplan et al., 2020; Chen et al., 2021; Liu et al., 2022; He et al., 2022), yet depth alone provides no structural guarantee that intermediate representations are geometrically valid approximations of the final output."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "f5fce752496df767eaf7ae034c2e0e1fb3b95e52",
          "title": "Scale-Consistent State-Space Dynamics via Fractal of Stationary Transformations",
          "year": 2026,
          "authors": [
            {
              "authorId": null,
              "name": "Geunhyeok Yu"
            },
            {
              "authorId": "2382972620",
              "name": "Hyoseok Hwang"
            }
          ]
        }
      },
      {
        "contexts": [
          "Feature-upsampling methods are a shortcut to create denser features from pre-trained, general-purpose visual backbones like DINO [4, 34, 42], MoCo [5, 6, 16], MAE [17], CLIP [37], and SigLip [45, 53]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "0cffd58f670a5f54b1098651c07170a627655fd5",
          "title": "UPLiFT: Efficient Pixel-Dense Feature Upsampling with Local Attenders",
          "year": 2026,
          "authors": [
            {
              "authorId": "27065386",
              "name": "Matthew Walmer"
            },
            {
              "authorId": "51896308",
              "name": "Saksham Suri"
            },
            {
              "authorId": "2367521667",
              "name": "Anirud Aggarwal"
            },
            {
              "authorId": "1781242",
              "name": "Abhinav Shrivastava"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "a78d54a93d3e6bae1bfe2970149fc78bf9f10e1a",
          "title": "StyleDecoupler: Generalizable Artistic Style Disentanglement",
          "year": 2026,
          "authors": [
            {
              "authorId": "2407765711",
              "name": "Zexi Jia"
            },
            {
              "authorId": "2406893803",
              "name": "Jinchao Zhang"
            },
            {
              "authorId": "2116575810",
              "name": "Jie Zhou"
            }
          ]
        }
      },
      {
        "contexts": [
          "Chen et al. (2021b) proposes a 2D variant of sinusoidal positional encoding of Vaswani (2017), sincos2d, which is found to improve the performance of the ImageNet-1k ViT-S/16 baseline (Beyer et al., 2022)."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "13b8b3813a059715ccab027280225cf47a8afc6b",
          "title": "ViT Registers and Fractal ViT",
          "year": 2026,
          "authors": [
            {
              "authorId": "2406042733",
              "name": "Jason Chuan-Chih Chou"
            },
            {
              "authorId": "2406117929",
              "name": "Abhinav Kumar"
            },
            {
              "authorId": "2325035385",
              "name": "Shivank Garg"
            }
          ]
        }
      },
      {
        "contexts": [
          "…followed by methods, mainly based on contrastive learning and clustering, which were able to compete with supervised learning [6, 9–11, 16–18, 20, 21, 35, 56, 75, 95, 108, 120]. iBOT [119] was able to outperform these with masked image modeling [1, 4, 39, 43], and recent entries in the DINO…"
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "c8aa3f0d1c1efb8cdb10460da8f78594263b3311",
          "title": "Implicit Neural Representation Facilitates Unified Universal Vision Encoding",
          "year": 2026,
          "authors": [
            {
              "authorId": "2397619280",
              "name": "Matthew Gwilliam"
            },
            {
              "authorId": "2405755952",
              "name": "Xiao Wang"
            },
            {
              "authorId": "2321897185",
              "name": "Xuefeng Hu"
            },
            {
              "authorId": "2322610678",
              "name": "Zhenheng Yang"
            }
          ]
        }
      },
      {
        "contexts": [
          "The models include iBOT-21K [78], iBOT-1K [78], DINO-1K [4] and MoCo-1K [7]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "8b98dfd17d06ce454f757825ccb666b1f1d4a755",
          "title": "Text-Prompted Prompt Generator with Uncertainty Regularization for Rehearsal-Free Class-Incremental Learning",
          "year": 2026,
          "authors": [
            {
              "authorId": "2276029875",
              "name": "Shaofan Wang"
            },
            {
              "authorId": "2232239438",
              "name": "Fuhao Wei"
            },
            {
              "authorId": "2390959032",
              "name": "Hong Ma"
            },
            {
              "authorId": "2306973391",
              "name": "Yanfeng Sun"
            },
            {
              "authorId": "2293456585",
              "name": "Baocai Yin"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "4807dc7723ed121313a60c8625763237c100fbc0",
          "title": "One Model, Many Behaviors: Training-Induced Effects on Out-of-Distribution Detection",
          "year": 2026,
          "authors": [
            {
              "authorId": "2290507515",
              "name": "Gerhard Krumpl"
            },
            {
              "authorId": "2405426186",
              "name": "Henning Avenhaus"
            },
            {
              "authorId": "2265515386",
              "name": "Horst Possegger"
            }
          ]
        }
      },
      {
        "contexts": [
          "We adhere to speci ﬁ ed partitioning protocols from respective publications for a fair performance comparison and follow publicly available code where applicable (Supplementary Tables 10, 11).",
          "More details on dataset composition and usage can be found in Methods section and Supplementary Tables 1 – 4.",
          "Each experiment was conducted with ﬁ ve random seeds, assessed by mean and standard deviation of the three overall metrics (Supplementary Tables 9, 10)."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "d0a53564350ac24d1bf7aeb1ace3022687f3e0d6",
          "title": "A unified time-frequency foundation model for sleep decoding",
          "year": 2026,
          "authors": [
            {
              "authorId": "2404583023",
              "name": "Weixuan Huang"
            },
            {
              "authorId": "2392623180",
              "name": "Yan Wang"
            },
            {
              "authorId": "2405045969",
              "name": "Hanrong Cheng"
            },
            {
              "authorId": "2315892597",
              "name": "Wei Xu"
            },
            {
              "authorId": "2375899321",
              "name": "Tingyue Li"
            },
            {
              "authorId": "2367308841",
              "name": "Xiuwen Wu"
            },
            {
              "authorId": "2233349764",
              "name": "Hui Xu"
            },
            {
              "authorId": "2322412802",
              "name": "Pan Liao"
            },
            {
              "authorId": "2241821670",
              "name": "Zaixu Cui"
            },
            {
              "authorId": "2262267181",
              "name": "Qihong Zou"
            },
            {
              "authorId": "2274195082",
              "name": "Jiahong Gao"
            }
          ]
        }
      },
      {
        "contexts": [
          "To improve training stability and increase sample diversity, we adopt the momentum contrastive learning framework of MoCo v3 (Chen, Xie, and He 2021) for pretraining."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "a19c1a4dbb964e8b169af3d86ee0173e3d0f275f",
          "title": "A Unified Shape-Aware Foundation Model for Time Series Classification",
          "year": 2026,
          "authors": [
            {
              "authorId": "2279763622",
              "name": "Zhen Liu"
            },
            {
              "authorId": "2204444421",
              "name": "Yucheng Wang"
            },
            {
              "authorId": "2360435684",
              "name": "Boyuan Li"
            },
            {
              "authorId": "2809051",
              "name": "Junhao Zheng"
            },
            {
              "authorId": "2086836960",
              "name": "Emadeldeen Eldele"
            },
            {
              "authorId": "2326071762",
              "name": "Min Wu"
            },
            {
              "authorId": "2153709283",
              "name": "Qianli Ma"
            }
          ]
        }
      },
      {
        "contexts": [
          "MoCo v3 [8], an incremental improvement over MoCo v1[24] and MoCo v2[7], investigated the instability of ViTs in self-supervised learning."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "92f3e9bf1eaea19be5c657911f4d25d42b7f1285",
          "title": "How to Build Robust, Scalable Models for GSV-Based Indicators in Neighborhood Research",
          "year": 2026,
          "authors": [
            {
              "authorId": "2312296469",
              "name": "Xiaoya Tang"
            },
            {
              "authorId": "2151775430",
              "name": "Xiaohe Yue"
            },
            {
              "authorId": "2132119812",
              "name": "Heran Mane"
            },
            {
              "authorId": "2248097724",
              "name": "Dapeng Li"
            },
            {
              "authorId": "2404275679",
              "name": "Quynh Nguyen"
            },
            {
              "authorId": "2191488732",
              "name": "Tolga Tasdizen"
            }
          ]
        }
      },
      {
        "contexts": [
          "Here, we trained a MoCo v3 algorithm 27 (methods)."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "8d903aacfbf3a6dfc2c450d8cb95280e66a5321f",
          "title": "sCellST predicts single-cell gene expression from H& E images",
          "year": 2026,
          "authors": [
            {
              "authorId": "2181917084",
              "name": "Loïc Chadoutaud"
            },
            {
              "authorId": "2273084649",
              "name": "Marvin Lerousseau"
            },
            {
              "authorId": "2330249227",
              "name": "Daniel Herrero-Saboya"
            },
            {
              "authorId": "2330248288",
              "name": "Julian Ostermaier"
            },
            {
              "authorId": "2403565751",
              "name": "Jacqueline Fontugne"
            },
            {
              "authorId": "2308624629",
              "name": "Emmanuel Barillot"
            },
            {
              "authorId": "2308625316",
              "name": "Thomas Walter"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "696446b22fbceea5b3d6c3c7ef4eeb4ddb6d911f",
          "title": "Bridging the Discrete-Continuous Gap: Unified Multimodal Generation via Coupled Manifold Discrete Absorbing Diffusion",
          "year": 2026,
          "authors": [
            {
              "authorId": "2295682821",
              "name": "Yuanfeng Xu"
            },
            {
              "authorId": "2295847042",
              "name": "Yuhao Chen"
            },
            {
              "authorId": "2296030747",
              "name": "Liang Lin"
            },
            {
              "authorId": "2292001007",
              "name": "Guangrun Wang"
            }
          ]
        }
      },
      {
        "contexts": [
          "For this purpose, we have selected the moco-v3 model from facebook [29].",
          "These patches are then encoded into robust morphological embeddings using self-supervised contrastive learning (MoCo-v3 [29]), such that each cell is represented by a high-dimensional vector encoding/capturing its morphology."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "f84aada80c780deec8ebaf81d914dc52c85820f2",
          "title": "HEDeST: An Integrative Approach to Enhance Spatial Transcriptomic Deconvolution with Histology",
          "year": 2026,
          "authors": [
            {
              "authorId": "2361458946",
              "name": "Luca Gortana"
            },
            {
              "authorId": "2181917084",
              "name": "Loïc Chadoutaud"
            },
            {
              "authorId": "2377796941",
              "name": "Raphael Bourgade"
            },
            {
              "authorId": "2308624629",
              "name": "Emmanuel Barillot"
            },
            {
              "authorId": "2308625316",
              "name": "Thomas Walter"
            }
          ]
        }
      },
      {
        "contexts": [
          "V ISION Transformer (ViT) models [1], [2], [3] have shown great success in large-scale datasets such as ImageNet [4], whether trained through supervised or self-supervised manner [5], [6], [7], [8], [9].",
          "The models employed in our method are all ViT-B / 16 [1], pre-training in ImageNet [4], using pre-trained self-supervised models including MAE [5] and MoCo v3 [6].",
          "Therefore, it could understand data structures and statistical patterns and improve performance in various downstream tasks [5], [6], [7], [8], [9].",
          "In this section, we primarily evaluate ResLoRA’s performance on self-supervised models (MAE [5] and MoCo v3 [6]).",
          "Especially in MoCo v3 [6] which is a typical self-supervised model, the performance could even degrade to 44%.",
          "However, some studies [14], [15], [29], [45] indicate that PETL methods lack transfer capabilities when applied these self-supervised models ( e.g. , MAE [5] and MoCo v3 [6]) for transfer learning, compared to supervised models.",
          "Restrictions apply. supervised, MAE [5], and MoCo v3 [6] pre-trained models.",
          "MoCo v3 [6], [43], a self-supervised model, implemented contrastive learning with a momentum encoder for ViTs, e ﬃ ciently managing feature representations through a dynamic queue system that maintains diversity without excessive memory demands."
        ],
        "intents": [],
        "isInfluential": true,
        "citingPaper": {
          "paperId": "5b85e30618d5692f80b8a668394bbb56d1386de4",
          "title": "Feature Responsive LoRA: Toward Parameter-Efficient Transfer Learning for Self-Supervised Visual Models",
          "year": 2026,
          "authors": [
            {
              "authorId": "2258416270",
              "name": "Shanshan Wang"
            },
            {
              "authorId": "2375860900",
              "name": "Xiaozheng Shen"
            },
            {
              "authorId": "2238337235",
              "name": "Xun Yang"
            },
            {
              "authorId": "2293300355",
              "name": "Ke Xu"
            },
            {
              "authorId": "2153650094",
              "name": "Xingyi Zhang"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "aa1d6436dd5b6ac1fe7018c73896c5aa25a767e9",
          "title": "Self-supervised exceptional prototypical network for few-shot grading of gastric intestinal metaplasia.",
          "year": 2026,
          "authors": [
            {
              "authorId": "1708171550",
              "name": "Xuanchi Chen"
            },
            {
              "authorId": "2146648891",
              "name": "Yonghui Xu"
            },
            {
              "authorId": "2405080302",
              "name": "Zhen Li"
            },
            {
              "authorId": "2157163017",
              "name": "Mingzhe Zhang"
            },
            {
              "authorId": "2234838968",
              "name": "Hangqi Yu"
            },
            {
              "authorId": "2361476301",
              "name": "Lizhen Cui"
            },
            {
              "authorId": "2237874515",
              "name": "Xiangwei Zheng"
            }
          ]
        }
      },
      {
        "contexts": [
          "…text corpora using next-token prediction objectives [37, 48]; a vision encoder pretrained on visual tasks through self-supervised learning [3, 6, 17], contrastive learning [25, 41] or supervised learning [11, 16, 53]; and a pro-jector that injects visual featrues into the language model."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "2a011ed155125d1427c609de96e52b825852f06a",
          "title": "Spatial-aware Vision Language Model for Autonomous Driving",
          "year": 2025,
          "authors": [
            {
              "authorId": "2401958380",
              "name": "Weijie Wei"
            },
            {
              "authorId": "2403014993",
              "name": "Zhipeng Luo"
            },
            {
              "authorId": "2401933235",
              "name": "Ling Feng"
            },
            {
              "authorId": "1754854",
              "name": "Venice Erin Liong"
            }
          ]
        }
      },
      {
        "contexts": [
          "In contrast, MoCo-v3 (Chen, Xie, and He 2021) incorporates momentum encoders and dynamic dictionaries to further refine the learning of representations."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "b0b24d2c7c955bf751db76a9e65dc5bc0537d4dc",
          "title": "Tracing the Heart's Pathways: ECG Representation Learning from a Cardiac Conduction Perspective",
          "year": 2025,
          "authors": [
            {
              "authorId": "2330191633",
              "name": "Tan Pan"
            },
            {
              "authorId": "2401926675",
              "name": "Yixuan Sun"
            },
            {
              "authorId": "2115485888",
              "name": "Chen Jiang"
            },
            {
              "authorId": "2402613168",
              "name": "Qiong Gao"
            },
            {
              "authorId": "2402060002",
              "name": "Rui Sun"
            },
            {
              "authorId": "2362659559",
              "name": "Xingmeng Zhang"
            },
            {
              "authorId": "2402038632",
              "name": "Zhenqi Yang"
            },
            {
              "authorId": "2316923731",
              "name": "Limei Han"
            },
            {
              "authorId": "2402615966",
              "name": "Yixiu Liang"
            },
            {
              "authorId": "2367133065",
              "name": "Yuan Cheng"
            },
            {
              "authorId": "2362723755",
              "name": "Kaiyu Guo"
            }
          ]
        }
      },
      {
        "contexts": [
          "…with the results summarized in Table I. Specifically, the compared approaches include models trained from scratch without pre-training; MoCov3 [54], DINO [55], IBOT [56], and MAE [18] pre-trained on the ImageNet-1K dataset; MAE and VehicleMAE pre-trained on the Autobot1M dataset; and…"
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "fc25940bc07b5d47dc93e51a679cb91ec9994cd6",
          "title": "Vehicle-centric Perception via Multimodal Structured Pre-training",
          "year": 2025,
          "authors": [
            {
              "authorId": "2275783706",
              "name": "Wentao Wu"
            },
            {
              "authorId": "2349647587",
              "name": "Xiao Wang"
            },
            {
              "authorId": "2275767046",
              "name": "Chenglong Li"
            },
            {
              "authorId": "2275030022",
              "name": "Jin Tang"
            },
            {
              "authorId": "2268490530",
              "name": "Bin Luo"
            }
          ]
        }
      },
      {
        "contexts": [
          "Early progress was driven by contrastive and self-distillation approaches [6, 8, 11–13, 27–29, 33, 43, 73], which learn in-variances across augmented views but often require large batches or memory banks.",
          "Predictive coding suggests that the brain forecasts latent causes of sensory inputs [53], while Contrastive Predictive Coding (CPC) and related frameworks apply this principle in practice [6, 8, 11– 13, 27, 29, 33, 43, 73]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "7fb0074a6f1e89d0065c01eeb122fa8c68a9fffa",
          "title": "Next-Embedding Prediction Makes Strong Vision Learners",
          "year": 2025,
          "authors": [
            {
              "authorId": "2261187041",
              "name": "Sihan Xu"
            },
            {
              "authorId": "2151006930",
              "name": "Ziqiao Ma"
            },
            {
              "authorId": "2324574706",
              "name": "Wenhao Chai"
            },
            {
              "authorId": "2290026490",
              "name": "Xuweiyi Chen"
            },
            {
              "authorId": "2402861835",
              "name": "Weiyang Jin"
            },
            {
              "authorId": "2244741764",
              "name": "Joyce Chai"
            },
            {
              "authorId": "2324769373",
              "name": "Saining Xie"
            },
            {
              "authorId": "2399200231",
              "name": "Stella X. Yu"
            }
          ]
        }
      },
      {
        "contexts": [
          "Third, pretraining paradigms that rely exclusively on SAR data are inherently limited by its semantic scope [7, 35, 49]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "b6b110eebf1e08df852be57b1b141195e69d04a6",
          "title": "SARMAE: Masked Autoencoder for SAR Representation Learning",
          "year": 2025,
          "authors": [
            {
              "authorId": "2400040797",
              "name": "Danxu Liu"
            },
            {
              "authorId": "2399383629",
              "name": "Di Wang"
            },
            {
              "authorId": "2188268705",
              "name": "Hebaixu Wang"
            },
            {
              "authorId": "2388679838",
              "name": "Haoyang Chen"
            },
            {
              "authorId": "2301521689",
              "name": "Wentao Jiang"
            },
            {
              "authorId": "2399436312",
              "name": "Yilin Cheng"
            },
            {
              "authorId": "2358251105",
              "name": "Haonan Guo"
            },
            {
              "authorId": "2399547938",
              "name": "Wei Cui"
            },
            {
              "authorId": "2395665323",
              "name": "Jing Zhang"
            }
          ]
        }
      },
      {
        "contexts": [
          "Unlike image-level contrastive learning [13, 9], this token receives no explicit loss super-vision, yet works effectively in classification tasks."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "6ae2fbff35e1ed3e0f25df86a06b91eb4325f98f",
          "title": "In Pursuit of Pixel Supervision for Visual Pre-training",
          "year": 2025,
          "authors": [
            {
              "authorId": "2268796616",
              "name": "Lihe Yang"
            },
            {
              "authorId": "2530311",
              "name": "Shang-Wen Li"
            },
            {
              "authorId": "2394390868",
              "name": "Yang Li"
            },
            {
              "authorId": "2384335758",
              "name": "Xinjie Lei"
            },
            {
              "authorId": "2346112830",
              "name": "Dong Wang"
            },
            {
              "authorId": "2249719379",
              "name": "Abdelrahman Mohamed"
            },
            {
              "authorId": "2399107735",
              "name": "Hengshuang Zhao"
            },
            {
              "authorId": "2356046304",
              "name": "Hu Xu"
            }
          ]
        }
      },
      {
        "contexts": [
          "In our study, we integrate the proposed method with several widely used SSL frameworks, including SimCLR, SimSiam, and MoCo-v3 [31], and compare them against both supervised models and the original SSL frameworks as baselines."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "df1e9a22fa0819f02a15fab5b0ff5c45ccc7d155",
          "title": "A contrastive learning method integrating pathological prior information for effective differentiation of histological categories in lung squamous cell carcinoma",
          "year": 2025,
          "authors": [
            {
              "authorId": "2399642275",
              "name": "Mingci Huang"
            },
            {
              "authorId": "144796866",
              "name": "Weijin Xiao"
            },
            {
              "authorId": "2400866127",
              "name": "Gen Lin"
            },
            {
              "authorId": "2281710560",
              "name": "Chao Li"
            },
            {
              "authorId": "2324643108",
              "name": "Haipeng Xu"
            },
            {
              "authorId": "2257085061",
              "name": "Yunjian Huang"
            },
            {
              "authorId": "2268033028",
              "name": "Shengjia Chen"
            },
            {
              "authorId": "2260826121",
              "name": "Chuanben Chen"
            },
            {
              "authorId": "2401114273",
              "name": "Yang Sun"
            },
            {
              "authorId": "2267890854",
              "name": "Qiaofeng Zhong"
            }
          ]
        }
      },
      {
        "contexts": [
          "We adopted seven state-of-the-art (SOTA) baselines, among which reconstruction-based methods include Autoencoder (AE) [26], PatchTST [12], PatchTS-Mixer [27], and ECGGAN [18]; contrastive learning methods include MoCoV3 [28], TS-TCC [17], and CRT [29]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "bca6583a758f24f85c1d3142cdb6ec3e5b2f51db",
          "title": "3S-ECG: Three Stage Self-Supervised Model for ECG Classification",
          "year": 2025,
          "authors": [
            {
              "authorId": "2407759578",
              "name": "Xianhao Song"
            },
            {
              "authorId": "2407805230",
              "name": "Junhao Huang"
            },
            {
              "authorId": "2407728959",
              "name": "Yuqi She"
            },
            {
              "authorId": "2407693980",
              "name": "Erke Wang"
            },
            {
              "authorId": "2392448760",
              "name": "Liping Wang"
            }
          ]
        }
      },
      {
        "contexts": [
          "During the reverse process, we employ frozen CLIP [16] and MoCo [17] image encoders to extract semantic features from the I T : Next, perturbing multi-scale features to adapt to the noise distribution of LDCT: where N (1 , σ 2 ) represents Gaussian noise with a mean of 1 and a standard deviation of…"
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "af1b10c502e1a5b0f6362763f79182c76fa2367b",
          "title": "Semantic-Guided Artifact-Aware Diffusion Model for Self-Supervised Low-Dose CT Denoising",
          "year": 2025,
          "authors": [
            {
              "authorId": "2117822084",
              "name": "Chunyan Yu"
            },
            {
              "authorId": "2407777792",
              "name": "Zexi Lin"
            },
            {
              "authorId": "2407759480",
              "name": "Wanjian Xu"
            },
            {
              "authorId": "2407777811",
              "name": "Shengbiao Huang"
            },
            {
              "authorId": "2339562861",
              "name": "Zejie Yan"
            },
            {
              "authorId": "2407848630",
              "name": "Jiannan You"
            }
          ]
        }
      },
      {
        "contexts": [
          "MoCoV3 [27], TS-TCC [14] and CRT [28] are classic contrastive learning methods."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "56e0dc9001e32c9c477bbcd3461d31b49ad46c6f",
          "title": "MTECG: A Multimodal Text-Enhanced Self-Supervised Framework for ECG Classification via Alignment with Pretrained Language Model",
          "year": 2025,
          "authors": [
            {
              "authorId": "2407693980",
              "name": "Erke Wang"
            },
            {
              "authorId": "2407805230",
              "name": "Junhao Huang"
            },
            {
              "authorId": null,
              "name": "Long Xiao"
            },
            {
              "authorId": "2392448760",
              "name": "Liping Wang"
            },
            {
              "authorId": "2407802467",
              "name": "Jiangtao Wang"
            }
          ]
        }
      },
      {
        "contexts": [
          "Comparisons were made against temporal-only methods (TS-TCC [14], CLOCS [15], ASTCL [16], CRT [17], ST-MEM [18]) and image-only methods (SimCLR [19], BYOL [20], BarlowTwins [21], MoCo-v3 [22], SimSiam [23])."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "d268ff8bda3bb842fe2ac5f0f4e0eb308a1d3611",
          "title": "Multi-Granular Temporal-Spatial Modeling: A Dual-Modality Framework for Fusing ECG Sequences and Visual Representations",
          "year": 2025,
          "authors": [
            {
              "authorId": "2337159684",
              "name": "Shuang Wang"
            },
            {
              "authorId": "2335774558",
              "name": "Zhenlong Pang"
            },
            {
              "authorId": "2407819632",
              "name": "Li Zhang"
            },
            {
              "authorId": "2407813121",
              "name": "Xinzhu Xu"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "7093699af642b31f9d7c405fc352c77b4910ede1",
          "title": "Pre-Training with Siamese Networks Using Self-Supervised Information for Unlabeled Images",
          "year": 2025,
          "authors": [
            {
              "authorId": "2262516935",
              "name": "Jie Zhang"
            },
            {
              "authorId": "2404635796",
              "name": "Chao Huang"
            },
            {
              "authorId": "2709461",
              "name": "M. Sahrim"
            },
            {
              "authorId": "1753216238",
              "name": "Mengfei Kang"
            },
            {
              "authorId": "2238258263",
              "name": "Minghua Zhao"
            },
            {
              "authorId": "2102159690",
              "name": "X. Hei"
            }
          ]
        }
      },
      {
        "contexts": [
          "For instance, CTransPath (Wang et al., 2022) uses a MoCov3-based SSL approach (Chen et al., 2021b) and is trained on 15.6 million image patches from 25 anatomical sites to demonstrate its ability in H&E staining image retrieval."
        ],
        "intents": [],
        "isInfluential": true,
        "citingPaper": {
          "paperId": "48bddb23434f0380c0c9db004dce867c3bbd9e29",
          "title": "StainNet: A Special Staining Self-Supervised Vision Transformer for Computational Pathology",
          "year": 2025,
          "authors": [
            {
              "authorId": "2357101815",
              "name": "Jiawen Li"
            },
            {
              "authorId": "2308406403",
              "name": "Jiali Hu"
            },
            {
              "authorId": "2316860335",
              "name": "Xitong Ling"
            },
            {
              "authorId": "2399569282",
              "name": "Yongqiang Lv"
            },
            {
              "authorId": "2290966127",
              "name": "Yuxuan Chen"
            },
            {
              "authorId": "2257866035",
              "name": "Yizhi Wang"
            },
            {
              "authorId": "2258108042",
              "name": "Tian Guan"
            },
            {
              "authorId": "2397820365",
              "name": "Yifei Liu"
            },
            {
              "authorId": "2257880233",
              "name": "Yonghong He"
            }
          ]
        }
      },
      {
        "contexts": [
          "Currently, a prevalent assumption is that encoder performance for representation alignment correlates strongly with ImageNet-1K validation accuracy, a proxy measure of global semantic understanding (Oquab et al., 2024; Chen et al., 2021)."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "4214cfd60b1925f738ac728fa4a5c8feaa489db0",
          "title": "What matters for Representation Alignment: Global Information or Spatial Structure?",
          "year": 2025,
          "authors": [
            {
              "authorId": "2112713578",
              "name": "Jaskirat Singh"
            },
            {
              "authorId": "2257003916",
              "name": "Xingjian Leng"
            },
            {
              "authorId": "34815981",
              "name": "Zongze Wu"
            },
            {
              "authorId": "2256947210",
              "name": "Liang Zheng"
            },
            {
              "authorId": "2268806125",
              "name": "Richard Zhang"
            },
            {
              "authorId": "2268760396",
              "name": "Eli Shechtman"
            },
            {
              "authorId": "2325817465",
              "name": "Saining Xie"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "8875d0877a54ef933c03e86080abe4f078490404",
          "title": "EMCF ecosystem: Towards pretrained foundation model for electron microscopy image analysis",
          "year": 2025,
          "authors": [
            {
              "authorId": "2397923919",
              "name": "Zeyu Yu"
            },
            {
              "authorId": "2247543914",
              "name": "Jiansheng Guo"
            },
            {
              "authorId": "2366918514",
              "name": "Feng Liu"
            },
            {
              "authorId": "2399783335",
              "name": "Mengze Du"
            },
            {
              "authorId": "2397992918",
              "name": "Shan Xu"
            },
            {
              "authorId": "2397984508",
              "name": "Guowei Zhang"
            },
            {
              "authorId": "2398863733",
              "name": "Li Xie"
            },
            {
              "authorId": "2398718914",
              "name": "Bo Han"
            },
            {
              "authorId": "2347806562",
              "name": "Zhonghua Chen"
            },
            {
              "authorId": "2397871856",
              "name": "Gaoliang Deng"
            },
            {
              "authorId": "2397870611",
              "name": "Chen Rui"
            },
            {
              "authorId": "2398794457",
              "name": "Yong He"
            },
            {
              "authorId": "9382075",
              "name": "Xuping Feng"
            }
          ]
        }
      },
      {
        "contexts": [
          "Prior work largely operates on single images [20, 21, 30, 34, 50]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "3b6d6215a07e77a98833cf6ab1c2a2d89e5d0e1b",
          "title": "GorillaWatch: An Automated System for In-the-Wild Gorilla Re-Identification and Population Monitoring",
          "year": 2025,
          "authors": [
            {
              "authorId": "2265751794",
              "name": "Maximilian Schall"
            },
            {
              "authorId": "2406265061",
              "name": "Felix Leonard Knöfel"
            },
            {
              "authorId": "2406265521",
              "name": "Noah Elias König"
            },
            {
              "authorId": "2397387207",
              "name": "Jan Jonas Kubeler"
            },
            {
              "authorId": "2397384136",
              "name": "Maximilian von Klinski"
            },
            {
              "authorId": "2397382256",
              "name": "Joan Wilhelm Linnemann"
            },
            {
              "authorId": "2397387769",
              "name": "Xiaoshi Liu"
            },
            {
              "authorId": "2345177128",
              "name": "I. Schlegelmilch"
            },
            {
              "authorId": "2397383433",
              "name": "Ole Woyciniuk"
            },
            {
              "authorId": "2397381028",
              "name": "Alexandra Schild"
            },
            {
              "authorId": "2393138041",
              "name": "Dante Wasmuht"
            },
            {
              "authorId": "2397383418",
              "name": "Magdalena Bermejo Espinet"
            },
            {
              "authorId": "2397383514",
              "name": "German Illera Basas"
            },
            {
              "authorId": "2397381233",
              "name": "Gerard de Melo"
            }
          ]
        }
      },
      {
        "contexts": [
          "To alleviate this limitation, ViT-based self-supervised learning (SSL) methods such as MoCo v3 [17], BEiT [18], and MAE (masked autoencoders) [19] have been developed to exploit large-scale unlabeled data."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "c8717ff1077ce1814bf7d288a8f211c86b1c6bfe",
          "title": "TransMedVision: Improving Medical Image Analysis Under Data Scarcity With Transferable Visual Representations",
          "year": 2025,
          "authors": [
            {
              "authorId": "2388837561",
              "name": "Hongwang Xiao"
            },
            {
              "authorId": "2287819303",
              "name": "Qiwei Ye"
            },
            {
              "authorId": "2398252447",
              "name": "Yu Shu"
            },
            {
              "authorId": "2398558027",
              "name": "Bo Li"
            },
            {
              "authorId": "2399228824",
              "name": "Yao Li"
            },
            {
              "authorId": "2353387967",
              "name": "Wenhao Huang"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "01bd290c0f1a94ca6d2cbbbad959b4e0c8c6afc6",
          "title": "Unifying Generative Self-Supervised Paradigms with Diffusion Models",
          "year": 2025,
          "authors": [
            {
              "authorId": "2237079389",
              "name": "Luping Zhou"
            },
            {
              "authorId": "2253502773",
              "name": "Xiaoyu Yue"
            }
          ]
        }
      },
      {
        "contexts": [
          "The Mo-CoV3 [8] pretrained resampler has low attention coverage with very similar output latents.",
          "These methods include MIL-NCE pre-training method [37] using randomly sampled tile-level tokens and their caption (report) pairs, pre-training only using LLM’s auto-regressive loss (LLaVA-style fusion), and pre-training using MoCoV3 [8].",
          "The stride sets are [1, 2, 4] and [4, 8, 16]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "d02c784df1fdbdee70e95000d13a46f45a748106",
          "title": "LoC-Path: Learning to Compress for Pathology Multimodal Large Language Models",
          "year": 2025,
          "authors": [
            {
              "authorId": "2374369101",
              "name": "Qingqiao Hu"
            },
            {
              "authorId": "2106019843",
              "name": "Weimin Lyu"
            },
            {
              "authorId": "2268630774",
              "name": "Meilong Xu"
            },
            {
              "authorId": "2372585729",
              "name": "Kehan Qi"
            },
            {
              "authorId": "2109753443",
              "name": "Xiaoling Hu"
            },
            {
              "authorId": "2355334097",
              "name": "Saumya Gupta"
            },
            {
              "authorId": "2397297470",
              "name": "Jiawei Zhou"
            },
            {
              "authorId": "2327555452",
              "name": "Chao Chen"
            }
          ]
        }
      },
      {
        "contexts": [
          "Partial-1 [4], which fine-tunes the last layer of the backbone."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "a744498909536ee4a0752f3fe2430c13268a2b82",
          "title": "Siamese: Stealing Fine-Tuned Visual Foundation Models via Diversified Prompting",
          "year": 2025,
          "authors": [
            {
              "authorId": "2178998612",
              "name": "Madhureeta Das"
            },
            {
              "authorId": "9139350",
              "name": "Gaurav Bagwe"
            },
            {
              "authorId": "2147192742",
              "name": "Miao Pan"
            },
            {
              "authorId": "2397110082",
              "name": "Kaichen Yang"
            },
            {
              "authorId": "2345878035",
              "name": "X. Yuan"
            },
            {
              "authorId": "2357231739",
              "name": "Lan Zhang"
            }
          ]
        }
      },
      {
        "contexts": [
          "6,7 A key strength of these models is their ability to learn structured feature spaces that capture color distributions, textures, edges, and higher-level semantic patterns, enabling broad generalization across datasets and tasks."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "76d8c6cfcaf6aa7096a4c68c8a205982a10c98c4",
          "title": "Vision Foundry: A System for Training Foundational Vision AI Models",
          "year": 2025,
          "authors": [
            {
              "authorId": "2398815805",
              "name": "MS Mahmut S. Gokmen"
            },
            {
              "authorId": "2322420298",
              "name": "BS Mitchell A. Klusty"
            },
            {
              "authorId": "2398815803",
              "name": "BS Evan W. Damron"
            },
            {
              "authorId": "2322421315",
              "name": "BS W. Vaiden Logan"
            },
            {
              "authorId": "2397493405",
              "name": "MS Aaron D. Mullen"
            },
            {
              "authorId": "2322422259",
              "name": "BS Caroline N. Leach"
            },
            {
              "authorId": "2397493402",
              "name": "MS Emily B. Collier"
            },
            {
              "authorId": "2308470560",
              "name": "MS Samuel E. Armstrong"
            },
            {
              "authorId": "2322423325",
              "name": "PhD V. K. Cody Bumgardner"
            }
          ]
        }
      }
    ]
  },
  {
    "paperId": "3a906b77fa218adc171fecb28bb81c24c14dcc7b",
    "title": "Transformers in Vision: A Survey",
    "year": 2021,
    "abstract": "Astounding results from Transformer models on natural language tasks have intrigued the vision community to study their application to computer vision problems. Among their salient benefits, Transformers enable modeling long dependencies between input sequence elements and support parallel processing of sequence as compared to recurrent networks, e.g., Long short-term memory. Different from convolutional networks, Transformers require minimal inductive biases for their design and are naturally suited as set-functions. Furthermore, the straightforward design of Transformers allows processing multiple modalities (e.g., images, videos, text, and speech) using similar processing blocks and demonstrates excellent scalability to very large capacity networks and huge datasets. These strengths have led to exciting progress on a number of vision tasks using Transformer networks. This survey aims to provide a comprehensive overview of the Transformer models in the computer vision discipline. We start with an introduction to fundamental concepts behind the success of Transformers, i.e., self-attention, large-scale pre-training, and bidirectional feature encoding. We then cover extensive applications of transformers in vision including popular recognition tasks (e.g., image classification, object detection, action recognition, and segmentation), generative modeling, multi-modal tasks (e.g., visual-question answering, visual reasoning, and visual grounding), video processing (e.g., activity recognition, video forecasting), low-level vision (e.g., image super-resolution, image enhancement, and colorization), and three-dimensional analysis (e.g., point cloud classification and segmentation). We compare the respective advantages and limitations of popular techniques both in terms of architectural design and their experimental value. Finally, we provide an analysis on open research directions and possible future works. We hope this effort will ignite further interest in the community to solve current challenges toward the application of transformer models in computer vision.",
    "url": "https://www.semanticscholar.org/paper/3a906b77fa218adc171fecb28bb81c24c14dcc7b",
    "venue": "ACM Computing Surveys",
    "publicationDate": "2021-01-04",
    "publicationTypes": [
      "JournalArticle",
      "Review"
    ],
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2101.01169",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2101.01169, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "152973423",
        "name": "Salman Hameed Khan",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "40894826",
        "name": "Muzammal Naseer",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "145684318",
        "name": "Munawar Hayat",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "3323621",
        "name": "Syed Waqas Zamir",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2358803",
        "name": "F. Khan",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "145103012",
        "name": "M. Shah",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 3212,
    "referenceCount": 286,
    "influentialCitationCount": 55,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "externalIds": {
      "DBLP": "journals/corr/abs-2101-01169",
      "ArXiv": "2101.01169",
      "DOI": "10.1145/3505244",
      "CorpusId": 230435805
    },
    "journal": {
      "name": "ACM Computing Surveys (CSUR)",
      "volume": "54",
      "pages": "1 - 41"
    },
    "tldr": "This survey aims to provide a comprehensive overview of the Transformer models in the computer vision discipline with an introduction to fundamental concepts behind the success of Transformers, i.e., self-attention, large-scale pre-training, and bidirectional feature encoding.",
    "citations": [
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "bf5ab4c254ecaa99d0be6aa2a2219d263496e879",
          "title": "ANNet: Adaptive nonlinear network for medical image segmentation with brightness balancing and noise suppression",
          "year": 2026,
          "authors": [
            {
              "authorId": "2398003269",
              "name": "Jinlin Yang"
            },
            {
              "authorId": "2224537026",
              "name": "Xintao Pang"
            },
            {
              "authorId": "2326690651",
              "name": "Chuan Lin"
            },
            {
              "authorId": "2144231632",
              "name": "Huan Wang"
            },
            {
              "authorId": "2333853342",
              "name": "Anwen Zhang"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "f096610c6cc0e7e8642b881f0859e1ce8a56c771",
          "title": "Integrating text and medical images for segmentation using interpretable graph neural network",
          "year": 2026,
          "authors": [
            {
              "authorId": "2119281874",
              "name": "Shurong Chai"
            },
            {
              "authorId": "2267055443",
              "name": "Rahul Kumar Jain"
            },
            {
              "authorId": "2325147437",
              "name": "Shaocong Mo"
            },
            {
              "authorId": "120809601",
              "name": "Jiaqing Liu"
            },
            {
              "authorId": "2100389304",
              "name": "Shiyu Teng"
            },
            {
              "authorId": "35133841",
              "name": "T. Tateyama"
            },
            {
              "authorId": "2289870137",
              "name": "Lanfen Lin"
            },
            {
              "authorId": "2260411130",
              "name": "Yen-Wei Chen"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "a03f0360127c2167add2a241098c5b3b81fc6a4e",
          "title": "Rethinking low-light image enhancement: A local–global synergy perspective",
          "year": 2026,
          "authors": [
            {
              "authorId": "2357332309",
              "name": "Qinghua Lin"
            },
            {
              "authorId": "2340430182",
              "name": "Yu Long"
            },
            {
              "authorId": "2406021781",
              "name": "Xudong Xiong"
            },
            {
              "authorId": "2215129080",
              "name": "Wenchao Jiang"
            },
            {
              "authorId": "2355239773",
              "name": "Zhihua Wang"
            },
            {
              "authorId": "2311313180",
              "name": "Qiuping Jiang"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "1edcf2b0d157e7f3d8ef6b447230a2bc59c75ec4",
          "title": "Knowledge Distillation-Enhanced Model for Water Leakage Segmentation in Tunnels",
          "year": 2026,
          "authors": [
            {
              "authorId": "2140054359",
              "name": "Wenjun Wang"
            },
            {
              "authorId": "2086780870",
              "name": "Chao Su"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "d6461f94d819e73007b611403ca298149f28bfd6",
          "title": "A survey of lightweight methods for object detection networks",
          "year": 2026,
          "authors": [
            {
              "authorId": "1500378476",
              "name": "Jing He"
            },
            {
              "authorId": "2397545602",
              "name": "Jianfei Jiang"
            },
            {
              "authorId": "150942167",
              "name": "Changfan Zhang"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "85fa114b899065c706e3f0925bc4b8f3ae28790a",
          "title": "Efficient dual-branch high-resolution transformer design for crack segmentation",
          "year": 2026,
          "authors": [
            {
              "authorId": "2154719486",
              "name": "Yongshang Li"
            },
            {
              "authorId": "2282542962",
              "name": "Zihan Ma"
            },
            {
              "authorId": "9360176",
              "name": "Ronggui Ma"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "8f6e5f0f4da00a92e736e516477da4c4c67f33b3",
          "title": "Cross-backbone pixel consistency and layer-wise attention fusion for weakly supervised semantic segmentation",
          "year": 2026,
          "authors": [
            {
              "authorId": "2403123361",
              "name": "Mengya Liu"
            },
            {
              "authorId": "2296233296",
              "name": "Lei Zhu"
            },
            {
              "authorId": "2403463691",
              "name": "Jiahui Cheng"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "38367da46e446c61b18c3d23f39d8d63445e26c6",
          "title": "K-nearest neighbor-enhanced Residual Learning Framework for image restoration",
          "year": 2026,
          "authors": [
            {
              "authorId": "2180617765",
              "name": "Yaoyun Zeng"
            },
            {
              "authorId": "2266004957",
              "name": "Hongxia Wang"
            },
            {
              "authorId": "1972160097",
              "name": "Xianchen Zhou"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "a481fd9aaa30573fe497db84f5c0d47160347fce",
          "title": "Z-shaped cropping and enhanced You Only Look Once Version 11 for object detection of occluded small targets",
          "year": 2026,
          "authors": [
            {
              "authorId": "2216814638",
              "name": "Runzi Liu"
            },
            {
              "authorId": "2327691673",
              "name": "Dongri Shan"
            },
            {
              "authorId": "2336919876",
              "name": "Shiyuan Liang"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "47a336f873b8929fa9c764c3fcb0872c5691781b",
          "title": "A Bayesian attention-based Transformer with epistemic and aleatoric uncertainty quantification for trustworthy remaining useful life prediction",
          "year": 2026,
          "authors": [
            {
              "authorId": "2326288467",
              "name": "Lei Wang"
            },
            {
              "authorId": "2406098053",
              "name": "Zaigang Chen"
            },
            {
              "authorId": "2237384721",
              "name": "Zhiwen Liu"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "2986f5f0f02c9aa14ccffd7f23eba9728a28e73e",
          "title": "Enhanced medical image segmentation via synergistic feature guidance and multi-scale refinement",
          "year": 2026,
          "authors": [
            {
              "authorId": "2395063200",
              "name": "Shaoqiang Wang"
            },
            {
              "authorId": "2403933510",
              "name": "Guiling Shi"
            },
            {
              "authorId": "2393718740",
              "name": "Xiaofeng Xu"
            },
            {
              "authorId": "2222893324",
              "name": "Tiyao Liu"
            },
            {
              "authorId": "2400640411",
              "name": "Yawu Zhao"
            },
            {
              "authorId": "2149477853",
              "name": "Xiaochun Cheng"
            },
            {
              "authorId": "2397560388",
              "name": "Yuchen Wang"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "b92d9f0d84b6bccc8cdd6f594927c3a52a50536a",
          "title": "The rise of hallucination in large language models: systematic reviews, performance analysis and challenges",
          "year": 2026,
          "authors": [
            {
              "authorId": "71947274",
              "name": "Shamsu Abdullahi"
            },
            {
              "authorId": "3074983",
              "name": "K. U. Danyaro"
            },
            {
              "authorId": "1721261",
              "name": "H. Chiroma"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "9cac8359af264c4fff3886046c9c4e722bfdfe87",
          "title": "Intelligent surface roughness detection for deep-hole drilling based on pyramid adaptive transformer with MSST data fusion",
          "year": 2026,
          "authors": [
            {
              "authorId": "2290783225",
              "name": "Yue Si"
            },
            {
              "authorId": "2393733493",
              "name": "Wang Xu"
            },
            {
              "authorId": "2386893832",
              "name": "Zhenchao Yang"
            },
            {
              "authorId": "2314066810",
              "name": "Yanghe Liu"
            },
            {
              "authorId": "2069276670",
              "name": "Lingfei Kong"
            },
            {
              "authorId": "2190682575",
              "name": "Yanfei Zhang"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "d7bd58e1af8e5ad046dd8cb15c3231e1e1dda38b",
          "title": "Interpret When Possible: A Tree-Based Hybrid Framework for Interpretable Classification",
          "year": 2026,
          "authors": [
            {
              "authorId": "2110489579",
              "name": "Yifan Li"
            },
            {
              "authorId": "1679286",
              "name": "Shuhan Qi"
            },
            {
              "authorId": "2315982467",
              "name": "Lei Cui"
            },
            {
              "authorId": "2387943825",
              "name": "Chao Xing"
            },
            {
              "authorId": "2198052554",
              "name": "Lei Zhang"
            },
            {
              "authorId": "2320524643",
              "name": "Xuan Wang"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "2cc665e29d335cd20e3a1a61e22ccd6362f092c1",
          "title": "A hybrid prediction model for deep-water semi-submersible platforms motion based on Transformer",
          "year": 2026,
          "authors": [
            {
              "authorId": "2352137457",
              "name": "Ying Li"
            },
            {
              "authorId": "2315741520",
              "name": "Qiyuan Zhong"
            },
            {
              "authorId": "2400045949",
              "name": "Xuanqi Chen"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "1362053f8bedbf1805afe5645360c32a7dc0f84a",
          "title": "Development of a data-driven user-defined hardening model for cyclic behavior via transformer",
          "year": 2026,
          "authors": [
            {
              "authorId": "2391904234",
              "name": "Jonghwan Lee"
            },
            {
              "authorId": "74316724",
              "name": "Burcu Tasdemir"
            },
            {
              "authorId": "2332309179",
              "name": "Michael Martin"
            },
            {
              "authorId": "2269529016",
              "name": "David M. Knowles"
            },
            {
              "authorId": "2269526996",
              "name": "M. Mostafavi"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "ce0cb244b72c29196c8c8e2a20628ad4a5171335",
          "title": "Time–frequency recognition of venting acoustic signal for lithium-ion battery safety warning",
          "year": 2026,
          "authors": [
            {
              "authorId": "2406454796",
              "name": "Tao Zhao"
            },
            {
              "authorId": "2403421050",
              "name": "Zhenqi Lu"
            },
            {
              "authorId": "2145042712",
              "name": "Zhirong Wang"
            },
            {
              "authorId": "2403318115",
              "name": "Rongrong Ji"
            },
            {
              "authorId": "144270928",
              "name": "D. Ouyang"
            },
            {
              "authorId": "2403316228",
              "name": "Zhicheng Yan"
            },
            {
              "authorId": "2404431946",
              "name": "Ganyu Hu"
            },
            {
              "authorId": "2326209153",
              "name": "Yangyan Zheng"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "432fbe08e9eeb5bc378f795d7220ebeb299aa655",
          "title": "MARSNet: A Mamba-driven adaptive framework for robust multisource remote sensing image matching in noisy environments",
          "year": 2026,
          "authors": [
            {
              "authorId": "2351401888",
              "name": "Weipeng Jing"
            },
            {
              "authorId": "2330412449",
              "name": "Peilun Kang"
            },
            {
              "authorId": "2309206329",
              "name": "Donglin Di"
            },
            {
              "authorId": "2330497032",
              "name": "Jian Wang"
            },
            {
              "authorId": "2333934407",
              "name": "Yang Song"
            },
            {
              "authorId": "2404529190",
              "name": "Chao Li"
            },
            {
              "authorId": "2318713798",
              "name": "Lei Fan"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "cfe80f191b209e1a9d13eda4b063a6d619234a8d",
          "title": "A comprehensive review on deep learning for multi-objective optimization",
          "year": 2026,
          "authors": [
            {
              "authorId": "2110818505",
              "name": "Haiping Ma"
            },
            {
              "authorId": "2403820137",
              "name": "Jin Liu"
            },
            {
              "authorId": "2396097769",
              "name": "Jiajun Li"
            },
            {
              "authorId": "2398085616",
              "name": "Junhan Jia"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "562e2fc8961c12a74dab9adb374164de5fc9d930",
          "title": "Real-time object detection for unmanned aerial vehicles based on vision transformer and edge computing.",
          "year": 2026,
          "authors": [
            {
              "authorId": "2309742916",
              "name": "Wenyao Zhu"
            },
            {
              "authorId": null,
              "name": "Ken Chen"
            }
          ]
        }
      },
      {
        "contexts": [
          "The inductive bias of contemporary vision models aims to maximize semantic invariance, systematically discarding local variations to optimize for categorization [21, 30, 20]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "87c27451c94e620532a5a4c4ee2b3cd4cf1f4618",
          "title": "Deep Models, Shallow Alignment: Uncovering the Granularity Mismatch in Neural Decoding",
          "year": 2026,
          "authors": [
            {
              "authorId": "2408093284",
              "name": "Yang Du"
            },
            {
              "authorId": null,
              "name": "Siyuan Dai"
            },
            {
              "authorId": "2407773667",
              "name": "Yonghao Song"
            },
            {
              "authorId": "2254281045",
              "name": "Paul M. Thompson"
            },
            {
              "authorId": "114333070",
              "name": "Haoteng Tang"
            },
            {
              "authorId": "2243975081",
              "name": "Liang Zhan"
            }
          ]
        }
      },
      {
        "contexts": [
          "Trained on large-scale corpora with massive visual data-to-label pairs, these models are capable of capturing deep semantic relationships and contextual dependencies [85]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "2ecbdeaf46e9c006c546e024464769fa479643b6",
          "title": "A Survey on Semantic Communication for Vision: Categories, Frameworks, Enabling Techniques, and Applications",
          "year": 2026,
          "authors": [
            {
              "authorId": "31052407",
              "name": "Runze Cheng"
            },
            {
              "authorId": "2356578740",
              "name": "Yao Sun"
            },
            {
              "authorId": "2338997422",
              "name": "Ahmad Taha"
            },
            {
              "authorId": null,
              "name": "Xuesong Liu"
            },
            {
              "authorId": "2319491303",
              "name": "David Flynn"
            },
            {
              "authorId": "2313782950",
              "name": "M. Imran"
            }
          ]
        }
      },
      {
        "contexts": [
          "Given its unprecedented success, adaptations have been made to this architecture in the past few years for images [16], time series [41], and, naturally, tabular data [14, 37, 44]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "a4744a20a2f100b2604b77fb06db1334d5047837",
          "title": "Exploring Transformer Placement in Variational Autoencoders for Tabular Data Generation",
          "year": 2026,
          "authors": [
            {
              "authorId": "2334529108",
              "name": "An'ibal Silva"
            },
            {
              "authorId": "2269699786",
              "name": "Moisés Santos"
            },
            {
              "authorId": "2315929484",
              "name": "Andr'e Restivo"
            },
            {
              "authorId": "2269473216",
              "name": "Carlos Soares"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "6728de39acedf2722cea8243842d67cdd8ef693c",
          "title": "Physics-Informed Transformer operator for the prediction of three-dimensional turbulence",
          "year": 2026,
          "authors": [
            {
              "authorId": "2407185930",
              "name": "Zhihong Guo"
            },
            {
              "authorId": "2329631645",
              "name": "Sunan Zhao"
            },
            {
              "authorId": "2276094975",
              "name": "Huiyu Yang"
            },
            {
              "authorId": "2143439264",
              "name": "Yunpeng Wang"
            },
            {
              "authorId": "2284029714",
              "name": "Jianchun Wang"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "f0a6c3809e681b11dbfeca43384f03fc3681369b",
          "title": "A Review of Object Detection in Aerial Images by Deep Learning Approaches",
          "year": 2026,
          "authors": [
            {
              "authorId": "70531620",
              "name": "Anupa Vijai"
            },
            {
              "authorId": "2363185485",
              "name": "S. Padmavathi"
            },
            {
              "authorId": "2363181867",
              "name": "D. Venkataraman"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "0cf7d43416b6884409a37579abd6bcbb14cfdfd2",
          "title": "An efficient dual path deep learning framework for COVID-19 classification using lung CT scans with explainable AI",
          "year": 2026,
          "authors": [
            {
              "authorId": "2406464651",
              "name": "Md Mahid Arfan Rahat"
            },
            {
              "authorId": "2304799739",
              "name": "Md. Imamul Islam"
            },
            {
              "authorId": "35628084",
              "name": "Md. Saef Ullah Miah"
            },
            {
              "authorId": "2238778813",
              "name": "Talal Alharbi"
            }
          ]
        }
      },
      {
        "contexts": [
          "Yet, their quadratic complexity ( O ( N 2 ) ) incurs prohibitive memory and computational costs [28]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "cd4bf5f684a934f7a700f1e9e73b408f2ce73411",
          "title": "Fluxamba: Topology-Aware Anisotropic State Space Models for Geological Lineament Segmentation in Multi-Source Remote Sensing",
          "year": 2026,
          "authors": [
            {
              "authorId": "2408178463",
              "name": "Jin Bai"
            },
            {
              "authorId": "2406945690",
              "name": "Huiyao Zhang"
            },
            {
              "authorId": "2406843071",
              "name": "Qi Wen"
            },
            {
              "authorId": "2407041362",
              "name": "Shengyang Li"
            },
            {
              "authorId": "2406969451",
              "name": "Xiaolin Tian"
            },
            {
              "authorId": null,
              "name": "Atta ur Rahman"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "5e6eaec012b3bf18866ad11ae64992e55de639c8",
          "title": "Transformer-Driven Semi-Supervised Learning for Prostate Cancer Histopathology: A DINOv2–TransUNet Framework",
          "year": 2026,
          "authors": [
            {
              "authorId": "2335338155",
              "name": "Rubina Akter Rabeya"
            },
            {
              "authorId": "2207426343",
              "name": "J. Seo"
            },
            {
              "authorId": "121835114",
              "name": "N. Cho"
            },
            {
              "authorId": "2316714520",
              "name": "Hee-Cheol Kim"
            },
            {
              "authorId": "2279264106",
              "name": "Heung-Kook Choi"
            }
          ]
        }
      },
      {
        "contexts": [
          "The ViT processes input images through a sequence of operations that transform 2D spatial data into a 1D sequence of embeddings suitable for transformer processing [33], [34]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "1f81f914a2d1a4b27348a55e0e8a7a0cbc7ee623",
          "title": "CONTEX-T: Contextual Privacy Exploitation via Transformer Spectral Analysis for IoT Device Fingerprinting",
          "year": 2026,
          "authors": [
            {
              "authorId": "2406046343",
              "name": "Nazmul Islam"
            },
            {
              "authorId": "2300373387",
              "name": "Mohammad Zulkernine"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "0fc3c7690be79bcc5498591447c811d856905098",
          "title": "FluidFormer : Transformer with continuous convolution for particle-based fluid simulation.",
          "year": 2026,
          "authors": [
            {
              "authorId": "2276735098",
              "name": "Nianyi Wang"
            },
            {
              "authorId": "2276707818",
              "name": "Shuai Zheng"
            },
            {
              "authorId": "2277185144",
              "name": "Yu Chen"
            },
            {
              "authorId": "2407018620",
              "name": "Hai Zhao"
            },
            {
              "authorId": "2406942354",
              "name": "Zhou Fang"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "325d7f6cc72a9f5caf3a2ca5d655ef8c2da0c397",
          "title": "Real-Time Defect Detection in Fused Deposition 3D Printing Process Using Transformer-based Models",
          "year": 2026,
          "authors": [
            {
              "authorId": "2277528356",
              "name": "Aarush Aggarwal"
            },
            {
              "authorId": "2406535621",
              "name": "Shaurya Bhardwaj"
            },
            {
              "authorId": "2406586863",
              "name": "Vinay Arora"
            },
            {
              "authorId": "2324313713",
              "name": "J. S. Saini"
            },
            {
              "authorId": "2406772025",
              "name": "Sachin Singh"
            }
          ]
        }
      },
      {
        "contexts": [
          "Ensemble Selection — Features selected if they appear in any of the three methods subject to multicollinearity constraint from Equations 11,12,13.",
          "Similarly, vision transformers (ViTs) have gained prominence in medical imaging tasks by leveraging self-attention mechanisms to capture global dependencies across image regions [12], [13]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "948c45b39ad4a2d52538d39e72084530eb1397b8",
          "title": "Early Prediction of Type 2 Diabetes Using Multimodal data and Tabular Transformers",
          "year": 2026,
          "authors": [
            {
              "authorId": "2144296161",
              "name": "Sulaiman Khan"
            },
            {
              "authorId": "2363338690",
              "name": "Md. Rafiul Biswas"
            },
            {
              "authorId": "2163026316",
              "name": "Zubair Shah"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "7813d67fc39d4c744690ab9ed916f586d5b215a3",
          "title": "Comparative Evaluation of Deep Learning Architectures for Non-Destructive Estimation of Carotenoid Content from Visible–Near-Infrared (400–850 nm) Spectral Reflectance Data",
          "year": 2026,
          "authors": [
            {
              "authorId": "2367917457",
              "name": "Yuta Tsuchiya"
            },
            {
              "authorId": "34802647",
              "name": "Y. Hirono"
            },
            {
              "authorId": "8269141",
              "name": "Rei Sonobe"
            }
          ]
        }
      },
      {
        "contexts": [
          "Vision Transformer (ViT) by Dosovitskiy et al. [36] and SegFormer by Xie et al. [37] improved diagnostic accuracy, while Zhang et al. [38] and Khan et al. [41] reviewed Transformer applications in image processing."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "3dbcd4ac06a1554d8bb25af32ce8d769a90a74cb",
          "title": "An Innovative Framework for Breast Cancer Detection Using Pyramid Adaptive Atrous Convolution, Transformer Integration, and Multi-Scale Feature Fusion",
          "year": 2026,
          "authors": [
            {
              "authorId": "2075620217",
              "name": "Ehsan Sadeghi Pour"
            },
            {
              "authorId": "2241432804",
              "name": "Mahdi Esmaeili"
            },
            {
              "authorId": "2631610",
              "name": "Morteza Romoozi"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "ad0c5df93929b996b4d4e7ad5d19ea9aaa07a203",
          "title": "A Robust Deep Learning Ensemble Framework for Waterbody Detection Using High-Resolution X-Band SAR Under Data-Constrained Conditions",
          "year": 2026,
          "authors": [
            {
              "authorId": "2339127248",
              "name": "Soyeon Choi"
            },
            {
              "authorId": "2109546817",
              "name": "S. Kim"
            },
            {
              "authorId": "2260695268",
              "name": "S. V. Nghiem"
            },
            {
              "authorId": "1842145",
              "name": "M. Kafatos"
            },
            {
              "authorId": "2260497501",
              "name": "Minha Choi"
            },
            {
              "authorId": "2294814844",
              "name": "Jinsoo Kim"
            },
            {
              "authorId": "2319951113",
              "name": "Yangwon Lee"
            }
          ]
        }
      },
      {
        "contexts": [
          "Recently, transformer architectures have demonstrated powerful modeling abilities in visual tasks [15], [16], with Swin Transformer surpassing traditional CNNs in image classification [17], [18], [19]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "f5b8cd8b1302b3d5371b858ba34ff115860e878a",
          "title": "A Deep Learning Framework for Fine-Grained Sea Fog Visibility Classification Using Forward-Scattering Sensor Data",
          "year": 2026,
          "authors": [
            {
              "authorId": "2397184332",
              "name": "Mengyang Shi"
            },
            {
              "authorId": "2281035528",
              "name": "Qinyou Hu"
            },
            {
              "authorId": "2369262430",
              "name": "Yuezhao Wu"
            },
            {
              "authorId": "2393374054",
              "name": "Zhe Ma"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "e78651e76bbeffc14ba7c0e835231ed23244bfef",
          "title": "FWISD: Flood and Waterfront Infrastructure Segmentation Dataset with Model Evaluations",
          "year": 2026,
          "authors": [
            {
              "authorId": "2315859380",
              "name": "Kaiwen Xue"
            },
            {
              "authorId": "2405672556",
              "name": "Cheng-Jie Jin"
            }
          ]
        }
      },
      {
        "contexts": [
          "We guarantee temporal alignment across sequences by using autocorrelation-based period identification to determine cycle boundaries[20], [21]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "4b345aefec25a6dbe86f0bf5ebb411f9fb9980c8",
          "title": "GAN and DINOv2 Framework for Robust Cross-Condition Gait Recognition",
          "year": 2026,
          "authors": [
            {
              "authorId": "2401903403",
              "name": "Zaid Derea"
            }
          ]
        }
      },
      {
        "contexts": [
          "Over the past decade, the Transformer architecture has undergone a remarkable transition from natural language processing to computer vision [1], [2], [3] and, more recently, to the natural sciences, demonstrating strong modeling capacity and broad applicability [4], [5], [6]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "8922d39260c2f6d76346ebf08d5b533f78aa7b56",
          "title": "Searth Transformer: A Transformer Architecture Incorporating Earth's Geospheric Physical Priors for Global Mid-Range Weather Forecasting",
          "year": 2026,
          "authors": [
            {
              "authorId": "2405553997",
              "name": "Tianye Li"
            },
            {
              "authorId": "2404759480",
              "name": "Qi Liu"
            },
            {
              "authorId": "2404774774",
              "name": "Hao Li"
            },
            {
              "authorId": "2405474707",
              "name": "Lei Chen"
            },
            {
              "authorId": "2405390593",
              "name": "Wencong Cheng"
            },
            {
              "authorId": "2406790992",
              "name": "Fei Zheng"
            },
            {
              "authorId": "2406199077",
              "name": "Xiangao Xia"
            },
            {
              "authorId": "2404592008",
              "name": "Ya Wang"
            },
            {
              "authorId": "2404592070",
              "name": "Gang Huang"
            },
            {
              "authorId": "2404588516",
              "name": "Weiwei Wang"
            },
            {
              "authorId": null,
              "name": "Xuan Tong"
            },
            {
              "authorId": "108230368",
              "name": "Z. Zu"
            },
            {
              "authorId": "2404638142",
              "name": "Yi Fang"
            },
            {
              "authorId": null,
              "name": "Shenming Fu"
            },
            {
              "authorId": "2404647555",
              "name": "Jiang Jiang"
            },
            {
              "authorId": "2382821899",
              "name": "Haochen Li"
            },
            {
              "authorId": "2309196681",
              "name": "Mingxin Li"
            },
            {
              "authorId": "2324862507",
              "name": "Jiangjiang Xia"
            }
          ]
        }
      },
      {
        "contexts": [
          "Since the Vision Transformer was first proposed by Dosovitskiy et al. [17], many related models have been put forward [23]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "8fd7d92aabebb37a3f0a31931d6203e3748e2958",
          "title": "Do Transformers Understand Ancient Roman Coin Motifs Better than CNNs?",
          "year": 2026,
          "authors": [
            {
              "authorId": "2219005152",
              "name": "David Reid"
            },
            {
              "authorId": "46837178",
              "name": "Ognjen Arandjelovíc"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "fbbaed8aa8daab6ae7e2654727640f36ddc972ca",
          "title": "Deep Learning Approaches for Brain Tumor Classification in MRI Scans: An Analysis of Model Interpretability",
          "year": 2026,
          "authors": [
            {
              "authorId": "2404674336",
              "name": "Emanuela F. Gomes"
            },
            {
              "authorId": "2303566197",
              "name": "Ramiro S. Barbosa"
            }
          ]
        }
      },
      {
        "contexts": [
          "Processing HSI encounters significant computational and memory constraints, thereby limiting its applicability in environments with restricted resources [9] ."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "c490c7988a7d6947051df51406568faad111447e",
          "title": "A lightweight multidirectional Mamba architecture for hyperspectral image classification",
          "year": 2026,
          "authors": [
            {
              "authorId": "2404690791",
              "name": "Rui Cao"
            },
            {
              "authorId": "1643955521",
              "name": "Renjian Zhai"
            }
          ]
        }
      },
      {
        "contexts": [
          "The patch-based processing structure of ViTs enables them to capture long-range dependencies across an image, enabling them for tasks that demand a comprehensive understanding of complex visual content [22]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "51350b3987b641170fbae2a6ad2f3f10843bc2e1",
          "title": "Vision-Language Model for Accurate Crater Detection",
          "year": 2026,
          "authors": [
            {
              "authorId": "2348973277",
              "name": "Patrick Bauer"
            },
            {
              "authorId": "2253650614",
              "name": "Marius Schwinning"
            },
            {
              "authorId": "2253650570",
              "name": "Florian Renk"
            },
            {
              "authorId": "2306996516",
              "name": "Andreas Weinmann"
            },
            {
              "authorId": "2302086179",
              "name": "Hichem Snoussi"
            }
          ]
        }
      },
      {
        "contexts": [
          "Lastly, the Transformer has the ability to capture long-range dependencies through self-attention mechanisms, efficient parallel processing for large datasets, and adaptability to complex, multivariate time-series data [20]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "e29f988937f7462617fba3d54416b88b5e695aff",
          "title": "Hypoxemia prediction in pediatric patients under general anesthesia using machine learning: A retrospective observational study and external validation",
          "year": 2026,
          "authors": [
            {
              "authorId": "31768372",
              "name": "S-H Baek"
            },
            {
              "authorId": "2404413524",
              "name": "Jung-Bin Park"
            },
            {
              "authorId": "2403431978",
              "name": "Jihye Heo"
            },
            {
              "authorId": "2321159089",
              "name": "Kyungsang Kim"
            },
            {
              "authorId": "2257342310",
              "name": "Donghyeon Baek"
            },
            {
              "authorId": "120739521",
              "name": "C. Oh"
            },
            {
              "authorId": "2203023320",
              "name": "Hyungchul Lee"
            },
            {
              "authorId": "2257368800",
              "name": "Dongheon Lee"
            },
            {
              "authorId": "48941778",
              "name": "B. Hong"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "a82a238dad699bf6500ce1b6d97bc3989271ed9e",
          "title": "Robotic estimation of single scuba diver respiration rate for safety in underwater human-robot collaboration",
          "year": 2026,
          "authors": [
            {
              "authorId": "20842378",
              "name": "Demetrious T. Kutzke"
            },
            {
              "authorId": "2404059561",
              "name": "Vennela Dupati"
            },
            {
              "authorId": "1765604",
              "name": "Junaed Sattar"
            }
          ]
        }
      },
      {
        "contexts": [
          "In contrast, transformer-based models such as ViT-B/16 [26] and Swin-S [32] are less consistent: while ViT [26] improves with pretraining in terms of accuracy, Swin-S [32] shows reduced performance, likely due to domain mismatch between ImageNet pretraining and our region mask–augmented input.",
          "We also explored whether other CNN-and transformer-based classifiers, such as ResNet-50, ResNet-101 [18], ViT-B/16 [26], Swin-S [32], and DeiT-S [58], would substantially improve binary classification."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "ef87f456c9bb1f4f4777a58605e58589137b4c8d",
          "title": "Unsupervised Modular Adaptive Region Growing and RegionMix Classification for Wind Turbine Segmentation",
          "year": 2026,
          "authors": [
            {
              "authorId": "2403064800",
              "name": "Raul P'erez-Gonzalo"
            },
            {
              "authorId": "2403065538",
              "name": "Riccardo Magro"
            },
            {
              "authorId": "2305622839",
              "name": "Andreas Espersen"
            },
            {
              "authorId": "2403065937",
              "name": "Antonio Agudo"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "14de9053cc1d247dce7369615684eba5187d6c02",
          "title": "InceptionMamba: A Lightweight and Effective Model for Medical Image Classification Revealing Mamba’s Low-Frequency Bias",
          "year": 2026,
          "authors": [
            {
              "authorId": "2406337127",
              "name": "Bingquan Huang"
            },
            {
              "authorId": "2269840173",
              "name": "Yue Liu"
            },
            {
              "authorId": "2405395779",
              "name": "Bin Tang"
            },
            {
              "authorId": "2403818101",
              "name": "Gang Fang"
            }
          ]
        }
      },
      {
        "contexts": [
          "Vision Transformers (ViTs) have fundamentally reshaped computer vision (Vaswani et al., 2017; Khan et al., 2022; Dosovitskiy et al., 2020; Carion et al., 2020)."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "d1ef0fb8a211bf407270a6f2fdc3d51383ea0c2a",
          "title": "On the Intrinsic Limits of Transformer Image Embeddings in Non-Solvable Spatial Reasoning",
          "year": 2026,
          "authors": [
            {
              "authorId": "2402893775",
              "name": "Siyi Lyu"
            },
            {
              "authorId": "2402930821",
              "name": "Quan Liu"
            },
            {
              "authorId": "2403024182",
              "name": "Feng Yan"
            }
          ]
        }
      },
      {
        "contexts": [
          "On the other hand, Vision Transformers have revolutionized general computer vision by modeling global dependencies through self–attention [20]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "522f8141163ffb85c5b3c4b2b059ac193e31f281",
          "title": "Deep Learning for Semantic Segmentation in Crops: Generalization from Opuntia spp.",
          "year": 2026,
          "authors": [
            {
              "authorId": "2314455825",
              "name": "Arturo Duarte-Rangel"
            },
            {
              "authorId": "1403460972",
              "name": "C. Camacho-Bello"
            },
            {
              "authorId": "1506891914",
              "name": "Eduardo Cornejo-Velázquez"
            },
            {
              "authorId": "1742302951",
              "name": "Mireya Clavel-Maqueda"
            }
          ]
        }
      },
      {
        "contexts": [
          "Transformer architectures have revolutionized sequential modeling across domains [25, 26, 27]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "187325115d29591c9d555a7dce627e6a162f9600",
          "title": "A Foundation Model Approach for Fetal Stress Prediction During Labor From cardiotocography (CTG) recordings",
          "year": 2026,
          "authors": [
            {
              "authorId": "2403997403",
              "name": "Naomi Fridman"
            },
            {
              "authorId": "2403990953",
              "name": "Berta Ben Shachar"
            }
          ]
        }
      }
    ]
  },
  {
    "paperId": "dbe077f8521ecbe0a1477d6148c726d4f053d9c9",
    "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet",
    "year": 2021,
    "abstract": "Transformers, which are popular for language modeling, have been explored for solving vision tasks recently, e.g., the Vision Transformer (ViT) for image classification. The ViT model splits each image into a sequence of tokens with fixed length and then applies multiple Transformer layers to model their global relation for classification. However, ViT achieves inferior performance to CNNs when trained from scratch on a midsize dataset like ImageNet. We find it is because: 1) the simple tokenization of input images fails to model the important local structure such as edges and lines among neighboring pixels, leading to low training sample efficiency; 2) the redundant attention backbone design of ViT leads to limited feature richness for fixed computation budgets and limited training samples. To overcome such limitations, we propose a new Tokens-To-Token Vision Transformer (T2T-VTT), which incorporates 1) a layer-wise Tokens-to-Token (T2T) transformation to progressively structurize the image to tokens by recursively aggregating neighboring Tokens into one Token (Tokens-to-Token), such that local structure represented by surrounding tokens can be modeled and tokens length can be reduced; 2) an efficient backbone with a deep-narrow structure for vision transformer motivated by CNN architecture design after empirical study. Notably, T2T-ViT reduces the parameter count and MACs of vanilla ViT by half, while achieving more than 3.0% improvement when trained from scratch on ImageNet. It also outperforms ResNets and achieves comparable performance with MobileNets by directly training on ImageNet. For example, T2T-ViT with comparable size to ResNet50 (21.5M parameters) can achieve 83.3% top1 accuracy in image resolution 384x384 on ImageNet.1",
    "url": "https://www.semanticscholar.org/paper/dbe077f8521ecbe0a1477d6148c726d4f053d9c9",
    "venue": "IEEE International Conference on Computer Vision",
    "publicationDate": "2021-01-28",
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2101.11986",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2101.11986, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "2087091296",
        "name": "Li Yuan",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2144861793",
        "name": "Yunpeng Chen",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": null,
        "name": "Tao Wang",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "23476952",
        "name": "Weihao Yu",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "145356288",
        "name": "Yujun Shi",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "40983412",
        "name": "Francis E. H. Tay",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "33221685",
        "name": "Jiashi Feng",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "143653681",
        "name": "Shuicheng Yan",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 2363,
    "referenceCount": 63,
    "influentialCitationCount": 232,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "externalIds": {
      "DBLP": "journals/corr/abs-2101-11986",
      "ArXiv": "2101.11986",
      "DOI": "10.1109/ICCV48922.2021.00060",
      "CorpusId": 231719476
    },
    "journal": {
      "name": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)",
      "volume": null,
      "pages": "538-547"
    },
    "tldr": "A new Tokens-To-Token Vision Transformer (T2T-VTT), which incorporates an efficient backbone with a deep-narrow structure for vision transformer motivated by CNN architecture design after empirical study and reduces the parameter count and MACs of vanilla ViT by half.",
    "citations": [
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "8ef9570784e3044ab211d2810d15200aa52ced91",
          "title": "Decoding vision transformer variations for image classification: A guide to performance and usability",
          "year": 2026,
          "authors": [
            {
              "authorId": "2395949759",
              "name": "João Montrezol"
            },
            {
              "authorId": "2220834940",
              "name": "Hugo S. Oliveira"
            },
            {
              "authorId": "2280342404",
              "name": "Hélder P. Oliveira"
            }
          ]
        }
      },
      {
        "contexts": [
          "Transformer-based architectures mainly include Swin Transformer (Swin T) [3], PVT [4], TNT [30], T2T [31], and others."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "ebb03c58c5cc3ac38073c533efaf75c91ce11faf",
          "title": "Rethinking RGB-D salient object detection",
          "year": 2026,
          "authors": [
            {
              "authorId": "2408414403",
              "name": "Chang Kou"
            },
            {
              "authorId": null,
              "name": "Jinyu Han"
            },
            {
              "authorId": "2408472217",
              "name": "Mengyin Wang"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "173ed1ce907758d77471ca62ae09e623b31b715b",
          "title": "YMAD: An efficient poultry gender classification method based on feature fusion and YOLO model",
          "year": 2026,
          "authors": [
            {
              "authorId": "2398299659",
              "name": "Xiaoming Zhao"
            },
            {
              "authorId": "2387791382",
              "name": "Yi Gao"
            },
            {
              "authorId": "2400131923",
              "name": "Biao Chen"
            },
            {
              "authorId": "2399904044",
              "name": "Hanbin Zhao"
            },
            {
              "authorId": "2398980020",
              "name": "Dinhui Luo"
            },
            {
              "authorId": "51338119",
              "name": "Liangliang Lou"
            },
            {
              "authorId": "2333082760",
              "name": "Shiqing Zhang"
            },
            {
              "authorId": "2304858601",
              "name": "Xianhai Guo"
            }
          ]
        }
      },
      {
        "contexts": [
          "The Transformer model, based on the self-attention mechanism, has achieved remarkable success in the ﬁelds of natural language processing [3], [8] and computer vision [2], [9], [18], [38], with the self-attention mechanism playing a crucial role."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "ee1ad4deb3a1b0c1100ec36c639d38f59f130df4",
          "title": "Based on Tensor Core Sparse Kernels Accelerating Deep Neural Networks",
          "year": 2026,
          "authors": [
            {
              "authorId": "2377347648",
              "name": "Shijie Lv"
            },
            {
              "authorId": "2155790180",
              "name": "Debin Liu"
            },
            {
              "authorId": "2386938678",
              "name": "Laurence T. Yang"
            },
            {
              "authorId": "2314812431",
              "name": "Xiao-bin Peng"
            },
            {
              "authorId": "4687333",
              "name": "Ruonan Zhao"
            },
            {
              "authorId": "2210951050",
              "name": "Zecan Yang"
            },
            {
              "authorId": "2144086140",
              "name": "Jun Feng"
            }
          ]
        }
      },
      {
        "contexts": [
          "Transformer architectures (Vaswani et al. 2017) have achieved remarkable success across diverse AI applications, including vision models such as ViT (Yuan et al. 2021), DETR (Carion et al. 2020), DiT (Peebles and Xie 2023), and large language models (LLMs) such as GPT (Floridi and Chiriatti 2020), LLaMA (Touvron et al. 2023), Qwen (Bai et al. 2023), and DeepSeek (Guo et al. 2025).",
          "For vision domain, we select ViT-Base/16 and ViT-Large/14 (Dosovitskiy et al. 2020) for experiments.",
          "Transformer architectures (Vaswani et al. 2017) have achieved remarkable success across diverse AI applications, including vision models such as ViT (Yuan et al. 2021), DETR (Carion et al. 2020), DiT (Peebles and Xie 2023), and large language models (LLMs) such as GPT (Floridi and Chiriatti 2020),…",
          "We apply this approach to both vision and language models, including ViT, LLM, and VLM backbones, and conduct extensive experiments to validate its effectiveness."
        ],
        "intents": [],
        "isInfluential": true,
        "citingPaper": {
          "paperId": "859f9be690a174cd54db31191335c015c6ec9e15",
          "title": "Learnable Permutation for Structured Sparsity on Transformer Models",
          "year": 2026,
          "authors": [
            {
              "authorId": null,
              "name": "Zekai Li"
            },
            {
              "authorId": "2279399536",
              "name": "Ji Liu"
            },
            {
              "authorId": "2301331844",
              "name": "Guanchen Li"
            },
            {
              "authorId": "2279769996",
              "name": "Yixing Xu"
            },
            {
              "authorId": null,
              "name": "Ziqiong Liu"
            },
            {
              "authorId": null,
              "name": "Xuanwu Yin"
            },
            {
              "authorId": "2297014912",
              "name": "Dong Li"
            },
            {
              "authorId": "2271751612",
              "name": "E. Barsoum"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "416cfe3b14003a85a4e128e73e46a88544e325cf",
          "title": "AtPatch: Debugging Transformers via Hot-Fixing Over-Attention",
          "year": 2026,
          "authors": [
            {
              "authorId": "2238663708",
              "name": "Shihao Weng"
            },
            {
              "authorId": "2407676790",
              "name": "Yang Feng"
            },
            {
              "authorId": "2407630145",
              "name": "Jincheng Li"
            },
            {
              "authorId": "2239163020",
              "name": "Yining Yin"
            },
            {
              "authorId": "2407678224",
              "name": "Xiaofei Xie"
            },
            {
              "authorId": "2127092888",
              "name": "Jia Liu"
            }
          ]
        }
      },
      {
        "contexts": [
          "Furthermore, ViTs capable of modeling global dependencies may lack the inductive bias to efficiently capture high-frequency local textures in low-SNR environments [44], potentially affecting their efficacy for weak interference classification."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "ee759cf7e667d4d72088a4872149c3a6a0256f6f",
          "title": "SKANet: A Cognitive Dual-Stream Framework with Adaptive Modality Fusion for Robust Compound GNSS Interference Classification",
          "year": 2026,
          "authors": [
            {
              "authorId": "2406449836",
              "name": "Zhihan Zeng"
            },
            {
              "authorId": "2322452087",
              "name": "Yang Zhao"
            },
            {
              "authorId": "2398605751",
              "name": "Kaihe Wang"
            },
            {
              "authorId": "2340230621",
              "name": "Dusit Niyato"
            },
            {
              "authorId": "2405886918",
              "name": "Hongyuan Shu"
            },
            {
              "authorId": "2405787495",
              "name": "Junchu Zhao"
            },
            {
              "authorId": "2406045328",
              "name": "Yanjun Huang"
            },
            {
              "authorId": "2322440760",
              "name": "Yue Xiu"
            },
            {
              "authorId": "2239194426",
              "name": "Zhongpei Zhang"
            },
            {
              "authorId": "2322444489",
              "name": "Ning Wei"
            }
          ]
        }
      },
      {
        "contexts": [
          "Recently, TransNeXt introduced biological-inspired foveal perception to Vision Transformers, effectively balancing global attention with local detail [49], while Tokens-to-Token (T2T) ViT optimizes feature tokenization for enhanced structure modeling [50]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "46da06e880737d8767be808fea6042ff1d8115c5",
          "title": "PhyG-MoE: A Physics-Guided Mixture-of-Experts Framework for Energy-Efficient GNSS Interference Recognition",
          "year": 2026,
          "authors": [
            {
              "authorId": "2406449836",
              "name": "Zhihan Zeng"
            },
            {
              "authorId": "2322452087",
              "name": "Yang Zhao"
            },
            {
              "authorId": "2398605751",
              "name": "Kaihe Wang"
            },
            {
              "authorId": "2340230621",
              "name": "Dusit Niyato"
            },
            {
              "authorId": "2322440760",
              "name": "Yue Xiu"
            },
            {
              "authorId": "2406378784",
              "name": "Lu Chen"
            },
            {
              "authorId": "2239194426",
              "name": "Zhongpei Zhang"
            },
            {
              "authorId": "2322444489",
              "name": "Ning Wei"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "366ffb8bf3f2fcd90561c3c41f8772d2fb5994b0",
          "title": "NeuroShield: A Neuro-Symbolic Framework for Adversarial Robustness",
          "year": 2026,
          "authors": [
            {
              "authorId": "2394416367",
              "name": "Ali Shafiee Sarvestani"
            },
            {
              "authorId": "2345801174",
              "name": "Jason Schmidt"
            },
            {
              "authorId": "2353129768",
              "name": "Arman Roohi"
            }
          ]
        }
      },
      {
        "contexts": [
          "A typi­ cal approach involves extracting features from an image 𝐼 using a convolutional neural network (CNN) [3] or vision Transformer (ViT) [61] as follows: where 𝜓 ( ⋅ ) is the feature extractor."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "6d3d68cf713f62048cf3849cf2a3612fd1abe11d",
          "title": "The paradigm shift: A comprehensive survey on large vision language models for multimodal fake news detection",
          "year": 2026,
          "authors": [
            {
              "authorId": "2058030542",
              "name": "Wei Ai"
            },
            {
              "authorId": "2405357081",
              "name": "Yilong Tan"
            },
            {
              "authorId": "2269467772",
              "name": "Yuntao Shou"
            },
            {
              "authorId": "2149703522",
              "name": "Tao Meng"
            },
            {
              "authorId": "2405566784",
              "name": "Haowen Chen"
            },
            {
              "authorId": "2291925303",
              "name": "Zhixiong He"
            },
            {
              "authorId": "2269747248",
              "name": "Keqin Li"
            }
          ]
        }
      },
      {
        "contexts": [
          "Soon after that, Transformer architectures (Yuan et al. 2021; Wang et al. 2022) rose to prominence due to their superior modeling capacity and effectively mitigated this limitation."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "be644fe960e74c6668401a0c4eceeeef8cb9aa21",
          "title": "Small but Mighty: Dynamic Wavelet Expert-Guided Fine-Tuning of Large-Scale Models for Optical Remote Sensing Object Segmentation",
          "year": 2026,
          "authors": [
            {
              "authorId": "2319603627",
              "name": "Yanguang Sun"
            },
            {
              "authorId": "2350407248",
              "name": "Chao Wang"
            },
            {
              "authorId": "2313359113",
              "name": "Jian Yang"
            },
            {
              "authorId": "2333975487",
              "name": "Lei Luo"
            }
          ]
        }
      },
      {
        "contexts": [
          "CLIP uses OpenAI’s Vision Transformer (ViT) [141] image encoder to extract semantic visual features."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "8f25cc5d19c0f0da490467a9f4d407295d342509",
          "title": "VisionAidQA: Advancing Visual Question Answering for the Visually Impaired",
          "year": 2026,
          "authors": [
            {
              "authorId": "1606672639",
              "name": "Ratnabali Pal"
            },
            {
              "authorId": "2239999732",
              "name": "Samarjit Kar"
            },
            {
              "authorId": "2356680315",
              "name": "Dilip K. Prasad"
            },
            {
              "authorId": "1607097044",
              "name": "A. Sekh"
            }
          ]
        }
      },
      {
        "contexts": [
          "Concurrently, T2T-ViT [13] refined patch tokenization by progressively aggregating neighboring tokens, enhancing local feature extraction while preserving global representation learning—an essential factor for effective image understanding.",
          "We conduct experiments on two standard ViT models, DeiT-Base [2] and DeiT-Small [2], as well as two representative ViT variants, T2T-ViT [13] and Swin Transformer [14]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "4982fef61ec74d7af1a2f3b182f21988ad26ea08",
          "title": "TAP-ViTs: Task-Adaptive Pruning for On-Device Deployment of Vision Transformers",
          "year": 2026,
          "authors": [
            {
              "authorId": "2140097649",
              "name": "Zhibo Wang"
            },
            {
              "authorId": "2402935926",
              "name": "Zuoyuan Zhang"
            },
            {
              "authorId": "81238297",
              "name": "Xiaoyi Pang"
            },
            {
              "authorId": "2402938842",
              "name": "Qile Zhang"
            },
            {
              "authorId": "2404423623",
              "name": "Xuanyi Hao"
            },
            {
              "authorId": "3187918",
              "name": "Shuguo Zhuo"
            },
            {
              "authorId": "2283262325",
              "name": "Peng Sun"
            }
          ]
        }
      },
      {
        "contexts": [
          "The tokenisation method utilised by T2T[79] was implemented to discern and document local structural information."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "7e65e5b937d5cafe2e73c8a7b650b50017c28201",
          "title": "Fusion-SSAT: Unleashing the Potential of Self-supervised Auxiliary Task by Feature Fusion for Generalized Deepfake Detection",
          "year": 2026,
          "authors": [
            {
              "authorId": "2311114721",
              "name": "Shukesh Reddy"
            },
            {
              "authorId": "2323107858",
              "name": "Srijan Das"
            },
            {
              "authorId": "2265303822",
              "name": "Abhijit Das"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "90108b8cc21603ae0005b872c85893c6cd9d0bd8",
          "title": "FMANet: Fused mamba attention model with multi-type preprocessing for simulated crack-contaminated complex environments",
          "year": 2026,
          "authors": [
            {
              "authorId": "2284637472",
              "name": "Junwen Zheng"
            },
            {
              "authorId": "2380352373",
              "name": "Houxin Lv"
            },
            {
              "authorId": "2380413808",
              "name": "Hangtian Song"
            },
            {
              "authorId": "2380572189",
              "name": "Jiang Li"
            },
            {
              "authorId": "2363590734",
              "name": "Rongrong Bai"
            },
            {
              "authorId": "2309719073",
              "name": "Lingkun Chen"
            },
            {
              "authorId": "2273233590",
              "name": "Qizhi Chen"
            },
            {
              "authorId": "2309673827",
              "name": "Lizhong Jiang"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "94a5d9af7e5288dba2b4f7f09dcd57791ad62d0e",
          "title": "Spatial-spectral patch-based multimodal hyperspectral-X data fusion classification network",
          "year": 2026,
          "authors": [
            {
              "authorId": "2153437175",
              "name": "Haizhu Pan"
            },
            {
              "authorId": "2372460432",
              "name": "Bopeng Ren"
            },
            {
              "authorId": "2328616492",
              "name": "Xuan Li"
            },
            {
              "authorId": "2267425713",
              "name": "Liguo Wang"
            },
            {
              "authorId": "97546346",
              "name": "H. Ge"
            },
            {
              "authorId": "144800793",
              "name": "Cuiping Shi"
            },
            {
              "authorId": "2135262968",
              "name": "Moqi Liu"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "db5c0bbc4ee12ea5b7f1fdfe49eeb478eef47742",
          "title": "PDAViT: Pyramid dual-attention vision transformer",
          "year": 2026,
          "authors": [
            {
              "authorId": "2265491496",
              "name": "Xin Zhou"
            },
            {
              "authorId": "2284570435",
              "name": "Zeyu Jiang"
            },
            {
              "authorId": "2117849395",
              "name": "Zhaohui Ren"
            },
            {
              "authorId": "2330881586",
              "name": "Yongchao Zhang"
            },
            {
              "authorId": "2000335723",
              "name": "Tianzhuang Yu"
            },
            {
              "authorId": "2392439565",
              "name": "Wenyao Ji"
            },
            {
              "authorId": "2162006486",
              "name": "Zheng Liu"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "67a7a7f72ba87175194e2f40ab92069809974afc",
          "title": "CI-TransCNN: A class imbalance hybrid CNN-Transformer Network for facial attribute recognition",
          "year": 2026,
          "authors": [
            {
              "authorId": "2108356572",
              "name": "Yanfei Liu"
            },
            {
              "authorId": "2392101694",
              "name": "Youchang Shi"
            },
            {
              "authorId": "2395849042",
              "name": "Yufei Long"
            },
            {
              "authorId": "2393369846",
              "name": "Miaosen Xu"
            },
            {
              "authorId": "2115267518",
              "name": "Junhua Chen"
            },
            {
              "authorId": "47002870",
              "name": "Yuanqian Li"
            },
            {
              "authorId": "2301467607",
              "name": "Hao Wen"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "050434fdaa1de0b6274276b80be647aa8cf2226d",
          "title": "FeedingMonitor: An improved TNT-based method for evaluating the fish feeding intensity in the recirculating aquaculture system",
          "year": 2026,
          "authors": [
            {
              "authorId": "2298857322",
              "name": "Haiqing Wang"
            },
            {
              "authorId": "2156139134",
              "name": "Huanliang Xu"
            },
            {
              "authorId": "2385507189",
              "name": "Chaoping Lu"
            },
            {
              "authorId": "2310709242",
              "name": "Xi Chen"
            },
            {
              "authorId": "2385482111",
              "name": "Zihan Zhang"
            },
            {
              "authorId": "2279814709",
              "name": "Zhaoyu Zhai"
            }
          ]
        }
      },
      {
        "contexts": [
          "The backbone models include ResNet-152 and ResNet-50 [18], VGG-19 [19], and Vision Transformers (ViT) [20], covering both convolutional and transformer-based architectures.",
          "• We conduct comprehensive evaluations of PatchBlock across multiple neural network models (ResNet-50 [18], VGG-19 [19], Vision Transformers [20], YOLOv4 [14]), datasets (ImageNet [21], INRIA [16], CASIA [22]), and adversarial patch attacks (Google Adversarial Patch (GAP) [6], AdvYOLO [15])."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "e788af149f58640d95799451f09aec8321c49533",
          "title": "PatchBlock: A Lightweight Defense Against Adversarial Patches for Embedded EdgeAI Devices",
          "year": 2026,
          "authors": [
            {
              "authorId": "1381314104",
              "name": "Nandish Chattopadhyay"
            },
            {
              "authorId": "2289835770",
              "name": "Abdul Basit"
            },
            {
              "authorId": "153290110",
              "name": "Amira Guesmi"
            },
            {
              "authorId": "98254755",
              "name": "M. Hanif"
            },
            {
              "authorId": "1413981295",
              "name": "B. Ouni"
            },
            {
              "authorId": "2283769762",
              "name": "Muhammad Shafique"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "6b925c22e1891faa6a995f90d752fabab33a2ac8",
          "title": "CRViT-YOLO: A method for multi-morphological blood cell detection using convolution-restructured vision transformer.",
          "year": 2026,
          "authors": [
            {
              "authorId": "2404473924",
              "name": "Yaning Du"
            },
            {
              "authorId": "2382724270",
              "name": "Yuliang Ma"
            },
            {
              "authorId": "36500931",
              "name": "Qingshan She"
            },
            {
              "authorId": "2283855493",
              "name": "Xugang Xi"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "4ad262970fd177cbc7f048e8479118c9a812b6c8",
          "title": "Transformer Optimization Algorithm for selecting Tokens based on genetic Algorithm",
          "year": 2026,
          "authors": [
            {
              "authorId": "2114111691",
              "name": "Tao Zhou"
            },
            {
              "authorId": "2280419078",
              "name": "Yuxia Niu"
            },
            {
              "authorId": "2115781353",
              "name": "Huiling Lu"
            },
            {
              "authorId": "2280985164",
              "name": "Yujie Guo"
            },
            {
              "authorId": "2249101439",
              "name": "Long Liu"
            },
            {
              "authorId": "2248478709",
              "name": "Huiyu Zhou"
            }
          ]
        }
      },
      {
        "contexts": [
          "…of model pretraining, DeiT [24] introduces a transformer-speciﬁc teacher-student strategy, employing a new distillation token to assimilate knowledge from CNNs. T2T-ViT [23] implements the T2T module to model local image structures, utilizing a deep-narrow transformer structure as the backbone.",
          "This characteristic enhances ViTs’ ability to capture critical image features for model prediction [23], [24]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "24493fb709bc364addda408b20688dc93925b90b",
          "title": "Boosting Adversarial Transferability of Vision Transformers",
          "year": 2026,
          "authors": [
            {
              "authorId": "2317527450",
              "name": "Yajie Wang"
            },
            {
              "authorId": "2335807498",
              "name": "Chuan Zhang"
            },
            {
              "authorId": "2296264516",
              "name": "Huipeng Zhou"
            },
            {
              "authorId": "2355105911",
              "name": "Zuobin Ying"
            },
            {
              "authorId": "2366015040",
              "name": "Zehui Xiong"
            },
            {
              "authorId": "2355446690",
              "name": "Wanlei Zhou"
            },
            {
              "authorId": "2257310187",
              "name": "Liehuang Zhu"
            }
          ]
        }
      },
      {
        "contexts": [
          "32 Standard ViT architectures are typically pretrained on large-scale natural image dataset to learn generalized visual representations."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "e7ed0f63a34a5ea3cf83970e8d4b52e0c50fea8a",
          "title": "An exploratory study on integrating radiomics with vision transformers for enhancing medical imaging classification accuracy",
          "year": 2026,
          "authors": [
            {
              "authorId": "2149232695",
              "name": "Zhenyu Yang"
            },
            {
              "authorId": "2317913147",
              "name": "Rihui Zhang"
            },
            {
              "authorId": "2318359322",
              "name": "Haiming Zhu"
            },
            {
              "authorId": "2355430868",
              "name": "Haipeng Zhang"
            },
            {
              "authorId": "2355786354",
              "name": "Jianliang Wang"
            },
            {
              "authorId": "2310485495",
              "name": "Minbin Chen"
            },
            {
              "authorId": "2240834290",
              "name": "F. Yin"
            },
            {
              "authorId": "2278623955",
              "name": "Chunhao Wang"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "3709759601bd1757653d6cd118de63e4952faeb5",
          "title": "Adversarial Example Generation for Infrared Images",
          "year": 2026,
          "authors": [
            {
              "authorId": "2343389642",
              "name": "Yuefeng Wang"
            },
            {
              "authorId": "2257238911",
              "name": "Weiguo Lin"
            },
            {
              "authorId": "2210047104",
              "name": "Junfeng Xu"
            },
            {
              "authorId": "2374911720",
              "name": "Yikun Xu"
            },
            {
              "authorId": "2313202051",
              "name": "Yong Gan"
            }
          ]
        }
      },
      {
        "contexts": [
          "…efficiency bottleneck of Transformer [35], SSM-based MIL methods have emerged: the bidirectional scanning strategy of MamMIL and the sequence rearrangement strategy of MambaMIL [25] both alleviate the loss of spatial information caused by flattening 2D WSIs into 1D sequences to a certain extent.",
          "However, the self-attention mechanism in Transformer models suffers from quadratic complexity, leading to significant challenges in computational efficiency and memory usage as input sequence length increases or network depth grows [24, 25]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "90ab39c224a0890d6cd90b1964a0dd6c82cc7dad",
          "title": "Multi‐Directional Context Modeling With HCSMIL: Enhancing Cancer Prediction and Subtype Classification From Whole Slide Images",
          "year": 2026,
          "authors": [
            {
              "authorId": null,
              "name": "Jingtao Qiu"
            },
            {
              "authorId": "2405202475",
              "name": "Yucheng Liu"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "a86465840d5d668b396e0bc2740ec3037be2718f",
          "title": "Distilling Structural Knowledge from CNNs to Vision Transformers for Data-Efficient Visual Recognition",
          "year": 2026,
          "authors": [
            {
              "authorId": "2183166660",
              "name": "Dingyao Chen"
            },
            {
              "authorId": "2061528299",
              "name": "Xiao Teng"
            },
            {
              "authorId": "2407268833",
              "name": "Xingyu Shen"
            },
            {
              "authorId": "2407412082",
              "name": "Xun Yang"
            },
            {
              "authorId": "2248489187",
              "name": "Long Lan"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "a06553e06e6575ff09159f5bf58891b0ec3c0d72",
          "title": "SCL-SOD: A hybrid self-supervised contrastive learning framework for salient object detection",
          "year": 2026,
          "authors": [
            {
              "authorId": "2335962794",
              "name": "Zhengda Wu"
            },
            {
              "authorId": "2407867992",
              "name": "Jinbao Wang"
            },
            {
              "authorId": null,
              "name": "Yingchun Cui"
            },
            {
              "authorId": "2298599159",
              "name": "Jinghua Zhu"
            }
          ]
        }
      },
      {
        "contexts": [
          "ViT 는 이미지를 작은 패 치 단위로 분할하여 이를 하나의 시 퀀 스로 변환하는 구조를 갖 는 Transformer 기반의 모델이 다 (Yuan et al., 2021; Yin et al., 2022 (Wu et al., 2022)."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "00709eb5353c87a5d3ab8256f9b2fb0df481526d",
          "title": "Performance Evaluation of Water-Body Monitoring Using High-Resolution Optical Satellite Imagery and Deep Learning Models: CAS500-1 Satellite Imagery and Transformer Model",
          "year": 2025,
          "authors": [
            {
              "authorId": "2310860700",
              "name": "Wanyub Kim"
            },
            {
              "authorId": "2319940773",
              "name": "Shinhyeon Cho"
            },
            {
              "authorId": "2329882555",
              "name": "Junhyuk Jeong"
            },
            {
              "authorId": "2319951113",
              "name": "Yangwon Lee"
            },
            {
              "authorId": "2401446434",
              "name": "Taejung Kim"
            },
            {
              "authorId": "2260497501",
              "name": "Minha Choi"
            }
          ]
        }
      },
      {
        "contexts": [
          "Despite significant advances in computer-aided diagnosis, traditional methods — particularly those based on classical machine learning or early deep learning models like CNNs — face notable limitations [5]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "11657d681f6c94e640a2dc8af24550aef1d84f28",
          "title": "Integrating Vision Transformers with Transfer Learning for Enhanced Kidney Cancer Classification in CT Imaging",
          "year": 2025,
          "authors": [
            {
              "authorId": "2402116573",
              "name": "Ali Mahmoud Ali"
            },
            {
              "authorId": "2402091033",
              "name": "Mahmood Khalsan"
            },
            {
              "authorId": "2402563322",
              "name": "Muntadher Idrees Ali"
            },
            {
              "authorId": "66598645",
              "name": "Mabrouka Ali Jelban"
            },
            {
              "authorId": "2402091051",
              "name": "Teresa Abuya"
            }
          ]
        }
      },
      {
        "contexts": [
          "This multi-modal structure facilitates the exploration of how different information modalities contribute to improved classification performance, as demonstrated in recent multi-view learning studies [5, 15].",
          "…architectures require extensive datasets for effective training, limiting applicability in specialized domains with constrained data availability [15–17] Motivated by these challenges, our approach seeks to augment the ViT framework with structural components that explicitly enhance both local…",
          "[15].",
          "…Refine predictions sequentially across taxonomic levels (e.g., fruits → apples → Pink-Lady), maintaining consistency through the hierarchy These innovations collectively enable HARVEST to achieve an optimal balance between computational efficiency and hierarchical classification accuracy [15, 16]."
        ],
        "intents": [],
        "isInfluential": true,
        "citingPaper": {
          "paperId": "8e3166a86caa99fc3e8d3effecac68e1b6857baf",
          "title": "HARVEST: A Locality‐Enhanced Vision Transformer for Efficient Multi‐Level Grocery Classification",
          "year": 2025,
          "authors": [
            {
              "authorId": "2344596983",
              "name": "Anuruddha Paul"
            },
            {
              "authorId": "145311535",
              "name": "Rishi Raj"
            },
            {
              "authorId": "20950360",
              "name": "M. Gourisaria"
            },
            {
              "authorId": "66803675",
              "name": "A. V. Jha"
            },
            {
              "authorId": "3473882",
              "name": "N. Bizon"
            }
          ]
        }
      },
      {
        "contexts": [
          "T2T-ViT [48] introduced a new token generation strategy to enhance the expressive power of the ViT model."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "33f6117860d3e3d62bf579d112e473b987ae475c",
          "title": "CSI-DMT: multi-focus image fusion via cross-task semantic interaction and dual-attention mixing transformer",
          "year": 2025,
          "authors": [
            {
              "authorId": "2260478104",
              "name": "Hao Zhai"
            },
            {
              "authorId": "2401595645",
              "name": "Yuanzhe Zhang"
            },
            {
              "authorId": "2292925503",
              "name": "Zhi-lin Zeng"
            },
            {
              "authorId": "2377324784",
              "name": "Minyu Deng"
            },
            {
              "authorId": "2401180994",
              "name": "Yiyang Ru"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "3e216c76e567054d38377d2a469057043ed8c3f0",
          "title": "HEART-VIT: Hessian-Guided Efficient Dynamic Attention and Token Pruning in Vision Transformer",
          "year": 2025,
          "authors": [
            {
              "authorId": "2316064277",
              "name": "Mohammad Helal Uddin"
            },
            {
              "authorId": "2336734603",
              "name": "Liam Seymour"
            },
            {
              "authorId": "143890566",
              "name": "S. Baidya"
            }
          ]
        }
      },
      {
        "contexts": [
          "The input image of size H × W × 3 is first grayscaled and decomposed into ViT‑ like patch blocks [25], which are then integrated into a one‑dimensional sequence of size W 32 × 8 C , respectively."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "f008a561de43ec866b66f25a64fd0e4fa679d1af",
          "title": "Breast Ultrasound Image Segmentation Integrating Mamba-CNN and Feature Interaction",
          "year": 2025,
          "authors": [
            {
              "authorId": "2243407272",
              "name": "Guoliang Yang"
            },
            {
              "authorId": "2401088736",
              "name": "Yuyu Zhang"
            },
            {
              "authorId": "2257352809",
              "name": "Hao Yang"
            }
          ]
        }
      },
      {
        "contexts": [
          "To better capture ﬁne-grained details, Yuan et al. (2021) incorporated local rigidity information by combining hierarchical Tokens-to-Token (T2T) transformations and recursively aggregating neighboring tokens to construct image representations.",
          "Chen et al. (2021) developed an Attention Pyramid (APNet) that integrates spatial and channel attention to learn multi-scale discriminative features.",
          "In recent years, Transformers (Vaswani et al. 2017) have been applied to computer vision tasks, including image classiﬁcation (Yuan et al. 2021), object detection (Carion et al. 2020), semantic segmentation (Liu et al. 2023; Pan et al. 2024), and ReID (He et al. 2021).",
          "Chen et al. (2021) proposed an omni-relational high-order Trans-former that models complex inter-feature relations, yielding improved performance.",
          "With the strong representational capacity of deep neural networks, CNN-based models (Zhou et al. 2022; Wei et al. 2019; Jiang et al. 2022; Yao et al. 2019; Ye et al. 2022; Chen et al. 2021) have become mainstream approaches in the person ReID domain and have achieved remarkable performance."
        ],
        "intents": [],
        "isInfluential": true,
        "citingPaper": {
          "paperId": "955076fa8eed9dc688f1c187d0570246e4afbe16",
          "title": "Hierarchical deep synergistic fusion of CNN and transformer for person re-identification",
          "year": 2025,
          "authors": [
            {
              "authorId": "2271299199",
              "name": "Yaxiong Liu"
            },
            {
              "authorId": "2256923845",
              "name": "Jingsong Li"
            },
            {
              "authorId": "2400803197",
              "name": "Guanzhi Lyu"
            },
            {
              "authorId": "2400774038",
              "name": "Yue Yang"
            },
            {
              "authorId": "2400852130",
              "name": "Zhenbo Li"
            },
            {
              "authorId": "2272846029",
              "name": "Yunling Liu"
            }
          ]
        }
      },
      {
        "contexts": [
          "SFGT-CD [ 23 ] leverages building semantic information and incorporates a Transformer-based mod - ule to enhance change analysis, significantly improving change detection accuracy while effectively reducing the reliance on large-scale change detection samples.",
          "Compared to CNNs, Transformers excel in global information modeling but have relatively weaker capabilities in capturing local image details [ 23 , 24 ].",
          "To efficiently capture global information, we adopt a standard ViT [ 23 ], as illustrated in Fig.",
          "MixCDNet [ 34 ] integrates CNNs with Vision Transformers (ViTs) [ 23 ], designing a hierarchical feature extraction module that leverages atten - tion mechanisms to fuse local and global features efficiently."
        ],
        "intents": [],
        "isInfluential": true,
        "citingPaper": {
          "paperId": "a24e9d7d9082d2b8eab99f148c3be6770cb50818",
          "title": "A hybrid CNN-transformer network with difference enhancement and frequency fusion for remote sensing image change detection",
          "year": 2025,
          "authors": [
            {
              "authorId": "2399850868",
              "name": "Meiru Wang"
            },
            {
              "authorId": "2271539725",
              "name": "Sheng Fang"
            },
            {
              "authorId": "2399658683",
              "name": "Yunfan Li"
            },
            {
              "authorId": "2346263649",
              "name": "Xingli Zhang"
            },
            {
              "authorId": "2267187899",
              "name": "Zhe Li"
            }
          ]
        }
      },
      {
        "contexts": [
          "Large language models (LLMs) have emerged as pivotal components in advancing artificial intelligence (AI) [6], [56], [73], [74], [92], [106], [140]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "cfe37bf1a43b71a76283c9d704fa515f9c62e9c5",
          "title": "TEMP: A Memory Efficient Physical-aware Tensor Partition-Mapping Framework on Wafer-scale Chips",
          "year": 2025,
          "authors": [
            {
              "authorId": "2290366430",
              "name": "Huizheng Wang"
            },
            {
              "authorId": "2335175414",
              "name": "Taiquan Wei"
            },
            {
              "authorId": "2380461203",
              "name": "Zichuan Wang"
            },
            {
              "authorId": "2337567053",
              "name": "Dingcheng Jiang"
            },
            {
              "authorId": "2290295076",
              "name": "Qize Yang"
            },
            {
              "authorId": "2367635090",
              "name": "Jiaxin Liu"
            },
            {
              "authorId": "2369885406",
              "name": "Jingxiang Hou"
            },
            {
              "authorId": "2303489619",
              "name": "Chao Li"
            },
            {
              "authorId": "2303925725",
              "name": "Jinyi Deng"
            },
            {
              "authorId": "2337488819",
              "name": "Yang Hu"
            },
            {
              "authorId": "2301577046",
              "name": "Shouyi Yin"
            }
          ]
        }
      },
      {
        "contexts": [
          "Additionally, Yuan et al. [14] introduced T2T ViT, a ViT network with a deep-narrow structure, which significantly reduces computational and parameter requirements."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "ceb96400afe2b16667057d9a025ded0a8f412d17",
          "title": "CTMOT: A CNN-Transformer Framework for Real-Time Multi-Ob-ject Tracking",
          "year": 2025,
          "authors": [
            {
              "authorId": "2403069644",
              "name": "Xiaoyan Liu"
            }
          ]
        }
      },
      {
        "contexts": [
          "…capture long-range dependencies without convolution. m m While ViT showed strong performance on large-scale datasets, follow-up models, such as DeiT [35], T2T-ViT [36], and PVT [37], improved performance in data-constrained settings via distillation, better tokenization, and hierarchical de-sign."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "704ab97c3812c208c5b2d5269c621869a3a8e576",
          "title": "USTM: Unified Spatial and Temporal Modeling for Continuous Sign Language Recognition",
          "year": 2025,
          "authors": [
            {
              "authorId": "2189467707",
              "name": "A. Hasanaath"
            },
            {
              "authorId": "145918569",
              "name": "Hamzah Luqman"
            }
          ]
        }
      },
      {
        "contexts": [
          "This study employs ResNet50 [17] to extract multi-scale local features and introduces ViT [18] to capture global contextual information, addressing the limitations of convolutional networks in global semantic modeling."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "c2b2b2eeeeb745a2942facb1759b84ba857170fe",
          "title": "MSC-TFN: Multi-Scale Convolutional and Transformer Fusion Network for Breast Tumor Classification in Ultrasound Images",
          "year": 2025,
          "authors": [
            {
              "authorId": null,
              "name": "Dan Lu"
            },
            {
              "authorId": "2374911698",
              "name": "Yanchen Xu"
            },
            {
              "authorId": null,
              "name": "Yingnan Zhao"
            },
            {
              "authorId": "2407745496",
              "name": "Hongyang Zhao"
            },
            {
              "authorId": "2339696362",
              "name": "Yi Lin"
            }
          ]
        }
      },
      {
        "contexts": [
          "This redundancy is well-documented in ViT architectures, where raw patch tokenization introduces spatial inefficiencies [18]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "c28333e48be6f40fbcd8a71a48f99254afe0f8e8",
          "title": "Efficient Vision-Language Reasoning via Adaptive Token Pruning",
          "year": 2025,
          "authors": [
            {
              "authorId": "2399203200",
              "name": "Xue Li"
            },
            {
              "authorId": "2398917544",
              "name": "Xiaonan Song"
            },
            {
              "authorId": "2398812813",
              "name": "Henry Hu"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "1dec3e7e104df6b32edee26c3c03838f095f8e1b",
          "title": "A privacy-preserving, on-board satellite image classification technique incorporating homomorphic encryption and transfer learning",
          "year": 2025,
          "authors": [
            {
              "authorId": "2398655094",
              "name": "Abhijit Roy"
            },
            {
              "authorId": "20950360",
              "name": "M. Gourisaria"
            },
            {
              "authorId": "2326959991",
              "name": "Rajdeep Chatterjee"
            },
            {
              "authorId": "66803675",
              "name": "A. V. Jha"
            },
            {
              "authorId": "31077647",
              "name": "B. Appasani"
            },
            {
              "authorId": "3473882",
              "name": "N. Bizon"
            },
            {
              "authorId": "48723541",
              "name": "A. Mazare"
            }
          ]
        }
      },
      {
        "contexts": [
          "This is also reflected in the many models that are available, many of which even offer good out-of-the-box performance [Redmon et al., 2016, Kirillov et al., 2023, He et al., 2017, Yuan et al., 2021].",
          "It could be investigated if this can be improved by collecting more images where small objects are depicted prominently, increasing image resolution, or improving the model, e.g., by exploring the use of vision transformers [Yuan et al., 2021]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "abfe930c47642f629a8a25806a388a11493ede6c",
          "title": "A graph generation pipeline for critical infrastructures based on heuristics, images and depth data",
          "year": 2025,
          "authors": [
            {
              "authorId": "2397386209",
              "name": "Mike Diessner"
            },
            {
              "authorId": "2305427009",
              "name": "Yannick E. Tarant"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "408706915580495878d53fe79090efc2eb3f21f9",
          "title": "Power of Boundary and Reflection: Semantic Transparent Object Segmentation using Pyramid Vision Transformer with Transparent Cues",
          "year": 2025,
          "authors": [
            {
              "authorId": "122699959",
              "name": "Tuan-Anh Vu"
            },
            {
              "authorId": "2047941615",
              "name": "Hai Nguyen-Truong"
            },
            {
              "authorId": "2239565262",
              "name": "Ziqiang Zheng"
            },
            {
              "authorId": "143807806",
              "name": "Binh-Son Hua"
            },
            {
              "authorId": "2366681649",
              "name": "Qing Guo"
            },
            {
              "authorId": "2267245494",
              "name": "Ivor W. Tsang"
            },
            {
              "authorId": "2243240642",
              "name": "Sai-Kit Yeung"
            }
          ]
        }
      },
      {
        "contexts": [
          "For example, Yuan, Li, et al. trained ViT on ImageNet and achieved 83.3% top-1 accuracy[8]."
        ],
        "intents": [],
        "isInfluential": true,
        "citingPaper": {
          "paperId": "d93721e56056172053a22492121e4f3f61ad887f",
          "title": "Foundation Model for Underwater Debris Classification",
          "year": 2025,
          "authors": [
            {
              "authorId": "2405993926",
              "name": "Ruoqing Yan"
            },
            {
              "authorId": "2356191917",
              "name": "Haonan Chen"
            },
            {
              "authorId": "2406263636",
              "name": "Xiaotong He"
            },
            {
              "authorId": "2405967538",
              "name": "Linhao Liu"
            },
            {
              "authorId": "50561338",
              "name": "Nianyin Zeng"
            },
            {
              "authorId": "2173817926",
              "name": "Peishu Wu"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "321f3f10d7524203c20bf55d7d4bc73caf54b538",
          "title": "Thyroid intelligent diagnosis based on THMSNet",
          "year": 2025,
          "authors": [
            {
              "authorId": "2397020646",
              "name": "Zhen Rao"
            },
            {
              "authorId": "2398986398",
              "name": "Tao Yu"
            },
            {
              "authorId": "2397146636",
              "name": "Xitan Yu"
            }
          ]
        }
      },
      {
        "contexts": [
          "( 𝑂𝑂 ( 𝑁𝑁 )) Recent advances in deformable attention [14] and multi-scale deformable attention mechanisms [15] enable efficient sparse and adaptive attention across multiple scales, offering a promising direction for efficient medical image analysis [16]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "7c8b553c0f588db40d5c58ca3420bd7146b47191",
          "title": "DAG-Adapters: Deformable Attention-Guided ViT Adaptation for Medical Image Segmentation",
          "year": 2025,
          "authors": [
            {
              "authorId": "2405732468",
              "name": "Muhammad Samsam Ul Haq"
            },
            {
              "authorId": "2405692725",
              "name": "Ahmad Mujtaba Ahmadi"
            },
            {
              "authorId": null,
              "name": "Taoran Sun"
            },
            {
              "authorId": "2275925324",
              "name": "Dong Yin"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "3bd2a90f0f8ed67ed13248ff939bd4b250279e67",
          "title": "Multi-omics approaches for image classification in disease diagnosis",
          "year": 2025,
          "authors": [
            {
              "authorId": "2396995644",
              "name": "Yan Lin"
            },
            {
              "authorId": "2387118655",
              "name": "Shu Chen"
            },
            {
              "authorId": "2374807718",
              "name": "Jinshan Che"
            },
            {
              "authorId": "2375096761",
              "name": "Mingming Sun"
            },
            {
              "authorId": "2374857713",
              "name": "Yuhong Wang"
            }
          ]
        }
      },
      {
        "contexts": [
          "However, their impact is limited in Transformer-based models, such as Vision Transformer (ViT) [34, 52, 53 ]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "151052d07f81e4e7b88972606d754ff7e6abb4e1",
          "title": "Contrastive adversarial learning with dual-sample guidance for transferable attacks on vision-language pre-training models",
          "year": 2025,
          "authors": [
            {
              "authorId": "2396317134",
              "name": "Yiming Ren"
            },
            {
              "authorId": "2261070542",
              "name": "Yang Xu"
            },
            {
              "authorId": "2108338990",
              "name": "Sicong Zhang"
            },
            {
              "authorId": "2288741749",
              "name": "Xiaoyao Xie"
            }
          ]
        }
      },
      {
        "contexts": [
          "At each time step t , the system receives an input image I t , which is first encoded into token representations F t through a vision transformer encoder [45]: where F t ∈ R N × C denotes the patch-level embeddings."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "56df21412c5fa700913ead3f73c088c12a7e74b8",
          "title": "MUT3R: Motion-aware Updating Transformer for Dynamic 3D Reconstruction",
          "year": 2025,
          "authors": [
            {
              "authorId": "2274933305",
              "name": "Guole Shen"
            },
            {
              "authorId": "2374048506",
              "name": "Tianchen Deng"
            },
            {
              "authorId": "2367585292",
              "name": "Xingrui Qin"
            },
            {
              "authorId": "2295878519",
              "name": "Nailin Wang"
            },
            {
              "authorId": "2275028622",
              "name": "Jianyu Wang"
            },
            {
              "authorId": "2257868132",
              "name": "Yanbo Wang"
            },
            {
              "authorId": "2353183418",
              "name": "Yongtao Chen"
            },
            {
              "authorId": "2238153778",
              "name": "Hesheng Wang"
            },
            {
              "authorId": "2366100741",
              "name": "Jingchuan Wang"
            }
          ]
        }
      }
    ]
  },
  {
    "paperId": "e775e649d815a02373eac840cf5e33a04ff85c95",
    "title": "CvT: Introducing Convolutions to Vision Transformers",
    "year": 2021,
    "abstract": "We present in this paper a new architecture, named Convolutional vision Transformer (CvT), that improves Vision Transformer (ViT) in performance and efficiency by introducing convolutions into ViT to yield the best of both de-signs. This is accomplished through two primary modifications: a hierarchy of Transformers containing a new convolutional token embedding, and a convolutional Transformer block leveraging a convolutional projection. These changes introduce desirable properties of convolutional neural networks (CNNs) to the ViT architecture (i.e. shift, scale, and distortion invariance) while maintaining the merits of Transformers (i.e. dynamic attention, global context, and better generalization). We validate CvT by conducting extensive experiments, showing that this approach achieves state-of-the-art performance over other Vision Transformers and ResNets on ImageNet-1k, with fewer parameters and lower FLOPs. In addition, performance gains are maintained when pretrained on larger datasets (e.g. ImageNet-22k) and fine-tuned to downstream tasks. Pretrained on ImageNet-22k, our CvT-W24 obtains a top-1 accuracy of 87.7% on the ImageNet-1k val set. Finally, our results show that the positional encoding, a crucial component in existing Vision Transformers, can be safely re-moved in our model, simplifying the design for higher resolution vision tasks. Code will be released at https://github.com/microsoft/CvT.",
    "url": "https://www.semanticscholar.org/paper/e775e649d815a02373eac840cf5e33a04ff85c95",
    "venue": "IEEE International Conference on Computer Vision",
    "publicationDate": "2021-03-29",
    "publicationTypes": [
      "JournalArticle",
      "Conference"
    ],
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2103.15808",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2103.15808, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "2119019500",
        "name": "Haiping Wu",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2054421528",
        "name": "Bin Xiao",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "40589056",
        "name": "N. Codella",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2152968847",
        "name": "Mengchen Liu",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "3386593",
        "name": "Xiyang Dai",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "145347147",
        "name": "Lu Yuan",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2152828578",
        "name": "Lei Zhang",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 2296,
    "referenceCount": 47,
    "influentialCitationCount": 191,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "externalIds": {
      "ArXiv": "2103.15808",
      "DBLP": "conf/iccv/WuXCLDY021",
      "DOI": "10.1109/ICCV48922.2021.00009",
      "CorpusId": 232417787
    },
    "journal": {
      "name": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)",
      "volume": null,
      "pages": "22-31"
    },
    "tldr": "A new architecture is presented that improves Vision Transformer (ViT) in performance and efficiency by introducing convolutions into ViT to yield the best of both de-signs, and the positional encoding, a crucial component in existing Vision Transformers, can be safely re-moved in this model.",
    "citations": [
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "09bd2a16aa65c6966a5c6a41699db9dc0d42fb6c",
          "title": "Unleash and integrate the power of pre-trained ViTs via feature fusion for open-vocabulary object detection",
          "year": 2026,
          "authors": [
            {
              "authorId": "2342652397",
              "name": "Xiangyu Gao"
            },
            {
              "authorId": "2294555228",
              "name": "Yu Dai"
            },
            {
              "authorId": "1993661016",
              "name": "Taijin Zhao"
            },
            {
              "authorId": "74787012",
              "name": "Benliu Qiu"
            },
            {
              "authorId": "1993431084",
              "name": "Lanxiao Wang"
            },
            {
              "authorId": "66125335",
              "name": "Heqian Qiu"
            },
            {
              "authorId": "144816629",
              "name": "Qingbo Wu"
            },
            {
              "authorId": "2211347525",
              "name": "Hongliang Li"
            }
          ]
        }
      },
      {
        "contexts": [
          "To solve this issue, state-of-the-art methods [22], [23], [24] combine convolution with Transformer.",
          "CvT [24] replaces linear projection in ViT with the convolutional projection to generate tokens from image patches for better modelling of local spatial context.",
          "Speciﬁcally, we ﬁrst replace the standard linear projections with convolutional projections [24] to better capture local dependencies, following the efﬁcient Transformer block [29]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "c457b5fa93c7e402c5de2d9ac3a6682d30e905a5",
          "title": "Automated Sperm DNA Integrity Prediction via a High-Frequency Enhanced Cross-Scale Transformer",
          "year": 2026,
          "authors": [
            {
              "authorId": "2109081960",
              "name": "Wenyuan Chen"
            },
            {
              "authorId": "2209777483",
              "name": "Haocong Song"
            },
            {
              "authorId": "40630832",
              "name": "Zhuoran Zhang"
            },
            {
              "authorId": "47050484",
              "name": "C. Dai"
            },
            {
              "authorId": "152410779",
              "name": "Guanqiao Shan"
            },
            {
              "authorId": "2140162933",
              "name": "Hang Liu"
            },
            {
              "authorId": "2188896167",
              "name": "Aojun Jiang"
            },
            {
              "authorId": "2356838303",
              "name": "Chen Sun"
            },
            {
              "authorId": "2756214",
              "name": "Changhai Ru"
            },
            {
              "authorId": "2302709436",
              "name": "Clifford Librach"
            },
            {
              "authorId": "2238142992",
              "name": "Yu Sun"
            }
          ]
        }
      },
      {
        "contexts": [
          "Representative works in this area include MobileFormer [31 ], CMT [ 32], CvT [ 33 ], BoTNet [ 34 ], Next-ViT [ 35 ], EdgeViTs [ 36], as well as MobileViT v1 [ 37] and v2 [38 ]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "9a6ab45039ad9840ba5333c667a372730ffe7f05",
          "title": "DPFA-net: a lightweight hybrid neural network with dual path feature aggregation for food image recognition",
          "year": 2026,
          "authors": [
            {
              "authorId": "2278774767",
              "name": "Xiangyi Zhu"
            },
            {
              "authorId": "2408472877",
              "name": "Wenli Zhang"
            },
            {
              "authorId": null,
              "name": "Yingnan Sheng"
            },
            {
              "authorId": null,
              "name": "Congrui Lv"
            },
            {
              "authorId": "2278723884",
              "name": "Guorui Sheng"
            },
            {
              "authorId": "2366119",
              "name": "Weiqing Min"
            },
            {
              "authorId": "2249173362",
              "name": "Shuqiang Jiang"
            }
          ]
        }
      },
      {
        "contexts": [
          "CVT [ 17] combines convolutional embedding with transformer self-attention, improving local feature extraction and efficiency.",
          "This has inspired further research [ 15–17] into hybrid models that combine the strengths of CNNs and transformers."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "7a1399d3be324af164e9a3ee46621472fe9e30ba",
          "title": "Multi-view and spatial-correlation interaction for multi-scale object detection",
          "year": 2026,
          "authors": [
            {
              "authorId": "2408442135",
              "name": "Yike Yang"
            },
            {
              "authorId": "2408472233",
              "name": "Zhaohui Zhu"
            },
            {
              "authorId": null,
              "name": "Zekun Li"
            },
            {
              "authorId": "2265805744",
              "name": "Peidong He"
            },
            {
              "authorId": null,
              "name": "Ziqi Zhang"
            },
            {
              "authorId": null,
              "name": "Yaqi Wang"
            },
            {
              "authorId": "2266140833",
              "name": "Yuan Ma"
            },
            {
              "authorId": null,
              "name": "Bing Li"
            },
            {
              "authorId": "2408462411",
              "name": "Yang Bai"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "6e47e13f59a51219f783b8428424002a9598d1ea",
          "title": "Few-shot transformers for image classification using masked self-distillation and optimal tokens based global-local feature interactions",
          "year": 2026,
          "authors": [
            {
              "authorId": "2274598546",
              "name": "Binquan Li"
            },
            {
              "authorId": "2335163880",
              "name": "Xin Guo"
            },
            {
              "authorId": "2291532951",
              "name": "Lishuang Gong"
            }
          ]
        }
      },
      {
        "contexts": [
          "Wu et al. [16] proposed the Swin Transformer, which confines self-attention computation to non-overlapping local windows while enabling cross-window connections, thereby reducing computational complexity."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "f311bbd9a910d168b2ec13a231cafecf19419ffe",
          "title": "An End-to-end learning for classification and segmentation of breast cancer",
          "year": 2026,
          "authors": [
            {
              "authorId": null,
              "name": "Jinling Chen"
            },
            {
              "authorId": "2408422410",
              "name": "Jie Chen"
            },
            {
              "authorId": null,
              "name": "Zhanbo Tan"
            },
            {
              "authorId": null,
              "name": "Zhuowei Tang"
            },
            {
              "authorId": "2408473727",
              "name": "Jihong Wei"
            },
            {
              "authorId": "2408429034",
              "name": "Qi ke"
            },
            {
              "authorId": null,
              "name": "Yuzhu Ji"
            },
            {
              "authorId": "2408474711",
              "name": "Ziqing Gao"
            }
          ]
        }
      },
      {
        "contexts": [
          "…(Ba et al., 2016) or RMSNorm throughout the model (standard approach), and a mixed approach with a single-group group normalization (equivalent to a LayerNorm on channel dimensions) in the input patching and output projection, RMSNorm elsewhere (our proposal), inspired by CvT (Wu et al., 2021a).",
          "We evaluate different normalization strategies, including homogeneous approaches with LayerNorm (Ba et al., 2016) or RMSNorm throughout the model (standard approach), and a mixed approach with a single-group group normalization (equivalent to a LayerNorm on channel dimensions) in the input patching and output projection, RMSNorm elsewhere (our proposal), inspired by CvT (Wu et al., 2021a)."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "16a6d4826ca05e107b5c435e57a203c5b19b852c",
          "title": "MoHETS: Long-term Time Series Forecasting with Mixture-of-Heterogeneous-Experts",
          "year": 2026,
          "authors": [
            {
              "authorId": "2083094475",
              "name": "Evandro S. Ortigossa"
            },
            {
              "authorId": "2291961834",
              "name": "Guy Lutsker"
            },
            {
              "authorId": "2316700433",
              "name": "Eran Segal"
            }
          ]
        }
      },
      {
        "contexts": [
          "Transformer-based: MAE [145], PVT [146], CVT [147], LXMERT [148], VisualBERT [149] Masked pretraining for targeted features, vision-languagereasoning,andsemantic importanceprediction [90],[150]–[152] L 8 , L 9 , L 10 , L 11 , L 12 .",
          "Transformer-based architecture: Transformers, particularly those based on MAE [145], PVT [146], CVT [147], LXMERT [148], and VisualBERT [149], are well-suited to SRC due to their strong capability in capturing global contextual dependencies and supporting selective attention mechanisms."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "2ecbdeaf46e9c006c546e024464769fa479643b6",
          "title": "A Survey on Semantic Communication for Vision: Categories, Frameworks, Enabling Techniques, and Applications",
          "year": 2026,
          "authors": [
            {
              "authorId": "31052407",
              "name": "Runze Cheng"
            },
            {
              "authorId": "2356578740",
              "name": "Yao Sun"
            },
            {
              "authorId": "2338997422",
              "name": "Ahmad Taha"
            },
            {
              "authorId": null,
              "name": "Xuesong Liu"
            },
            {
              "authorId": "2319491303",
              "name": "David Flynn"
            },
            {
              "authorId": "2313782950",
              "name": "M. Imran"
            }
          ]
        }
      },
      {
        "contexts": [
          "With the rapid growth of model scale, training from scratch has become increasingly inefficient (Liu et al., 2021; Wu et al., 2021), making pre-training a cornerstone of modern visual learning, particularly in data-limited scenarios (Qiu et al., 2020; Han et al., 2021)."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "81b99ca2344ae361518231474aee64d137f9ae5c",
          "title": "Self-Supervised Weight Templates for Scalable Vision Model Initialization",
          "year": 2026,
          "authors": [
            {
              "authorId": "2308169564",
              "name": "Yucheng Xie"
            },
            {
              "authorId": "2220294125",
              "name": "Fu Feng"
            },
            {
              "authorId": null,
              "name": "Ruixiao Shi"
            },
            {
              "authorId": "2382932866",
              "name": "Jing Wang"
            },
            {
              "authorId": "2240856453",
              "name": "Yong Rui"
            },
            {
              "authorId": "2273326407",
              "name": "Xin Geng"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "2c3908fe00e5e9a02cc9c83839ae27ff728ce40f",
          "title": "MSCF-LUNet: a lightweight three-stage pine wilt disease segmentation model with multi-scale context fusion mechanism",
          "year": 2026,
          "authors": [
            {
              "authorId": "2404588275",
              "name": "Dejing Zhou"
            },
            {
              "authorId": "2377284977",
              "name": "Junxian Chen"
            },
            {
              "authorId": "2407648200",
              "name": "Wenxi Cai"
            },
            {
              "authorId": "2404723303",
              "name": "Jie Lin"
            },
            {
              "authorId": "2404535165",
              "name": "Tiantian Meng"
            },
            {
              "authorId": "2332819378",
              "name": "Yuanhang Li"
            },
            {
              "authorId": "2405876738",
              "name": "Baihan Liu"
            },
            {
              "authorId": "2405064587",
              "name": "Mengting Luo"
            },
            {
              "authorId": "2323075203",
              "name": "Yubin Lan"
            },
            {
              "authorId": "2218910790",
              "name": "Tianyi Liu"
            },
            {
              "authorId": "2404561634",
              "name": "Jing Zhao"
            }
          ]
        }
      },
      {
        "contexts": [
          "…injecting multi-scale inductive biases from CNNs into ViTs by integrating convolutional operations at various stages: prior to patch embedding, within attention blocks[9](e.g., replacing linear projections as in [69]), via pooling mechanisms[14], or through hybrid hierarchical designs[61], [66]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "92f3e9bf1eaea19be5c657911f4d25d42b7f1285",
          "title": "How to Build Robust, Scalable Models for GSV-Based Indicators in Neighborhood Research",
          "year": 2026,
          "authors": [
            {
              "authorId": "2312296469",
              "name": "Xiaoya Tang"
            },
            {
              "authorId": "2151775430",
              "name": "Xiaohe Yue"
            },
            {
              "authorId": "2132119812",
              "name": "Heran Mane"
            },
            {
              "authorId": "2248097724",
              "name": "Dapeng Li"
            },
            {
              "authorId": "2404275679",
              "name": "Quynh Nguyen"
            },
            {
              "authorId": "2191488732",
              "name": "Tolga Tasdizen"
            }
          ]
        }
      },
      {
        "contexts": [
          "1 √ (cid:48)"
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "4be20ed19410b0dc18e0080c7ad7bb0ff21be525",
          "title": "Attention-driven and multi-scale feature integrated approach for earth surface temperature data reconstruction",
          "year": 2026,
          "authors": [
            {
              "authorId": "2354992952",
              "name": "Minghui Zhang"
            },
            {
              "authorId": "2373956496",
              "name": "Yunjie Chen"
            },
            {
              "authorId": "2338793665",
              "name": "Fan Yang"
            },
            {
              "authorId": "2376046928",
              "name": "Zhengkun Qin"
            }
          ]
        }
      },
      {
        "contexts": [
          "For example, when handling visual contextual information in wireless networks, adding convolutional layers enables FMs to understand and capture the correlations between neighboring pixels [251]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "6d8c8ef39b0bc65ffbd458ceac8a6835efc47b06",
          "title": "Multi-Modal Data-Enhanced Foundation Models for Prediction and Control in Wireless Networks: A Survey",
          "year": 2026,
          "authors": [
            {
              "authorId": "2268476421",
              "name": "Han Zhang"
            },
            {
              "authorId": "2165534716",
              "name": "Mohammad Farzanullah"
            },
            {
              "authorId": "2327774536",
              "name": "Mohammad Ghassemi"
            },
            {
              "authorId": "2292324541",
              "name": "Akram Bin Sediq"
            },
            {
              "authorId": "145845665",
              "name": "Ali Afana"
            },
            {
              "authorId": "2259944377",
              "name": "Melike Erol-Kantarci"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "953ca697d43a422e6475998ba6f984e4026061ca",
          "title": "CBRFormer: rendering technology-based transformer for refinement segmentation of bridge crack images",
          "year": 2026,
          "authors": [
            {
              "authorId": "2267509078",
              "name": "Honghu Chu"
            },
            {
              "authorId": "2381608820",
              "name": "Jiahao Gai"
            },
            {
              "authorId": "2298859923",
              "name": "Weiwei Chen"
            },
            {
              "authorId": "2308280461",
              "name": "Jun Ma"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "cc56688a3af946b9fb14a936493d1a6c567a532c",
          "title": "Vision differential Transformer for brain tumor classification",
          "year": 2026,
          "authors": [
            {
              "authorId": "2260488135",
              "name": "Muhammed Celik"
            },
            {
              "authorId": "104452015",
              "name": "Özkan Inik"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "db5c0bbc4ee12ea5b7f1fdfe49eeb478eef47742",
          "title": "PDAViT: Pyramid dual-attention vision transformer",
          "year": 2026,
          "authors": [
            {
              "authorId": "2265491496",
              "name": "Xin Zhou"
            },
            {
              "authorId": "2284570435",
              "name": "Zeyu Jiang"
            },
            {
              "authorId": "2117849395",
              "name": "Zhaohui Ren"
            },
            {
              "authorId": "2330881586",
              "name": "Yongchao Zhang"
            },
            {
              "authorId": "2000335723",
              "name": "Tianzhuang Yu"
            },
            {
              "authorId": "2392439565",
              "name": "Wenyao Ji"
            },
            {
              "authorId": "2162006486",
              "name": "Zheng Liu"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "67a7a7f72ba87175194e2f40ab92069809974afc",
          "title": "CI-TransCNN: A class imbalance hybrid CNN-Transformer Network for facial attribute recognition",
          "year": 2026,
          "authors": [
            {
              "authorId": "2108356572",
              "name": "Yanfei Liu"
            },
            {
              "authorId": "2392101694",
              "name": "Youchang Shi"
            },
            {
              "authorId": "2395849042",
              "name": "Yufei Long"
            },
            {
              "authorId": "2393369846",
              "name": "Miaosen Xu"
            },
            {
              "authorId": "2115267518",
              "name": "Junhua Chen"
            },
            {
              "authorId": "47002870",
              "name": "Yuanqian Li"
            },
            {
              "authorId": "2301467607",
              "name": "Hao Wen"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "abaf1d9b10f7bb70d21753807f52b60215e54f9f",
          "title": "Hierarchical feature-guided dynamic collaborative learning transformer model for ventricular septal defect identification",
          "year": 2026,
          "authors": [
            {
              "authorId": "2109529391",
              "name": "Cheng Zhao"
            },
            {
              "authorId": "2328439420",
              "name": "Weiling Chen"
            },
            {
              "authorId": "2119185501",
              "name": "Peng Yang"
            },
            {
              "authorId": "2147427211",
              "name": "Zhuo Xiang"
            },
            {
              "authorId": "2260613390",
              "name": "Yiyao Liu"
            },
            {
              "authorId": "2316753838",
              "name": "Bei Xia"
            },
            {
              "authorId": "2404269949",
              "name": "Jing Qin"
            },
            {
              "authorId": "2269572275",
              "name": "Tianfu Wang"
            },
            {
              "authorId": "2251028190",
              "name": "Baiying Lei"
            },
            {
              "authorId": "2328618361",
              "name": "Luyao Zhou"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "5b87aa26be4eb4ee36f696f9f50e5fd8d5d6727b",
          "title": "Frequency-Aware and Lifting-Based Efficient Transformer for Person Search",
          "year": 2026,
          "authors": [
            {
              "authorId": "2371074663",
              "name": "Qilin Shu"
            },
            {
              "authorId": "2312788321",
              "name": "Qixian Zhang"
            },
            {
              "authorId": "2082063772",
              "name": "Duoqian Miao"
            },
            {
              "authorId": "2330662170",
              "name": "Qi Zhang"
            },
            {
              "authorId": "2108879245",
              "name": "Hongyun Zhang"
            },
            {
              "authorId": "2282652052",
              "name": "Cairong Zhao"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "02f47bb0a49edd253c5f945ad1f450eb2bb1d546",
          "title": "Task prior attention network for multi-task learning of dense prediction",
          "year": 2026,
          "authors": [
            {
              "authorId": "2279740037",
              "name": "Yangyang Xu"
            },
            {
              "authorId": "2279765629",
              "name": "Yibo Yang"
            },
            {
              "authorId": "2107901992",
              "name": "Lefei Zhang"
            },
            {
              "authorId": "2295212041",
              "name": "Bo Du"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "bb214d67f81bc0188f9b9971d1528e025cff3d22",
          "title": "DSCA-former: Dual-stem cross-attentive transformer for image denoising",
          "year": 2026,
          "authors": [
            {
              "authorId": "2265389450",
              "name": "Yuxuan Hu"
            },
            {
              "authorId": "2320341",
              "name": "Debo Cheng"
            },
            {
              "authorId": "2305468696",
              "name": "Zhirong Huang"
            },
            {
              "authorId": "2157343006",
              "name": "Boyan Chen"
            },
            {
              "authorId": "2327503711",
              "name": "Shilong Lin"
            },
            {
              "authorId": "2380507129",
              "name": "Shichao Zhang"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "72494f64d31c998abe0488c95888602fb41bcfdc",
          "title": "Mapping alpine grassland species with the VI-MDACvT model based on UAV hyperspectral imagery",
          "year": 2026,
          "authors": [
            {
              "authorId": "153716283",
              "name": "Hui Zhao"
            },
            {
              "authorId": "2404694010",
              "name": "Jundi Wang"
            },
            {
              "authorId": "2404704617",
              "name": "Huaan Jin"
            },
            {
              "authorId": "2274219825",
              "name": "Da Wei"
            },
            {
              "authorId": "2248522173",
              "name": "Zhengrong Yuan"
            },
            {
              "authorId": "2404946435",
              "name": "Zhaoyi Zhang"
            },
            {
              "authorId": "2404931835",
              "name": "Yaohua Luo"
            },
            {
              "authorId": "2274460701",
              "name": "Xiaodan Wang"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "970ede72fc85ce78d0238559689f78bb9ab3f080",
          "title": "Learning Deformable Image Registration with Dilated Attention Transformer",
          "year": 2026,
          "authors": [
            {
              "authorId": "2292215581",
              "name": "Yungeng Zhang"
            },
            {
              "authorId": "2281536506",
              "name": "Yuan Chang"
            },
            {
              "authorId": "2302315914",
              "name": "Xiaohou Shi"
            },
            {
              "authorId": "2297435737",
              "name": "Yaqi Song"
            },
            {
              "authorId": null,
              "name": "ke li"
            },
            {
              "authorId": "2350246374",
              "name": "Feng Wang"
            },
            {
              "authorId": "2281092299",
              "name": "Mingchuan Yang"
            }
          ]
        }
      },
      {
        "contexts": [
          "Lightweight Vision Transformers [42] achieve competitive performance with significantly reduced parameters through bidirectional interaction between local and global features."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "df22fac15a612186a6a9253b5dc12d2713608f3a",
          "title": "An Improved RODNet for Object Detection Based on Radar and Camera Fusion",
          "year": 2026,
          "authors": [
            {
              "authorId": "2404595745",
              "name": "Manman Fan"
            },
            {
              "authorId": "2118002549",
              "name": "Xianpeng Wang"
            },
            {
              "authorId": "41078887",
              "name": "Mingcheng Fu"
            },
            {
              "authorId": "2307497874",
              "name": "Yanqiu Yang"
            },
            {
              "authorId": "51453394",
              "name": "Yuehao Guo"
            },
            {
              "authorId": "2119090987",
              "name": "Xiang Lan"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "baa21ae79a9becba194f5af2de5c1c0533278daf",
          "title": "DiVOT: Differentiated Interaction-Guided Video-Level Object Tracking",
          "year": 2026,
          "authors": [
            {
              "authorId": "2289936126",
              "name": "Zhixi Wu"
            },
            {
              "authorId": "2144339304",
              "name": "Si Chen"
            },
            {
              "authorId": "2256087154",
              "name": "Dahan Wang"
            },
            {
              "authorId": "2260854619",
              "name": "Shunzhi Zhu"
            }
          ]
        }
      },
      {
        "contexts": [
          "In our work, we utilize video-level action understanding, employing a conv-transformer [43], [44], [45] to model temporal tokens both globally and locally at di ﬀ erent temporal scales [38], [46] and enhance feature through a multi-modal knowledge understanding framework leveraging information from…"
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "0e5aeba9a692e076ba21ed403ab9db11fbc683ba",
          "title": "HAhb-KG: Hierarchical Augmented Knowledge Graph for Human Behavior Assisting Cross-Modal Learning Action Detection",
          "year": 2026,
          "authors": [
            {
              "authorId": "2175077094",
              "name": "Xiaocheng Wang"
            },
            {
              "authorId": "1679198",
              "name": "Dehui Kong"
            },
            {
              "authorId": "2108991161",
              "name": "Jinghua Li"
            },
            {
              "authorId": "2323916447",
              "name": "Jing Wang"
            },
            {
              "authorId": "2271735047",
              "name": "Baocai Yin"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "a8b411ea544c7bdce27c713c20fb46c111c979fe",
          "title": "TSNUNet: Two-Stage Nested U-Network for salient object detection",
          "year": 2026,
          "authors": [
            {
              "authorId": "66230037",
              "name": "Luna Sun"
            },
            {
              "authorId": "2249294794",
              "name": "Zhenxue Chen"
            },
            {
              "authorId": null,
              "name": "Xinming Zhu"
            },
            {
              "authorId": "2303928553",
              "name": "Yu Bi"
            },
            {
              "authorId": null,
              "name": "Chengyun Liu"
            },
            {
              "authorId": "2218663386",
              "name": "Q.M. Jonathan Wu"
            }
          ]
        }
      },
      {
        "contexts": [
          "For feature extraction, we respectively applied the pre-trained models including ResNet101 [45], Tresnet [56], and Cvt [57] trained on the ImageNet dataset for the input of the proposed context-aware network, showing the robustness to the feature initializations."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "db79291af58b23dec388b478a950040bf03a70fa",
          "title": "Multi-label Classification with Panoptic Context Aggregation Networks",
          "year": 2025,
          "authors": [
            {
              "authorId": "2841798",
              "name": "Mingyuan Jiu"
            },
            {
              "authorId": "2338118992",
              "name": "Hailong Zhu"
            },
            {
              "authorId": "2401958396",
              "name": "Wenchuan Wei"
            },
            {
              "authorId": "1692389",
              "name": "H. Sahbi"
            },
            {
              "authorId": "2401513954",
              "name": "Rongrong Ji"
            },
            {
              "authorId": "2401955788",
              "name": "Mingliang Xu"
            }
          ]
        }
      },
      {
        "contexts": [
          "CvT [50] combines convolution and transformers, using convolution operations to extract local features while employing transformers for global modeling."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "33f6117860d3e3d62bf579d112e473b987ae475c",
          "title": "CSI-DMT: multi-focus image fusion via cross-task semantic interaction and dual-attention mixing transformer",
          "year": 2025,
          "authors": [
            {
              "authorId": "2260478104",
              "name": "Hao Zhai"
            },
            {
              "authorId": "2401595645",
              "name": "Yuanzhe Zhang"
            },
            {
              "authorId": "2292925503",
              "name": "Zhi-lin Zeng"
            },
            {
              "authorId": "2377324784",
              "name": "Minyu Deng"
            },
            {
              "authorId": "2401180994",
              "name": "Yiyang Ru"
            }
          ]
        }
      },
      {
        "contexts": [
          "Mixformer (Cui et al., 2022) proposes a Mixed Attention Module to replace the standard Siamese architecture’s classiﬁcation and regression branches, beneﬁting from the contributions of the masked convolution autoencoders (ConvMAE) module (Wu et al., 2021)."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "7c97ea9b026c31658044b2c464d3e157fe5affca",
          "title": "A Comprehensive Benchmark for Evaluating Night-time Visual Object Tracking",
          "year": 2025,
          "authors": [
            {
              "authorId": "2328454147",
              "name": "Yu Liu"
            },
            {
              "authorId": "2242997372",
              "name": "Arif Mahmood"
            },
            {
              "authorId": "2328164311",
              "name": "Muhammad Haris Khan"
            }
          ]
        }
      },
      {
        "contexts": [
          "For example, the Convolutional Vision Transformer (CvT) [17] replaces linear projections in self-attention with convolutional ones, enhancing locality modeling and reducing computational cost.",
          "Repre-sentative designs include CvT [17], which replaces linear projections with convolutional ones; CoAtNet [30], which combines depthwise convolutions with attention in a staged hierarchy; and Conformer [31], which fuses convolutional and attention pathways."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "c842cc2371566ad4efd36d4bac0803be7b7519fa",
          "title": "Hardware-Friendly and Efficient Vision Transformer for Deployment on Low-Power Embedded Device",
          "year": 2025,
          "authors": [
            {
              "authorId": "2313718327",
              "name": "Ziyang Chen"
            },
            {
              "authorId": "2400463468",
              "name": "Ming Hao"
            },
            {
              "authorId": "2156923627",
              "name": "Xinye Cao"
            },
            {
              "authorId": "2241652705",
              "name": "Jingwei Zhang"
            },
            {
              "authorId": "2330209021",
              "name": "Chaoyao Shen"
            },
            {
              "authorId": "2319074825",
              "name": "Guoqing Li"
            },
            {
              "authorId": "2292593241",
              "name": "Meng Zhang"
            }
          ]
        }
      },
      {
        "contexts": [
          "These methods demonstrate a strong ability to extract ﬁne-grained local features from person images and achieve a degree of invariance to translation, scaling, and deformation (LeCun et al. 1999; Wu et al. 2021).",
          "Some methods (Wu et al. 2021) replace the linear projections of self-attention in the Transformer with convolutional projections to enhance the local feature encoding."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "955076fa8eed9dc688f1c187d0570246e4afbe16",
          "title": "Hierarchical deep synergistic fusion of CNN and transformer for person re-identification",
          "year": 2025,
          "authors": [
            {
              "authorId": "2271299199",
              "name": "Yaxiong Liu"
            },
            {
              "authorId": "2256923845",
              "name": "Jingsong Li"
            },
            {
              "authorId": "2400803197",
              "name": "Guanzhi Lyu"
            },
            {
              "authorId": "2400774038",
              "name": "Yue Yang"
            },
            {
              "authorId": "2400852130",
              "name": "Zhenbo Li"
            },
            {
              "authorId": "2272846029",
              "name": "Yunling Liu"
            }
          ]
        }
      },
      {
        "contexts": [
          "For each stage, we initialize a CvT-13 backbone from the publicly available ImageNet-22k pretrained checkpoint Wu et al. (2021a) and adapt it to the corresponding binary task by attaching a 2-unit classification head.",
          "All stages in the proposed framework are implemented using the official Microsoft CvT codebase Wu et al. (2021c) with the CvT-13 configuration for 384 × 384 inputs.",
          "In this study, we investigate whether multi-stage Convolutional Vision Transformers (CvTs) Wu et al. (2021b) can model the hierarchical decision-making process used by human seed analysts for single-kernel evaluation.",
          "Each stage employs an independently fine-tuned CvT-13 model Wu et al. (2021c) operating on 384 × 384 RGB images, leveraging CvT’s combination of convolutional inductive biases with the global context modeling of Transformers."
        ],
        "intents": [],
        "isInfluential": true,
        "citingPaper": {
          "paperId": "d7f2f56fa7483918b5362fbcd256e8507986b3dd",
          "title": "CornViT: A Multi-Stage Convolutional Vision Transformer Framework for Hierarchical Corn Kernel Analysis",
          "year": 2025,
          "authors": [
            {
              "authorId": "2335669819",
              "name": "Sai Teja Erukude"
            },
            {
              "authorId": "2400794255",
              "name": "Jane Mascarenhas"
            },
            {
              "authorId": "2283332840",
              "name": "Lior Shamir"
            }
          ]
        }
      },
      {
        "contexts": [
          "It improves the extraction of local features while preserving a global self-attention mechanism [30]. b) Swin: The Swin Transformer (Shifted Window Trans-former) adopts a hierarchical modeling approach using shifted window attention and successfully models local and global dependencies within…"
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "a0f8b132e598627bdbf14ed30b21afb4a3ab7e3b",
          "title": "Recognition of Fabric Construction Types with CNNs and Transformers: Analysis of Knit, Woven, and Non-Woven Fabrics",
          "year": 2025,
          "authors": [
            {
              "authorId": "2407274978",
              "name": "Shohag Babu"
            },
            {
              "authorId": "90467274",
              "name": "M. Nesa"
            },
            {
              "authorId": "2407267142",
              "name": "Md. Selim"
            },
            {
              "authorId": "2269457577",
              "name": "H. Shibu"
            }
          ]
        }
      },
      {
        "contexts": [
          "With similar model capacity, ViT requires signiﬁcantly more training data [41] than CNN to perform better due to the weak inductive bias of Transformer [42].",
          "As previously discussed [41], [42], [43], the jagged edges in Transformers [29], [38], [39] can be contributed to their weak inductive bias, which lacks spatial locality.",
          "While Transformer has the advantage of global representation, it ignores local information, especially when trained on comparatively small datasets [41]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "68a9a8c41cf9964e64a33d08d232522fa71c6215",
          "title": "AMD-HookNet++: Evolution of AMD-HookNet With Hybrid CNN–Transformer Feature Enhancement for Glacier Calving Front Segmentation",
          "year": 2025,
          "authors": [
            {
              "authorId": "2339559033",
              "name": "Fei Wu"
            },
            {
              "authorId": "2319371906",
              "name": "Marcel Dreier"
            },
            {
              "authorId": "2090175979",
              "name": "Nora Gourmelon"
            },
            {
              "authorId": "2356436337",
              "name": "Sebastian Wind"
            },
            {
              "authorId": "2284127613",
              "name": "Jianlin Zhang"
            },
            {
              "authorId": "46693303",
              "name": "T. Seehaus"
            },
            {
              "authorId": "2238062455",
              "name": "Matthias H. Braun"
            },
            {
              "authorId": "2243430949",
              "name": "Andreas K. Maier"
            },
            {
              "authorId": "2321535314",
              "name": "Vincent Christlein"
            }
          ]
        }
      },
      {
        "contexts": [
          "To leverage the advantage of both CNNs and transformers, the hybrid architecture has proven to be e ﬀ ective in computer vision tasks [24], [25], [26], [27], [28], [29], [30], [31].",
          "CvT [28] ﬁrst introduces CNNs before self-attention operation to construct hierarchical representation for local and spatial context modeling."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "7614380b6ab530a12031c82820996b79fa3dfff1",
          "title": "UAGLNet: Uncertainty-Aggregated Global–Local Fusion Network With Cooperative CNN–Transformer for Building Extraction",
          "year": 2025,
          "authors": [
            {
              "authorId": "2316786956",
              "name": "Siyuan Yao"
            },
            {
              "authorId": "2398798817",
              "name": "Dongxiu Liu"
            },
            {
              "authorId": "2266559766",
              "name": "Taotao Li"
            },
            {
              "authorId": "2399249208",
              "name": "Shengjie Li"
            },
            {
              "authorId": "2265263152",
              "name": "Wenqi Ren"
            },
            {
              "authorId": "2265310728",
              "name": "Xiaochun Cao"
            }
          ]
        }
      },
      {
        "contexts": [
          "This article combines the strengths of self-attention in capturing global features with CNN advantages—such as local receptive ﬁelds and weight sharing—to create the CiT model for LPBF defect prediction [28], [29], [30], as shown in Fig."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "6242d7195ebca957110028c186a2f40197a30b1d",
          "title": "A CNN-Integrated Transformer Model for Defect Prediction and Anomaly Localization of Laser Powder Bed Fusion",
          "year": 2025,
          "authors": [
            {
              "authorId": "2238336095",
              "name": "Jiewu Leng"
            },
            {
              "authorId": "2165872663",
              "name": "Zisheng Lin"
            },
            {
              "authorId": "2314615719",
              "name": "Junxing Xie"
            },
            {
              "authorId": "2372446463",
              "name": "Gang Wang"
            },
            {
              "authorId": "2199230629",
              "name": "Xueliang Zhou"
            },
            {
              "authorId": "2304362677",
              "name": "Zhipeng Ye"
            },
            {
              "authorId": "2237416200",
              "name": "Qiang Liu"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "4778f46c75e4618b60997fbd4224b24df7d658c1",
          "title": "LitePT: Lighter Yet Stronger Point Transformer",
          "year": 2025,
          "authors": [
            {
              "authorId": "2146491509",
              "name": "Yuanwen Yue"
            },
            {
              "authorId": "47018294",
              "name": "Damien Robert"
            },
            {
              "authorId": "2398854460",
              "name": "Jianyuan Wang"
            },
            {
              "authorId": "2399523217",
              "name": "Sunghwan Hong"
            },
            {
              "authorId": "1753678",
              "name": "J. D. Wegner"
            },
            {
              "authorId": "2398799997",
              "name": "Christian Rupprecht"
            },
            {
              "authorId": "2241731358",
              "name": "Konrad Schindler"
            }
          ]
        }
      },
      {
        "contexts": [
          "Later variants like Swin Transformer [38] and CvT [39] introduced local inductive biases and convolutional priors, enabling ViTs to close the performance gap with CNNs on dense prediction tasks."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "704ab97c3812c208c5b2d5269c621869a3a8e576",
          "title": "USTM: Unified Spatial and Temporal Modeling for Continuous Sign Language Recognition",
          "year": 2025,
          "authors": [
            {
              "authorId": "2189467707",
              "name": "A. Hasanaath"
            },
            {
              "authorId": "145918569",
              "name": "Hamzah Luqman"
            }
          ]
        }
      },
      {
        "contexts": [
          "To address this limitation, Wu et al. [8] incorporated convolutional operations into the Transformer, allowing the model to combine the inductive bias of CNNs with the dynamic attention mechanism."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "2ca0cbcfb8938e4aae390f79951285f11ad39b3a",
          "title": "Edge-Guided CNN-Transformer Network for Retinal Images Registration",
          "year": 2025,
          "authors": [
            {
              "authorId": "2407926888",
              "name": "Bo Wu"
            }
          ]
        }
      },
      {
        "contexts": [
          "Diverging from the traditional OCR model SVTRv2 (Du et al. 2025c), and more complex interleaved architectures like CvT (Wu et al. 2021) or task-specific models like DETR (Carion et al. 2020), our architecture explores the complementarity of well-established CNN and Transformer, leveraging their respective strengths in a two-step process: Step 1: CNN for fine-grained feature capture.",
          "Diverging from the traditional OCR model SVTRv2 (Du et al. 2025c), and more complex interleaved architectures like CvT (Wu et al. 2021) or task-specific models like DETR (Carion et al. 2020), our architecture explores the complementarity of well-established CNN and Transformer, leveraging their…"
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "cbb3f8281c249284b3124ddec73a039ce8380bc7",
          "title": "Complex Mathematical Expression Recognition: Benchmark, Large-Scale Dataset and Strong Baseline",
          "year": 2025,
          "authors": [
            {
              "authorId": "2398903837",
              "name": "W. Bai"
            },
            {
              "authorId": "1878735637",
              "name": "Yongkun Du"
            },
            {
              "authorId": "2281944597",
              "name": "Yuchen Su"
            },
            {
              "authorId": "2400649105",
              "name": "Yazhen Xie"
            },
            {
              "authorId": "2281998567",
              "name": "Zhineng Chen"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "713e0dc270171d4ff7224150a3ada5abf4d5ebbe",
          "title": "Mobile TantivyFormer U-Net: A Lightweight Network with Dynamic Tanh for UAV-Compatible Crack Segmentation",
          "year": 2025,
          "authors": [
            {
              "authorId": "2326122746",
              "name": "Nan Jiang"
            },
            {
              "authorId": "2405497420",
              "name": "Zhixiang Qian"
            },
            {
              "authorId": "2405542999",
              "name": "Tongtong Zhou"
            },
            {
              "authorId": "2303026752",
              "name": "Lihong Tong"
            },
            {
              "authorId": "2404512358",
              "name": "Lingxiao Guan"
            },
            {
              "authorId": "2352199316",
              "name": "Ziyi Li"
            }
          ]
        }
      },
      {
        "contexts": [
          "Firstly, CNN-based local feature extraction focuses on single images [14], failing to model cross-image intra-class correlations and inter-class distinctions, which hinders the differentiation of images with similar emotional semantics [15]."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "dd1b46a1a4406098bcd39c28808dced875f80340",
          "title": "CNN-Transformer Based Image Emotion Recognition with Multi-Scale Feature Enhancement",
          "year": 2025,
          "authors": [
            {
              "authorId": "2383085489",
              "name": "Xiaoyu Li"
            },
            {
              "authorId": "2403111555",
              "name": "Boya Li"
            },
            {
              "authorId": "2403929617",
              "name": "Yi Qian"
            },
            {
              "authorId": "2404412852",
              "name": "Nan Zhang"
            },
            {
              "authorId": "2308669732",
              "name": "Yate Feng"
            },
            {
              "authorId": "2349564555",
              "name": "Yimin Wen"
            }
          ]
        }
      },
      {
        "contexts": [
          "…balances computational efficiency and multi-scale modeling capability; Wang et al. 12 proposed the Pyramid Vision Transformer (PVT) and Wu et al. 13 developed the Convolutional Vision Transformer (CvT), both of which integrate the strengths of convolution and Transformer architectures to…"
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "3ad8ed6d90369c691075c8f564f47dd0e80e78c4",
          "title": "A spatiotemporal transformer with cross-frame encoding and trajectory-aware decoding for multi-target fish tracking",
          "year": 2025,
          "authors": [
            {
              "authorId": "2314373139",
              "name": "Yang Li"
            },
            {
              "authorId": "2314674039",
              "name": "Lei Han"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "321f3f10d7524203c20bf55d7d4bc73caf54b538",
          "title": "Thyroid intelligent diagnosis based on THMSNet",
          "year": 2025,
          "authors": [
            {
              "authorId": "2397020646",
              "name": "Zhen Rao"
            },
            {
              "authorId": "2398986398",
              "name": "Tao Yu"
            },
            {
              "authorId": "2397146636",
              "name": "Xitan Yu"
            }
          ]
        }
      },
      {
        "contexts": [
          "We adopt Convolution-based Positional Encoding (ConvPE) (Chu et al. 2021; Wu et al. 2021). to inject position information while accommodating varying input features x l w h c 1 × × ( w , h and c , denote width, height and channel)."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "0fe166496d1f3ee9a7b1d480bd59b4ad474c5bfb",
          "title": "RTFormer: radiative transfer model-coupled transformer for cloud removal in optical remote sensing imagery",
          "year": 2025,
          "authors": [
            {
              "authorId": "2269176374",
              "name": "Shiyao Meng"
            },
            {
              "authorId": "2261249094",
              "name": "Siwei Li"
            },
            {
              "authorId": "2354651654",
              "name": "Xinyu Wang"
            },
            {
              "authorId": "2149543154",
              "name": "Ge Song"
            },
            {
              "authorId": "2118580425",
              "name": "Jie Yang"
            },
            {
              "authorId": "2261258452",
              "name": "Yu Ding"
            },
            {
              "authorId": "2318771308",
              "name": "Weishu Gong"
            }
          ]
        }
      },
      {
        "contexts": [
          "Processing a video clip, which consists of a sequence of frames that are further divided into many patches, results in an overwhelming number of tokens, making the model prohibitively slow and resource-intensive [8].",
          "The competitors include reconstruction-based methods (Conv-AE [3], Mem-AE [8]), prediction-based methods (Frame-Pred [6]), and recent Transformer-based methods (VT-ADL [14]).",
          "More recently, memory-based models [7, 8] have been introduced to explicitly model the prototypical patterns of normality in a memory module, achieving robust performance."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "4098313f217f397d658b4df9704a589694f055bd",
          "title": "Video Anomaly Detection with Lightweight Vision Transformer and Adaptive Frame Selection",
          "year": 2025,
          "authors": [
            {
              "authorId": "2405730029",
              "name": "Junxiaowen Pan"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "275d8cc8b19eaff50ff8e042400530ed1ac39819",
          "title": "Comparative performance of deep learning models and non-dermatologists in diagnosing psoriasis, dermatophytosis, and eczema",
          "year": 2025,
          "authors": [
            {
              "authorId": "82192884",
              "name": "Nutcha Yodrabum"
            },
            {
              "authorId": "11436547",
              "name": "C. Wongpraparut"
            },
            {
              "authorId": "3334166",
              "name": "Taravichet Titijaroonroj"
            },
            {
              "authorId": "5698205",
              "name": "L. Chularojanamontri"
            },
            {
              "authorId": "3683725",
              "name": "S. Bunyaratavej"
            },
            {
              "authorId": "1398734807",
              "name": "N. Silpa‐archa"
            },
            {
              "authorId": "5368423",
              "name": "Chayada Chaiyabutr"
            },
            {
              "authorId": "3113866",
              "name": "Thanapon Noraset"
            },
            {
              "authorId": "2223916543",
              "name": "Teerapat Paringkarn"
            },
            {
              "authorId": "2248018111",
              "name": "Thrit Hutachoke"
            },
            {
              "authorId": "2315911050",
              "name": "Prameyuda Watchirakaeyoon"
            },
            {
              "authorId": "2224031894",
              "name": "Pantaree Kobkurkul"
            },
            {
              "authorId": "2350104734",
              "name": "Sirin Apichonbancha"
            },
            {
              "authorId": "7744278",
              "name": "P. Chiowchanwisawakit"
            }
          ]
        }
      },
      {
        "contexts": [
          "Recognizing that standard MLPs in FFNs are limited in capturing local context [8], [9], we explicitly inject convolutional inductive biases into the Transformer block."
        ],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "4bb03b3a51299337b4c2769d8e3a57baf99b9e55",
          "title": "HBFormer: A Hybrid-Bridge Transformer for Microtumor and Miniature Organ Segmentation",
          "year": 2025,
          "authors": [
            {
              "authorId": "2319449329",
              "name": "Fuchen Zheng"
            },
            {
              "authorId": "2301414126",
              "name": "Xinyi Chen"
            },
            {
              "authorId": "2320809929",
              "name": "Weixuan Li"
            },
            {
              "authorId": "2321144703",
              "name": "Quanjun Li"
            },
            {
              "authorId": "2352304047",
              "name": "Junhua Zhou"
            },
            {
              "authorId": "2311993504",
              "name": "Xiaojiao Guo"
            },
            {
              "authorId": "2328314044",
              "name": "Xuhang Chen"
            },
            {
              "authorId": "2276794526",
              "name": "Chi-Man Pun"
            },
            {
              "authorId": "2320818244",
              "name": "Shoujun Zhou"
            }
          ]
        }
      },
      {
        "contexts": [],
        "intents": [],
        "isInfluential": false,
        "citingPaper": {
          "paperId": "298434eb61f424841a8d8995d2f8979ccde9a54d",
          "title": "Background noise suppression for advanced fine-grained visual classification",
          "year": 2025,
          "authors": [
            {
              "authorId": "2396984855",
              "name": "Zhi-Gang Wang"
            },
            {
              "authorId": "2206509724",
              "name": "Siqi Chen"
            },
            {
              "authorId": "2269222268",
              "name": "Bin Luo"
            }
          ]
        }
      }
    ]
  }
]