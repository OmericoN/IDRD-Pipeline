[
  {
    "paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "year": 2019,
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1810.04805, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "39172707",
        "name": "Jacob Devlin",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1744179",
        "name": "Ming-Wei Chang",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2544107",
        "name": "Kenton Lee",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "3259253",
        "name": "Kristina Toutanova",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "A new language representation model, BERT, designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers, which can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks."
  },
  {
    "paperId": "ad7ddcc14984caae308c397f1a589aae75d4ab71",
    "title": "Training data-efficient image transformers & distillation through attention",
    "year": 2020,
    "abstract": "Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. However, these visual transformers are pre-trained with hundreds of millions of images using an expensive infrastructure, thereby limiting their adoption. In this work, we produce a competitive convolution-free transformer by training on Imagenet only. We train them on a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop evaluation) on ImageNet with no external data. More importantly, we introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention. We show the interest of this token-based distillation, especially when using a convnet as a teacher. This leads us to report results competitive with convnets for both Imagenet (where we obtain up to 85.2% accuracy) and when transferring to other tasks. We share our code and models.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2012.12877, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "2113243762",
        "name": "Hugo Touvron",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "51021910",
        "name": "M. Cord",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "3271933",
        "name": "Matthijs Douze",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1403239967",
        "name": "Francisco Massa",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "3469062",
        "name": "Alexandre Sablayrolles",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2065248680",
        "name": "Herv'e J'egou",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "This work produces a competitive convolution-free transformer by training on Imagenet only and introduces a teacher-student strategy specific to transformers that relies on a distillation token ensuring that the student learns from the teacher through attention."
  },
  {
    "paperId": "e3d7778a47c6cab4ea1ef3ee9d19ec1510c15c60",
    "title": "SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers",
    "year": 2021,
    "abstract": "We present SegFormer, a simple, efficient yet powerful semantic segmentation framework which unifies Transformers with lightweight multilayer perception (MLP) decoders. SegFormer has two appealing features: 1) SegFormer comprises a novel hierarchically structured Transformer encoder which outputs multiscale features. It does not need positional encoding, thereby avoiding the interpolation of positional codes which leads to decreased performance when the testing resolution differs from training. 2) SegFormer avoids complex decoders. The proposed MLP decoder aggregates information from different layers, and thus combining both local attention and global attention to render powerful representations. We show that this simple and lightweight design is the key to efficient segmentation on Transformers. We scale our approach up to obtain a series of models from SegFormer-B0 to SegFormer-B5, reaching significantly better performance and efficiency than previous counterparts. For example, SegFormer-B4 achieves 50.3% mIoU on ADE20K with 64M parameters, being 5x smaller and 2.2% better than the previous best method. Our best model, SegFormer-B5, achieves 84.0% mIoU on Cityscapes validation set and shows excellent zero-shot robustness on Cityscapes-C. Code will be released at: github.com/NVlabs/SegFormer.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2105.15203, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "41020000",
        "name": "Enze Xie",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "71074736",
        "name": "Wenhai Wang",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1751019",
        "name": "Zhiding Yu",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2047844",
        "name": "Anima Anandkumar",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2974008",
        "name": "J. Álvarez",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "47571885",
        "name": "Ping Luo",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "SegFormer is presented, a simple, efficient yet powerful semantic segmentation framework which unifies Transformers with lightweight multilayer perception (MLP) decoders and shows excellent zero-shot robustness on Cityscapes-C."
  },
  {
    "paperId": "24b8a0b02bcb7934967757fc59d273a71ba67e30",
    "title": "TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation",
    "year": 2021,
    "abstract": "Medical image segmentation is an essential prerequisite for developing healthcare systems, especially for disease diagnosis and treatment planning. On various medical image segmentation tasks, the u-shaped architecture, also known as U-Net, has become the de-facto standard and achieved tremendous success. However, due to the intrinsic locality of convolution operations, U-Net generally demonstrates limitations in explicitly modeling long-range dependency. Transformers, designed for sequence-to-sequence prediction, have emerged as alternative architectures with innate global self-attention mechanisms, but can result in limited localization abilities due to insufficient low-level details. In this paper, we propose TransUNet, which merits both Transformers and U-Net, as a strong alternative for medical image segmentation. On one hand, the Transformer encodes tokenized image patches from a convolution neural network (CNN) feature map as the input sequence for extracting global contexts. On the other hand, the decoder upsamples the encoded features which are then combined with the high-resolution CNN feature maps to enable precise localization. We argue that Transformers can serve as strong encoders for medical image segmentation tasks, with the combination of U-Net to enhance finer details by recovering localized spatial information. TransUNet achieves superior performances to various competing methods on different medical applications including multi-organ segmentation and cardiac segmentation. Code and models are available at https://github.com/Beckschen/TransUNet.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2102.04306, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "90972805",
        "name": "Jieneng Chen",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2946371",
        "name": "Yongyi Lu",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2156559",
        "name": "Qihang Yu",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1932191260",
        "name": "Xiangde Luo",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2397747903",
        "name": "Ehsan Adeli",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2152539822",
        "name": "Yan Wang",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "50706692",
        "name": "Le Lu",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "145081362",
        "name": "A. Yuille",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "7743268",
        "name": "Yuyin Zhou",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "It is argued that Transformers can serve as strong encoders for medical image segmentation tasks, with the combination of U-Net to enhance finer details by recovering localized spatial information."
  },
  {
    "paperId": "ad4a0938c48e61b7827869e4ac3baffd0aefab35",
    "title": "Emerging Properties in Self-Supervised Vision Transformers",
    "year": 2021,
    "abstract": "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) [16] that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder [26], multi-crop training [9], and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2104.14294",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2104.14294, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "2062862676",
        "name": "Mathilde Caron",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2113243762",
        "name": "Hugo Touvron",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1806773",
        "name": "Ishan Misra",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2065248680",
        "name": "Herv'e J'egou",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2599292",
        "name": "J. Mairal",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2329288",
        "name": "Piotr Bojanowski",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2319608",
        "name": "Armand Joulin",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "This paper questions if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets) and implements DINO, a form of self-distillation with no labels, which implements the synergy between DINO and ViTs."
  },
  {
    "paperId": "962dc29fdc3fbdc5930a10aba114050b82fe5a3e",
    "title": "End-to-End Object Detection with Transformers",
    "year": 2020,
    "abstract": "We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at this https URL.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2005.12872, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "3422899",
        "name": "Nicolas Carion",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1403239967",
        "name": "Francisco Massa",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2282478",
        "name": "Gabriel Synnaeve",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1746841",
        "name": "Nicolas Usunier",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "144843400",
        "name": "Alexander Kirillov",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2134433",
        "name": "Sergey Zagoruyko",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "This work presents a new method that views object detection as a direct set prediction problem, and demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset."
  },
  {
    "paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a",
    "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
    "year": 2020,
    "abstract": "While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2010.11929, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "2841331",
        "name": "Alexey Dosovitskiy",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "39611591",
        "name": "Lucas Beyer",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "144629422",
        "name": "Alexander Kolesnikov",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "3319373",
        "name": "Dirk Weissenborn",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2743563",
        "name": "Xiaohua Zhai",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2465270",
        "name": "Thomas Unterthiner",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2274215058",
        "name": "Mostafa Dehghani",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "46352821",
        "name": "M. Minderer",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2280399",
        "name": "G. Heigold",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1802148",
        "name": "S. Gelly",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "39328010",
        "name": "Jakob Uszkoreit",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2815290",
        "name": "N. Houlsby",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train."
  },
  {
    "paperId": "736973165f98105fec3729b7db414ae4d80fcbeb",
    "title": "Scalable Diffusion Models with Transformers",
    "year": 2022,
    "abstract": "We explore a new class of diffusion models based on the transformer architecture. We train latent diffusion models of images, replacing the commonly-used U-Net backbone with a transformer that operates on latent patches. We analyze the scalability of our Diffusion Transformers (DiTs) through the lens of forward pass complexity as measured by Gflops. We find that DiTs with higher Gflops—through increased transformer depth/width or increased number of input tokens—consistently have lower FID. In addition to possessing good scalability properties, our largest DiT-XL/2 models outperform all prior diffusion models on the class-conditional ImageNet 512×512 and 256×256 benchmarks, achieving a state-of-the-art FID of 2.27 on the latter.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2212.09748",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2212.09748, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "35235273",
        "name": "William S. Peebles",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1817030",
        "name": "Saining Xie",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "A new class of diffusion models based on the transformer architecture is explored, replacing the commonly-used U-Net backbone with a transformer that operates on latent patches that outperform all prior diffusion models on the class-conditional ImageNet 512×512 and 256×256 benchmarks."
  },
  {
    "paperId": "39ca8f8ff28cc640e3b41a6bd7814ab85c586504",
    "title": "Deformable DETR: Deformable Transformers for End-to-End Object Detection",
    "year": 2020,
    "abstract": "DETR has been recently proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance. However, it suffers from slow convergence and limited feature spatial resolution, due to the limitation of Transformer attention modules in processing image feature maps. To mitigate these issues, we proposed Deformable DETR, whose attention modules only attend to a small set of key sampling points around a reference. Deformable DETR can achieve better performance than DETR (especially on small objects) with 10$\\times$ less training epochs. Extensive experiments on the COCO benchmark demonstrate the effectiveness of our approach. Code shall be released.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2010.04159, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "2578924",
        "name": "Xizhou Zhu",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "145499378",
        "name": "Weijie Su",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "152309485",
        "name": "Lewei Lu",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2183101614",
        "name": "Bin Li",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "93768810",
        "name": "Xiaogang Wang",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "3304536",
        "name": "Jifeng Dai",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "Deformable DETR, whose attention modules only attend to a small set of key sampling points around a reference, can achieve better performance than DETR (especially on small objects) with 10$\\times less training epochs."
  },
  {
    "paperId": "47f7ec3d0a5e6e83b6768ece35206a94dc81919c",
    "title": "Taming Transformers for High-Resolution Image Synthesis",
    "year": 2020,
    "abstract": "Designed to learn long-range interactions on sequential data, transformers continue to show state-of-the-art results on a wide variety of tasks. In contrast to CNNs, they contain no inductive bias that prioritizes local interactions. This makes them expressive, but also computationally infeasible for long sequences, such as high-resolution images. We demonstrate how combining the effectiveness of the inductive bias of CNNs with the expressivity of transformers enables them to model and thereby synthesize high-resolution images. We show how to (i) use CNNs to learn a context-rich vocabulary of image constituents, and in turn (ii) utilize transformers to efficiently model their composition within high-resolution images. Our approach is readily applied to conditional synthesis tasks, where both non-spatial information, such as object classes, and spatial information, such as segmentations, can control the generated image. In particular, we present the first results on semantically-guided synthesis of megapixel images with transformers. Project page at https://git.io/JLlvY.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2012.09841",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2012.09841, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "35175531",
        "name": "Patrick Esser",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1660819540",
        "name": "Robin Rombach",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1796707",
        "name": "B. Ommer",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "It is demonstrated how combining the effectiveness of the inductive bias of CNNs with the expressivity of transformers enables them to model and thereby synthesize high-resolution images."
  },
  {
    "paperId": "fc46ccb83dc121c33de7ab6bdedab7d970780b2f",
    "title": "Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting",
    "year": 2021,
    "abstract": "Extending the forecasting time is a critical demand for real applications, such as extreme weather early warning and long-term energy consumption planning. This paper studies the long-term forecasting problem of time series. Prior Transformer-based models adopt various self-attention mechanisms to discover the long-range dependencies. However, intricate temporal patterns of the long-term future prohibit the model from finding reliable dependencies. Also, Transformers have to adopt the sparse versions of point-wise self-attentions for long series efficiency, resulting in the information utilization bottleneck. Going beyond Transformers, we design Autoformer as a novel decomposition architecture with an Auto-Correlation mechanism. We break with the pre-processing convention of series decomposition and renovate it as a basic inner block of deep models. This design empowers Autoformer with progressive decomposition capacities for complex time series. Further, inspired by the stochastic process theory, we design the Auto-Correlation mechanism based on the series periodicity, which conducts the dependencies discovery and representation aggregation at the sub-series level. Auto-Correlation outperforms self-attention in both efficiency and accuracy. In long-term forecasting, Autoformer yields state-of-the-art accuracy, with a 38% relative improvement on six benchmarks, covering five practical applications: energy, traffic, economics, weather and disease. Code is available at this repository: \\url{https://github.com/thuml/Autoformer}.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2106.13008, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "2051867856",
        "name": "Haixu Wu",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2111064536",
        "name": "Jiehui Xu",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2144499343",
        "name": "Jianmin Wang",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2054275000",
        "name": "Mingsheng Long",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "This paper designs Autoformer as a novel decomposition architecture with an Auto-Correlation mechanism based on the series periodicity, which conducts the dependencies discovery and representation aggregation at the sub-series level and yields state-of-the-art accuracy in long-term forecasting."
  },
  {
    "paperId": "41a66997ce0a366bba3becf7c3f37c9aebb13fbd",
    "title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis",
    "year": 2024,
    "abstract": "Diffusion models create data from noise by inverting the forward paths of data towards noise and have emerged as a powerful generative modeling technique for high-dimensional, perceptual data such as images and videos. Rectified flow is a recent generative model formulation that connects data and noise in a straight line. Despite its better theoretical properties and conceptual simplicity, it is not yet decisively established as standard practice. In this work, we improve existing noise sampling techniques for training rectified flow models by biasing them towards perceptually relevant scales. Through a large-scale study, we demonstrate the superior performance of this approach compared to established diffusion formulations for high-resolution text-to-image synthesis. Additionally, we present a novel transformer-based architecture for text-to-image generation that uses separate weights for the two modalities and enables a bidirectional flow of information between image and text tokens, improving text comprehension, typography, and human preference ratings. We demonstrate that this architecture follows predictable scaling trends and correlates lower validation loss to improved text-to-image synthesis as measured by various metrics and human evaluations. Our largest models outperform state-of-the-art models, and we will make our experimental data, code, and model weights publicly available.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.03206, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "35175531",
        "name": "Patrick Esser",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "3411322",
        "name": "Sumith Kulal",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "119843260",
        "name": "A. Blattmann",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2316859494",
        "name": "Rahim Entezari",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2188737195",
        "name": "Jonas Muller",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2289994508",
        "name": "Harry Saini",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2290013499",
        "name": "Yam Levi",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2053482699",
        "name": "Dominik Lorenz",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "40562186",
        "name": "Axel Sauer",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2290014125",
        "name": "Frederic Boesel",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2221125727",
        "name": "Dustin Podell",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "102541178",
        "name": "Tim Dockhorn",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2221127565",
        "name": "Zion English",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2221126982",
        "name": "Kyle Lacey",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2290014122",
        "name": "Alex Goodwin",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2290014387",
        "name": "Yannik Marek",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1660819540",
        "name": "Robin Rombach",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "This work improves existing noise sampling techniques for training rectified flow models by biasing them towards perceptually relevant scales and presents a novel transformer-based architecture for text-to-image generation that uses separate weights for the two modalities and enables a bidirectional flow of information between image and text tokens."
  },
  {
    "paperId": "722ad6ac92286507437b31486f47987d6ece05c9",
    "title": "BEiT: BERT Pre-Training of Image Transformers",
    "year": 2021,
    "abstract": "We introduce a self-supervised vision representation model BEiT, which stands for Bidirectional Encoder representation from Image Transformers. Following BERT developed in the natural language processing area, we propose a masked image modeling task to pretrain vision Transformers. Specifically, each image has two views in our pre-training, i.e, image patches (such as 16x16 pixels), and visual tokens (i.e., discrete tokens). We first\"tokenize\"the original image into visual tokens. Then we randomly mask some image patches and fed them into the backbone Transformer. The pre-training objective is to recover the original visual tokens based on the corrupted image patches. After pre-training BEiT, we directly fine-tune the model parameters on downstream tasks by appending task layers upon the pretrained encoder. Experimental results on image classification and semantic segmentation show that our model achieves competitive results with previous pre-training methods. For example, base-size BEiT achieves 83.2% top-1 accuracy on ImageNet-1K, significantly outperforming from-scratch DeiT training (81.8%) with the same setup. Moreover, large-size BEiT obtains 86.3% only using ImageNet-1K, even outperforming ViT-L with supervised pre-training on ImageNet-22K (85.2%). The code and pretrained models are available at https://aka.ms/beit.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2106.08254, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "10699417",
        "name": "Hangbo Bao",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "145307652",
        "name": "Li Dong",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "49807919",
        "name": "Furu Wei",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "A self-supervised vision representation model BEiT, which stands for Bidirectional Encoder representation from Image Transformers, is introduced, and results on image classification and semantic segmentation show that the model achieves competitive results with previous pre-training methods."
  },
  {
    "paperId": "5f404dbba07619cc7f28d75d03f124a52290046e",
    "title": "Are Transformers Effective for Time Series Forecasting?",
    "year": 2022,
    "abstract": "Recently, there has been a surge of Transformer-based solutions for the long-term time series forecasting (LTSF) task. Despite the growing performance over the past few years, we question the validity of this line of research in this work. Specifically, Transformers is arguably the most successful solution to extract the semantic correlations among the elements in a long sequence. However, in time series modeling, we are to extract the temporal relations in an ordered set of continuous points. While employing positional encoding and using tokens to embed sub-series in Transformers facilitate preserving some ordering information, the nature of the permutation-invariant self-attention mechanism inevitably results in temporal information loss. \nTo validate our claim, we introduce a set of embarrassingly simple one-layer linear models named LTSF-Linear for comparison. Experimental results on nine real-life datasets show that LTSF-Linear surprisingly outperforms existing sophisticated Transformer-based LTSF models in all cases, and often by a large margin. Moreover, we conduct comprehensive empirical studies to explore the impacts of various design elements of LTSF models on their temporal relation extraction capability. We hope this surprising finding opens up new research directions for the LTSF task. We also advocate revisiting the validity of Transformer-based solutions for other time series analysis tasks (e.g., anomaly detection) in the future.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2205.13504",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2205.13504, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "51286000",
        "name": "Ailing Zeng",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "122004634",
        "name": "Mu-Hwa Chen",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "47058944",
        "name": "L. Zhang",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2149106517",
        "name": "Qiang Xu",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "Experimental results on nine real-life datasets show that LTSF-Linear surprisingly outperforms existing sophisticated Transformer-based L TSF models in all cases, and often by a large margin."
  },
  {
    "paperId": "c95383f251a62c63217586059c67f63507c3e839",
    "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing",
    "year": 2019,
    "abstract": "Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. \\textit{Transformers} is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. \\textit{Transformers} is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at \\url{this https URL}.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1910.03771, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "2257007291",
        "name": "Thomas Wolf",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1380459402",
        "name": "Lysandre Debut",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "51918868",
        "name": "Victor Sanh",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "40811585",
        "name": "Julien Chaumond",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "40899333",
        "name": "Clement Delangue",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1382164294",
        "name": "Anthony Moi",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1382164165",
        "name": "Pierric Cistac",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1382164170",
        "name": "Tim Rault",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2185329",
        "name": "Rémi Louf",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "97662964",
        "name": "Morgan Funtowicz",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "48776237",
        "name": "Joe Davison",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "88728159",
        "name": "Sam Shleifer",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "138609838",
        "name": "Patrick von Platen",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2257128341",
        "name": "Clara Ma",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2268491803",
        "name": "Yacine Jernite",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "3008389",
        "name": "J. Plu",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2257127518",
        "name": "Canwen Xu",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1379806208",
        "name": "Teven Le Scao",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "103682620",
        "name": "Sylvain Gugger",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2125818054",
        "name": "Mariama Drame",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2113836945",
        "name": "Quentin Lhoest",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2260132137",
        "name": "Alexander M. Rush",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "The \\textit{Transformers} library is an open-source library that consists of carefully engineered state-of-the art Transformer architectures under a unified API and a curated collection of pretrained models made by and available for the community."
  },
  {
    "paperId": "739ceacfafb1c4eaa17509351b647c773270b3ae",
    "title": "An Empirical Study of Training Self-Supervised Vision Transformers",
    "year": 2021,
    "abstract": "This paper does not describe a novel method. Instead, it studies a straightforward, incremental, yet must-know baseline given the recent progress in computer vision: self-supervised learning for Vision Transformers (ViT). While the training recipes for standard convolutional networks have been highly mature and robust, the recipes for ViT are yet to be built, especially in the self-supervised scenarios where training becomes more challenging. In this work, we go back to basics and investigate the effects of several fundamental components for training self-supervised ViT. We observe that instability is a major issue that degrades accuracy, and it can be hidden by apparently good results. We reveal that these results are indeed partial failure, and they can be improved when training is made more stable. We benchmark ViT results in MoCo v3 and several other self-supervised frameworks, with ablations in various aspects. We discuss the currently positive evidence as well as challenges and open questions. We hope that this work will provide useful data points and experience for future research.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2104.02057",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2104.02057, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "39717886",
        "name": "Xinlei Chen",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1817030",
        "name": "Saining Xie",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2058350112",
        "name": "Kaiming He",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "This work investigates the effects of several fundamental components for training self-supervised ViT, and reveals that these results are indeed partial failure, and they can be improved when training is made more stable."
  },
  {
    "paperId": "8e33914d6051dd031a5e096962b9398fc1d16067",
    "title": "Vision Transformers for Dense Prediction",
    "year": 2021,
    "abstract": "We introduce dense prediction transformers, an architecture that leverages vision transformers in place of convolutional networks as a backbone for dense prediction tasks. We assemble tokens from various stages of the vision transformer into image-like representations at various resolutions and progressively combine them into full-resolution predictions using a convolutional decoder. The transformer backbone processes representations at a constant and relatively high resolution and has a global receptive field at every stage. These properties allow the dense prediction transformer to provide finer-grained and more globally coherent predictions when compared to fully-convolutional networks. Our experiments show that this architecture yields substantial improvements on dense prediction tasks, especially when a large amount of training data is available. For monocular depth estimation, we observe an improvement of up to 28% in relative performance when compared to a state-of-the-art fully-convolutional network. When applied to semantic segmentation, dense prediction transformers set a new state of the art on ADE20K with 49.02% mIoU. We further show that the architecture can be fine-tuned on smaller datasets such as NYUv2, KITTI, and Pascal Context where it also sets the new state of the art. Our models are available at https://github.com/intel-isl/DPT.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2103.13413",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2103.13413, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "2774325",
        "name": "René Ranftl",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1651204675",
        "name": "Alexey Bochkovskiy",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "145231047",
        "name": "V. Koltun",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "D dense prediction transformers, an architecture that leverages vision transformers in place of convolutional networks as a backbone for dense prediction tasks, can be fine-tuned on smaller datasets such as NYUv2, KITTI, and Pascal Context where it also sets the new state of the art."
  },
  {
    "paperId": "7519a1e9e7371df79bd8a21cee871feb0ec597a5",
    "title": "UNETR: Transformers for 3D Medical Image Segmentation",
    "year": 2021,
    "abstract": "Fully Convolutional Neural Networks (FCNNs) with contracting and expanding paths have shown prominence for the majority of medical image segmentation applications since the past decade. In FCNNs, the encoder plays an integral role by learning both global and local features and contextual representations which can be utilized for semantic output prediction by the decoder. Despite their success, the locality of convolutional layers in FCNNs, limits the capability of learning long-range spatial dependencies. Inspired by the recent success of transformers for Natural Language Processing (NLP) in long-range sequence learning, we reformulate the task of volumetric (3D) medical image segmentation as a sequence-to-sequence prediction problem. We introduce a novel architecture, dubbed as UNEt TRansformers (UNETR), that utilizes a transformer as the encoder to learn sequence representations of the input volume and effectively capture the global multi-scale information, while also following the successful \"U-shaped\" network design for the encoder and decoder. The transformer encoder is directly connected to a decoder via skip connections at different resolutions to compute the final semantic segmentation output. We have validated the performance of our method on the Multi Atlas Labeling Beyond The Cranial Vault (BTCV) dataset for multi-organ segmentation and the Medical Segmentation Decathlon (MSD) dataset for brain tumor and spleen segmentation tasks. Our benchmarks demonstrate new state-of-the-art performance on the BTCV leaderboard.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2103.10504",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2103.10504, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "31374559",
        "name": "Ali Hatamizadeh",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "144041873",
        "name": "Dong Yang",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "144531567",
        "name": "H. Roth",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "3262394",
        "name": "Daguang Xu",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "This work reformulates the task of volumetric (3D) medical image segmentation as a sequence-to-sequence prediction problem and introduces a novel architecture, dubbed as UNEt TRansformers (UNETR), that utilizes a transformer as the encoder to learn sequence representations of the input volume and effectively capture the global multi-scale information."
  },
  {
    "paperId": "3a906b77fa218adc171fecb28bb81c24c14dcc7b",
    "title": "Transformers in Vision: A Survey",
    "year": 2021,
    "abstract": "Astounding results from Transformer models on natural language tasks have intrigued the vision community to study their application to computer vision problems. Among their salient benefits, Transformers enable modeling long dependencies between input sequence elements and support parallel processing of sequence as compared to recurrent networks, e.g., Long short-term memory. Different from convolutional networks, Transformers require minimal inductive biases for their design and are naturally suited as set-functions. Furthermore, the straightforward design of Transformers allows processing multiple modalities (e.g., images, videos, text, and speech) using similar processing blocks and demonstrates excellent scalability to very large capacity networks and huge datasets. These strengths have led to exciting progress on a number of vision tasks using Transformer networks. This survey aims to provide a comprehensive overview of the Transformer models in the computer vision discipline. We start with an introduction to fundamental concepts behind the success of Transformers, i.e., self-attention, large-scale pre-training, and bidirectional feature encoding. We then cover extensive applications of transformers in vision including popular recognition tasks (e.g., image classification, object detection, action recognition, and segmentation), generative modeling, multi-modal tasks (e.g., visual-question answering, visual reasoning, and visual grounding), video processing (e.g., activity recognition, video forecasting), low-level vision (e.g., image super-resolution, image enhancement, and colorization), and three-dimensional analysis (e.g., point cloud classification and segmentation). We compare the respective advantages and limitations of popular techniques both in terms of architectural design and their experimental value. Finally, we provide an analysis on open research directions and possible future works. We hope this effort will ignite further interest in the community to solve current challenges toward the application of transformer models in computer vision.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2101.01169",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2101.01169, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "152973423",
        "name": "Salman Hameed Khan",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "40894826",
        "name": "Muzammal Naseer",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "145684318",
        "name": "Munawar Hayat",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "3323621",
        "name": "Syed Waqas Zamir",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2358803",
        "name": "F. Khan",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "145103012",
        "name": "M. Shah",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "This survey aims to provide a comprehensive overview of the Transformer models in the computer vision discipline with an introduction to fundamental concepts behind the success of Transformers, i.e., self-attention, large-scale pre-training, and bidirectional feature encoding."
  },
  {
    "paperId": "dbe077f8521ecbe0a1477d6148c726d4f053d9c9",
    "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet",
    "year": 2021,
    "abstract": "Transformers, which are popular for language modeling, have been explored for solving vision tasks recently, e.g., the Vision Transformer (ViT) for image classification. The ViT model splits each image into a sequence of tokens with fixed length and then applies multiple Transformer layers to model their global relation for classification. However, ViT achieves inferior performance to CNNs when trained from scratch on a midsize dataset like ImageNet. We find it is because: 1) the simple tokenization of input images fails to model the important local structure such as edges and lines among neighboring pixels, leading to low training sample efficiency; 2) the redundant attention backbone design of ViT leads to limited feature richness for fixed computation budgets and limited training samples. To overcome such limitations, we propose a new Tokens-To-Token Vision Transformer (T2T-VTT), which incorporates 1) a layer-wise Tokens-to-Token (T2T) transformation to progressively structurize the image to tokens by recursively aggregating neighboring Tokens into one Token (Tokens-to-Token), such that local structure represented by surrounding tokens can be modeled and tokens length can be reduced; 2) an efficient backbone with a deep-narrow structure for vision transformer motivated by CNN architecture design after empirical study. Notably, T2T-ViT reduces the parameter count and MACs of vanilla ViT by half, while achieving more than 3.0% improvement when trained from scratch on ImageNet. It also outperforms ResNets and achieves comparable performance with MobileNets by directly training on ImageNet. For example, T2T-ViT with comparable size to ResNet50 (21.5M parameters) can achieve 83.3% top1 accuracy in image resolution 384x384 on ImageNet.1",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2101.11986",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2101.11986, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "2087091296",
        "name": "Li Yuan",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2144861793",
        "name": "Yunpeng Chen",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": null,
        "name": "Tao Wang",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "23476952",
        "name": "Weihao Yu",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "145356288",
        "name": "Yujun Shi",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "40983412",
        "name": "Francis E. H. Tay",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "33221685",
        "name": "Jiashi Feng",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "143653681",
        "name": "Shuicheng Yan",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "A new Tokens-To-Token Vision Transformer (T2T-VTT), which incorporates an efficient backbone with a deep-narrow structure for vision transformer motivated by CNN architecture design after empirical study and reduces the parameter count and MACs of vanilla ViT by half."
  },
  {
    "paperId": "e775e649d815a02373eac840cf5e33a04ff85c95",
    "title": "CvT: Introducing Convolutions to Vision Transformers",
    "year": 2021,
    "abstract": "We present in this paper a new architecture, named Convolutional vision Transformer (CvT), that improves Vision Transformer (ViT) in performance and efficiency by introducing convolutions into ViT to yield the best of both de-signs. This is accomplished through two primary modifications: a hierarchy of Transformers containing a new convolutional token embedding, and a convolutional Transformer block leveraging a convolutional projection. These changes introduce desirable properties of convolutional neural networks (CNNs) to the ViT architecture (i.e. shift, scale, and distortion invariance) while maintaining the merits of Transformers (i.e. dynamic attention, global context, and better generalization). We validate CvT by conducting extensive experiments, showing that this approach achieves state-of-the-art performance over other Vision Transformers and ResNets on ImageNet-1k, with fewer parameters and lower FLOPs. In addition, performance gains are maintained when pretrained on larger datasets (e.g. ImageNet-22k) and fine-tuned to downstream tasks. Pretrained on ImageNet-22k, our CvT-W24 obtains a top-1 accuracy of 87.7% on the ImageNet-1k val set. Finally, our results show that the positional encoding, a crucial component in existing Vision Transformers, can be safely re-moved in our model, simplifying the design for higher resolution vision tasks. Code will be released at https://github.com/microsoft/CvT.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2103.15808",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2103.15808, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "2119019500",
        "name": "Haiping Wu",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2054421528",
        "name": "Bin Xiao",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "40589056",
        "name": "N. Codella",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2152968847",
        "name": "Mengchen Liu",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "3386593",
        "name": "Xiyang Dai",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "145347147",
        "name": "Lu Yuan",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2152828578",
        "name": "Lei Zhang",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "A new architecture is presented that improves Vision Transformer (ViT) in performance and efficiency by introducing convolutions into ViT to yield the best of both de-signs, and the positional encoding, a crucial component in existing Vision Transformers, can be safely re-moved in this model."
  },
  {
    "paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3",
    "title": "Big Bird: Transformers for Longer Sequences",
    "year": 2020,
    "abstract": "Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having $O(1)$ global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2007.14062, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "1771307",
        "name": "M. Zaheer",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1947314",
        "name": "Guru Guruganesh",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "89890133",
        "name": "Kumar Avinava Dubey",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1643737606",
        "name": "J. Ainslie",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "114577307",
        "name": "Chris Alberti",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1722671",
        "name": "Santiago Ontañón",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "38552691",
        "name": "Philip Pham",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "101210026",
        "name": "Anirudh Ravula",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2401629861",
        "name": "Qifan Wang",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "113906155",
        "name": "Li Yang",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "143629707",
        "name": "Amr Ahmed",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "It is shown that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model."
  },
  {
    "paperId": "dad15404d372a23b4b3bf9a63b3124693df3c85e",
    "title": "A Time Series is Worth 64 Words: Long-term Forecasting with Transformers",
    "year": 2022,
    "abstract": "We propose an efficient design of Transformer-based models for multivariate time series forecasting and self-supervised representation learning. It is based on two key components: (i) segmentation of time series into subseries-level patches which are served as input tokens to Transformer; (ii) channel-independence where each channel contains a single univariate time series that shares the same embedding and Transformer weights across all the series. Patching design naturally has three-fold benefit: local semantic information is retained in the embedding; computation and memory usage of the attention maps are quadratically reduced given the same look-back window; and the model can attend longer history. Our channel-independent patch time series Transformer (PatchTST) can improve the long-term forecasting accuracy significantly when compared with that of SOTA Transformer-based models. We also apply our model to self-supervised pre-training tasks and attain excellent fine-tuning performance, which outperforms supervised training on large datasets. Transferring of masked pre-trained representation on one dataset to others also produces SOTA forecasting accuracy. Code is available at: https://github.com/yuqinie98/PatchTST.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2211.14730",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2211.14730, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "152972535",
        "name": "Yuqi Nie",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "144547425",
        "name": "Nam H. Nguyen",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "40913517",
        "name": "Phanwadee Sinthong",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1682581",
        "name": "J. Kalagnanam",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "The channel-independent patch time series Transformer (PatchTST) can improve the long-term forecasting accuracy significantly when compared with that of SOTA Transformer-based models and applies to self-supervised pre-training tasks and attain excellent fine-tuning performance."
  },
  {
    "paperId": "6f68e1bb253925d8431588555d3010419f322e04",
    "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention",
    "year": 2020,
    "abstract": "Transformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input's length, they are prohibitively slow for very long sequences. To address this limitation, we express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from $\\mathcal{O}\\left(N^2\\right)$ to $\\mathcal{O}\\left(N\\right)$, where $N$ is the sequence length. We show that this formulation permits an iterative implementation that dramatically accelerates autoregressive transformers and reveals their relationship to recurrent neural networks. Our linear transformers achieve similar performance to vanilla transformers and they are up to 4000x faster on autoregressive prediction of very long sequences.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2006.16236, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "3493855",
        "name": "Angelos Katharopoulos",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2992087",
        "name": "Apoorv Vyas",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "143958923",
        "name": "Nikolaos Pappas",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "116272138",
        "name": "Franccois Fleuret",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "This work expresses the self-attention as a linear dot-product of kernel feature maps and makes use of the associativity property of matrix products to reduce the complexity from O(N) to N, where N is the sequence length."
  },
  {
    "paperId": "ca9f5b3bf0f54ad97513e6175b30497873670fed",
    "title": "Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality",
    "year": 2024,
    "abstract": "While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.21060, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "2269146652",
        "name": "Tri Dao",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2269161650",
        "name": "Albert Gu",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "The state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling."
  },
  {
    "paperId": "79c93274429d6355959f1e4374c2147bb81ea649",
    "title": "LXMERT: Learning Cross-Modality Encoder Representations from Transformers",
    "year": 2019,
    "abstract": "Vision-and-language reasoning requires an understanding of visual concepts, language semantics, and, most importantly, the alignment and relationships between these two modalities. We thus propose the LXMERT (Learning Cross-Modality Encoder Representations from Transformers) framework to learn these vision-and-language connections. In LXMERT, we build a large-scale Transformer model that consists of three encoders: an object relationship encoder, a language encoder, and a cross-modality encoder. Next, to endow our model with the capability of connecting vision and language semantics, we pre-train the model with large amounts of image-and-sentence pairs, via five diverse representative pre-training tasks: masked language modeling, masked object prediction (feature regression and label classification), cross-modality matching, and image question answering. These tasks help in learning both intra-modality and cross-modality relationships. After fine-tuning from our pre-trained parameters, our model achieves the state-of-the-art results on two visual question answering datasets (i.e., VQA and GQA). We also show the generalizability of our pre-trained cross-modality model by adapting it to a challenging visual-reasoning task, NLVR2, and improve the previous best result by 22% absolute (54% to 76%). Lastly, we demonstrate detailed ablation studies to prove that both our novel model components and pre-training strategies significantly contribute to our strong results. Code and pre-trained models publicly available at: https://github.com/airsplay/lxmert",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "https://www.aclweb.org/anthology/D19-1514.pdf",
      "status": "HYBRID",
      "license": "CCBY",
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1908.07490, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "3218666",
        "name": "Hao Hao Tan",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "143977268",
        "name": "Mohit Bansal",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "The LXMERT (Learning Cross-Modality Encoder Representations from Transformers) framework, a large-scale Transformer model that consists of three encoders, achieves the state-of-the-art results on two visual question answering datasets and shows the generalizability of the pre-trained cross-modality model."
  },
  {
    "paperId": "fdacf2a732f55befdc410ea927091cad3b791f13",
    "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity",
    "year": 2021,
    "abstract": "In deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) defies this and instead selects different parameters for each incoming example. The result is a sparsely-activated model -- with outrageous numbers of parameters -- but a constant computational cost. However, despite several notable successes of MoE, widespread adoption has been hindered by complexity, communication costs and training instability -- we address these with the Switch Transformer. We simplify the MoE routing algorithm and design intuitive improved models with reduced communication and computational costs. Our proposed training techniques help wrangle the instabilities and we show large sparse models may be trained, for the first time, with lower precision (bfloat16) formats. We design models based off T5-Base and T5-Large to obtain up to 7x increases in pre-training speed with the same computational resources. These improvements extend into multilingual settings where we measure gains over the mT5-Base version across all 101 languages. Finally, we advance the current scale of language models by pre-training up to trillion parameter models on the\"Colossal Clean Crawled Corpus\"and achieve a 4x speedup over the T5-XXL model.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2101.03961, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "26958176",
        "name": "W. Fedus",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2368067",
        "name": "Barret Zoph",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1846258",
        "name": "Noam Shazeer",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "This work simplifies the MoE routing algorithm and design intuitive improved models with reduced communication and computational costs and shows large sparse models may be trained, for the first time, with lower precision formats."
  },
  {
    "paperId": "d29430adccb805ab57b349afa8553954347b3197",
    "title": "Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers",
    "year": 2020,
    "abstract": "Most recent semantic segmentation methods adopt a fully-convolutional network (FCN) with an encoder-decoder architecture. The encoder progressively reduces the spatial resolution and learns more abstract/semantic visual concepts with larger receptive fields. Since context modeling is critical for segmentation, the latest efforts have been focused on increasing the receptive field, through either dilated/atrous convolutions or inserting attention modules. However, the encoder-decoder based FCN architecture remains unchanged. In this paper, we aim to provide an alternative perspective by treating semantic segmentation as a sequence-to-sequence prediction task. Specifically, we deploy a pure transformer (i.e., without convolution and resolution reduction) to encode an image as a sequence of patches. With the global context modeled in every layer of the transformer, this encoder can be combined with a simple decoder to provide a powerful segmentation model, termed SEgmentation TRansformer (SETR). Extensive experiments show that SETR achieves new state of the art on ADE20K (50.28% mIoU), Pascal Context (55.83% mIoU) and competitive results on Cityscapes. Particularly, we achieve the first position in the highly competitive ADE20K test server leaderboard on the day of submission.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2012.15840",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2012.15840, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "40895011",
        "name": "Sixiao Zheng",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "31727033",
        "name": "Jiachen Lu",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "3459894",
        "name": "Hengshuang Zhao",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2171228",
        "name": "Xiatian Zhu",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "79454234",
        "name": "Zekun Luo",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2628601",
        "name": "Yabiao Wang",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "35782003",
        "name": "Yanwei Fu",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1384556269",
        "name": "Jianfeng Feng",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "145406421",
        "name": "T. Xiang",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "143635540",
        "name": "Philip H. S. Torr",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "48459110",
        "name": "Li Zhang",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "This paper deploys a pure transformer to encode an image as a sequence of patches, termed SEgmentation TRansformer (SETR), and shows that SETR achieves new state of the art on ADE20K, Pascal Context, and competitive results on Cityscapes."
  },
  {
    "paperId": "afeeb8f5018eebb1a1d334b94dbbfc48d167efef",
    "title": "iTransformer: Inverted Transformers Are Effective for Time Series Forecasting",
    "year": 2023,
    "abstract": "The recent boom of linear forecasting models questions the ongoing passion for architectural modifications of Transformer-based forecasters. These forecasters leverage Transformers to model the global dependencies over temporal tokens of time series, with each token formed by multiple variates of the same timestamp. However, Transformers are challenged in forecasting series with larger lookback windows due to performance degradation and computation explosion. Besides, the embedding for each temporal token fuses multiple variates that represent potential delayed events and distinct physical measurements, which may fail in learning variate-centric representations and result in meaningless attention maps. In this work, we reflect on the competent duties of Transformer components and repurpose the Transformer architecture without any modification to the basic components. We propose iTransformer that simply applies the attention and feed-forward network on the inverted dimensions. Specifically, the time points of individual series are embedded into variate tokens which are utilized by the attention mechanism to capture multivariate correlations; meanwhile, the feed-forward network is applied for each variate token to learn nonlinear representations. The iTransformer model achieves state-of-the-art on challenging real-world datasets, which further empowers the Transformer family with promoted performance, generalization ability across different variates, and better utilization of arbitrary lookback windows, making it a nice alternative as the fundamental backbone of time series forecasting. Code is available at this repository: https://github.com/thuml/iTransformer.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2310.06625",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.06625, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "2257375992",
        "name": "Yong Liu",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2203368846",
        "name": "Tengge Hu",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2257340579",
        "name": "Haoran Zhang",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2051867856",
        "name": "Haixu Wu",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2255363760",
        "name": "Shiyu Wang",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2253908414",
        "name": "Lintao Ma",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2054275000",
        "name": "Mingsheng Long",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "The iTransformer model achieves state-of-the-art on challenging real-world datasets, which further empowers the Transformer family with promoted performance, generalization ability across different variates, and better utilization of arbitrary lookback windows, making it a nice alternative as the fundamental backbone of time series forecasting."
  },
  {
    "paperId": "f7410f535bda5b5a9888512fb954193245c1d0b2",
    "title": "Swin UNETR: Swin Transformers for Semantic Segmentation of Brain Tumors in MRI Images",
    "year": 2022,
    "abstract": "Semantic segmentation of brain tumors is a fundamental medical image analysis task involving multiple MRI imaging modalities that can assist clinicians in diagnosing the patient and successively studying the progression of the malignant entity. In recent years, Fully Convolutional Neural Networks (FCNNs) approaches have become the de facto standard for 3D medical image segmentation. The popular\"U-shaped\"network architecture has achieved state-of-the-art performance benchmarks on different 2D and 3D semantic segmentation tasks and across various imaging modalities. However, due to the limited kernel size of convolution layers in FCNNs, their performance of modeling long-range information is sub-optimal, and this can lead to deficiencies in the segmentation of tumors with variable sizes. On the other hand, transformer models have demonstrated excellent capabilities in capturing such long-range information in multiple domains, including natural language processing and computer vision. Inspired by the success of vision transformers and their variants, we propose a novel segmentation model termed Swin UNEt TRansformers (Swin UNETR). Specifically, the task of 3D brain tumor semantic segmentation is reformulated as a sequence to sequence prediction problem wherein multi-modal input data is projected into a 1D sequence of embedding and used as an input to a hierarchical Swin transformer as the encoder. The swin transformer encoder extracts features at five different resolutions by utilizing shifted windows for computing self-attention and is connected to an FCNN-based decoder at each resolution via skip connections. We have participated in BraTS 2021 segmentation challenge, and our proposed model ranks among the top-performing approaches in the validation phase. Code: https://monai.io/research/swin-unetr",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2201.01266, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "1856024",
        "name": "Ali Hatamizadeh",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "10751841",
        "name": "V. Nath",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "46556781",
        "name": "Yucheng Tang",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "144041873",
        "name": "Dong Yang",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "144531567",
        "name": "H. Roth",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "3262394",
        "name": "Daguang Xu",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "The task of 3D brain tumor semantic segmentation is reformulated as a sequence to sequence prediction problem wherein multi-modal input data is projected into a 1D sequence of embedding and used as an input to a hierarchical Swin transformer as the encoder."
  },
  {
    "paperId": "1eac5d12f30697aa74d66f4026fb662c5d51bd43",
    "title": "SiT: Exploring Flow and Diffusion-based Generative Models with Scalable Interpolant Transformers",
    "year": 2024,
    "abstract": "We present Scalable Interpolant Transformers (SiT), a family of generative models built on the backbone of Diffusion Transformers (DiT). The interpolant framework, which allows for connecting two distributions in a more flexible way than standard diffusion models, makes possible a modular study of various design choices impacting generative models built on dynamical transport: learning in discrete or continuous time, the objective function, the interpolant that connects the distributions, and deterministic or stochastic sampling. By carefully introducing the above ingredients, SiT surpasses DiT uniformly across model sizes on the conditional ImageNet 256x256 and 512x512 benchmark using the exact same model structure, number of parameters, and GFLOPs. By exploring various diffusion coefficients, which can be tuned separately from learning, SiT achieves an FID-50K score of 2.06 and 2.62, respectively.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.08740, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "2279750963",
        "name": "Nanye Ma",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2279751634",
        "name": "Mark Goldstein",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "27590460",
        "name": "M. S. Albergo",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "12061674",
        "name": "N. Boffi",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2244622633",
        "name": "Eric Vanden-Eijnden",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2275940976",
        "name": "Saining Xie",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "Scalable Interpolant Transformers is presented, a family of generative models built on the backbone of Diffusion Transformers that surpasses DiT uniformly across model sizes on the conditional ImageNet 256x256 and 512x512 benchmark using the exact same model structure, number of parameters, and GFLOPs."
  },
  {
    "paperId": "7da0f2501034522e3d50af7e9b8fa7ec9d7b65b6",
    "title": "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers",
    "year": 2022,
    "abstract": "Generative Pre-trained Transformer models, known as GPT or OPT, set themselves apart through breakthrough performance across complex language modelling tasks, but also by their extremely high computational and storage costs. Specifically, due to their massive size, even inference for large, highly-accurate GPT models may require multiple performant GPUs, which limits the usability of such models. While there is emerging work on relieving this pressure via model compression, the applicability and performance of existing compression techniques is limited by the scale and complexity of GPT models. In this paper, we address this challenge, and propose GPTQ, a new one-shot weight quantization method based on approximate second-order information, that is both highly-accurate and highly-efficient. Specifically, GPTQ can quantize GPT models with 175 billion parameters in approximately four GPU hours, reducing the bitwidth down to 3 or 4 bits per weight, with negligible accuracy degradation relative to the uncompressed baseline. Our method more than doubles the compression gains relative to previously-proposed one-shot quantization methods, preserving accuracy, allowing us for the first time to execute an 175 billion-parameter model inside a single GPU for generative inference. Moreover, we also show that our method can still provide reasonable accuracy in the extreme quantization regime, in which weights are quantized to 2-bit or even ternary quantization levels. We show experimentally that these improvements can be leveraged for end-to-end inference speedups over FP16, of around 3.25x when using high-end GPUs (NVIDIA A100) and 4.5x when using more cost-effective ones (NVIDIA A6000). The implementation is available at https://github.com/IST-DASLab/gptq.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2210.17323, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "1502248377",
        "name": "Elias Frantar",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "9543395",
        "name": "Saleh Ashkboos",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1713648",
        "name": "T. Hoefler",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "3311387",
        "name": "Dan Alistarh",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "GPTQ, a new one-shot weight quantization method based on approximate second-order information, that is both highly-accurate and highly-efficient, is proposed, allowing for the first time to execute an 175 billion-parameter model inside a single GPU for generative inference."
  },
  {
    "paperId": "d8e9f8c8a37cb4cd26b92ad0d942d641cd512644",
    "title": "Fast Inference from Transformers via Speculative Decoding",
    "year": 2022,
    "abstract": "Inference from large autoregressive models like Transformers is slow - decoding K tokens takes K serial runs of the model. In this work we introduce speculative decoding - an algorithm to sample from autoregressive models faster without any changes to the outputs, by computing several tokens in parallel. At the heart of our approach lie the observations that (1) hard language-modeling tasks often include easier subtasks that can be approximated well by more efficient models, and (2) using speculative execution and a novel sampling method, we can make exact decoding from the large models faster, by running them in parallel on the outputs of the approximation models, potentially generating several tokens concurrently, and without changing the distribution. Our method can accelerate existing off-the-shelf models without retraining or architecture changes. We demonstrate it on T5-XXL and show a 2X-3X acceleration compared to the standard T5X implementation, with identical outputs.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2211.17192",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2211.17192, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "2188067081",
        "name": "Yaniv Leviathan",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "21605105",
        "name": "Matan Kalman",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1745572",
        "name": "Yossi Matias",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": null
  },
  {
    "paperId": "21da617a0f79aabf94272107184606cefe90ab75",
    "title": "Generating Long Sequences with Sparse Transformers",
    "year": 2019,
    "abstract": "Transformers are powerful sequence models, but require time and memory that grows quadratically with the sequence length. In this paper we introduce sparse factorizations of the attention matrix which reduce this to $O(n \\sqrt{n})$. We also introduce a) a variation on architecture and initialization to train deeper networks, b) the recomputation of attention matrices to save memory, and c) fast attention kernels for training. We call networks with these changes Sparse Transformers, and show they can model sequences tens of thousands of timesteps long using hundreds of layers. We use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, CIFAR-10, and ImageNet-64. We generate unconditional samples that demonstrate global coherence and great diversity, and show it is possible in principle to use self-attention to model sequences of length one million or more.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1904.10509, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "48422824",
        "name": "R. Child",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "145565184",
        "name": "Scott Gray",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "38909097",
        "name": "Alec Radford",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1701686",
        "name": "I. Sutskever",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "This paper introduces sparse factorizations of the attention matrix which reduce this to $O(n)$, and generates unconditional samples that demonstrate global coherence and great diversity, and shows it is possible in principle to use self-attention to model sequences of length one million or more."
  },
  {
    "paperId": "1e178f0c5bb9709ae5c7bdb60ecd76f00b0fcd86",
    "title": "Transformers without Normalization",
    "year": 2025,
    "abstract": "Normalization layers are ubiquitous in modern neural networks and have long been considered essential. This work demonstrates that Transformers without normalization can achieve the same or better performance using a remarkably simple technique. We introduce Dynamic Tanh (DyT), an element-wise operation DyT(x) = tanh(αx), as a dropin replacement for normalization layers in Transformers. DyT is inspired by the observation that layer normalization in Transformers often produces tanh-like, S-shaped input-output mappings. By incorporating DyT, Transformers without normalization can match or exceed the performance of their normalized counterparts, mostly without hyperparameter tuning. We validate the effectiveness of Transformers with DyT across diverse settings, from recognition to generation, supervised to self-supervised learning, and computer vision to language models. These findings challenge the conventional understanding that normalization layers are indispensable in modern neural networks, and offer new insights into their role in deep networks.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.10622, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "2336019460",
        "name": "Jiachen Zhu",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2281064954",
        "name": "Xinlei Chen",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2058350112",
        "name": "Kaiming He",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2265899558",
        "name": "Yann LeCun",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2349957935",
        "name": "Zhuang Liu",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "Dynamic Tanh (DyT), an element-wise operation DyT(x) = tanh(αx), is introduced as a dropin replacement for normalization layers in Transformers, inspired by the observation that layer normalization in Transformers often produces tanh-like, S-shaped input-output mappings."
  },
  {
    "paperId": "7afaabb73bec969c0937be46b9f0f757e07c8534",
    "title": "Parallelizing Linear Transformers with the Delta Rule over Sequence Length",
    "year": 2024,
    "abstract": "Transformers with linear attention (i.e., linear transformers) and state-space models have recently been suggested as a viable linear-time alternative to transformers with softmax attention. However, these models still underperform transformers especially on tasks that require in-context retrieval. While more expressive variants of linear transformers which replace the additive update in linear transformers with the delta rule (DeltaNet) have been found to be more effective at associative recall, existing algorithms for training such models do not parallelize over sequence length and are thus inefficient to train on modern hardware. This work describes a hardware-efficient algorithm for training linear transformers with the delta rule, which exploits a memory-efficient representation for computing products of Householder matrices. This algorithm allows us to scale up DeltaNet to standard language modeling settings. We train a 1.3B model for 100B tokens and find that it outperforms recent linear-time baselines such as Mamba and GLA in terms of perplexity and zero-shot performance on downstream tasks. We also experiment with two hybrid models which combine DeltaNet layers with (1) sliding-window attention layers every other layer or (2) two global attention layers, and find that these hybrids outperform strong transformer baselines.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.06484, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "50591392",
        "name": "Songlin Yang",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2257409822",
        "name": "Bailin Wang",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "49890808",
        "name": "Yu Zhang",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2273540596",
        "name": "Yikang Shen",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2261394948",
        "name": "Yoon Kim",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "This work describes a hardware-efficient algorithm for training linear transformers with the delta rule, which exploits a memory-efficient representation for computing products of Householder matrices and finds that these hybrids outperform strong transformer baselines."
  },
  {
    "paperId": "ed4087f6e8d77452810979f58246c5b2ad846cf8",
    "title": "Transformers in Time Series: A Survey",
    "year": 2022,
    "abstract": "Transformers have achieved superior performances in many tasks in natural language processing and computer vision, which also triggered great interest in the time series community. Among multiple advantages of Transformers, the ability to capture long-range dependencies and interactions is especially attractive for time series modeling, leading to exciting progress in various time series applications. In this paper, we systematically review Transformer schemes for time series modeling by highlighting their strengths as well as limitations. In particular, we examine the development of time series Transformers in two perspectives. From the perspective of network structure, we summarize the adaptations and modifications that have been made to Transformers in order to accommodate the challenges in time series analysis. From the perspective of applications, we categorize time series Transformers based on common tasks including forecasting, anomaly detection, and classification. Empirically, we perform robust analysis, model size analysis, and seasonal-trend decomposition analysis to study how Transformers perform in time series. Finally, we discuss and suggest future directions to provide useful research guidance.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "https://www.ijcai.org/proceedings/2023/0759.pdf",
      "status": "BRONZE",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2202.07125, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "3308963",
        "name": "Qingsong Wen",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "50018188",
        "name": "Tian Zhou",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "144677904",
        "name": "Chao Zhang",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2109780096",
        "name": "Weiqiu Chen",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1387898803",
        "name": "Ziqing Ma",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "3063894",
        "name": "Junchi Yan",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2110940896",
        "name": "Liang Sun",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "This paper systematically review Transformer schemes for time series modeling by highlighting their strengths as well as limitations and categorizes time series Transformers based on common tasks including forecasting, anomaly detection, and classification."
  },
  {
    "paperId": "b91de7d12ec1103f6ef9eb0720d697a9e7ecc9fe",
    "title": "LoFTR: Detector-Free Local Feature Matching with Transformers",
    "year": 2021,
    "abstract": "We present a novel method for local image feature matching. Instead of performing image feature detection, description, and matching sequentially, we propose to first establish pixel-wise dense matches at a coarse level and later refine the good matches at a fine level. In contrast to dense methods that use a cost volume to search correspondences, we use self and cross attention layers in Transformer to obtain feature descriptors that are conditioned on both images. The global receptive field provided by Transformer enables our method to produce dense matches in low-texture areas, where feature detectors usually struggle to produce repeatable interest points. The experiments on indoor and outdoor datasets show that LoFTR outperforms state-of-the-art methods by a large margin. LoFTR also ranks first on two public benchmarks of visual localization among the published methods. Code is available at our project page: https://zju3dv.github.io/loftr/.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2104.00680",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2104.00680, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "153552118",
        "name": "Jiaming Sun",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1384523019",
        "name": "Zehong Shen",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "37075603",
        "name": "Yuang Wang",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1679542",
        "name": "H. Bao",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "145453113",
        "name": "Xiaowei Zhou",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "The proposed method, LoFTR, uses self and cross attention layers in Transformer to obtain feature descriptors that are conditioned on both images, and enables the method to produce dense matches in low-texture areas, where feature detectors usually struggle to produce repeatable interest points."
  },
  {
    "paperId": "18863dbfa32eaa1ccdb56ff180e6ab079a7f1ec6",
    "title": "Multiscale Vision Transformers",
    "year": 2021,
    "abstract": "We present Multiscale Vision Transformers (MViT) for video and image recognition, by connecting the seminal idea of multiscale feature hierarchies with transformer models. Multiscale Transformers have several channel-resolution scale stages. Starting from the input resolution and a small channel dimension, the stages hierarchically expand the channel capacity while reducing the spatial resolution. This creates a multiscale pyramid of features with early layers operating at high spatial resolution to model simple low-level visual information, and deeper layers at spatially coarse, but complex, high-dimensional features. We evaluate this fundamental architectural prior for modeling the dense nature of visual signals for a variety of video recognition tasks where it outperforms concurrent vision transformers that rely on large scale external pre-training and are 5-10× more costly in computation and parameters. We further remove the temporal dimension and apply our model for image classification where it outperforms prior work on vision transformers. Code is available at: https://github.com/facebookresearch/SlowFast.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2104.11227",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2104.11227, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "146884473",
        "name": "Haoqi Fan",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "144752314",
        "name": "Bo Xiong",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "11379939",
        "name": "K. Mangalam",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2359205979",
        "name": "Yanghao Li",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "151485208",
        "name": "Zhicheng Yan",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "153652147",
        "name": "J. Malik",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2322150",
        "name": "Christoph Feichtenhofer",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "This fundamental architectural prior for modeling the dense nature of visual signals for a variety of video recognition tasks where it outperforms concurrent vision transformers that rely on large scale external pre-training and are 5-10× more costly in computation and parameters is evaluated."
  },
  {
    "paperId": "5a20aa49b81b4e14fdb36814e557b3da60259ce9",
    "title": "Chain of Thought Empowers Transformers to Solve Inherently Serial Problems",
    "year": 2024,
    "abstract": "Instructing the model to generate a sequence of intermediate steps, a.k.a., a chain of thought (CoT), is a highly effective method to improve the accuracy of large language models (LLMs) on arithmetics and symbolic reasoning tasks. However, the mechanism behind CoT remains unclear. This work provides a theoretical understanding of the power of CoT for decoder-only transformers through the lens of expressiveness. Conceptually, CoT empowers the model with the ability to perform inherently serial computation, which is otherwise lacking in transformers, especially when depth is low. Given input length $n$, previous works have shown that constant-depth transformers with finite precision $\\mathsf{poly}(n)$ embedding size can only solve problems in $\\mathsf{TC}^0$ without CoT. We first show an even tighter expressiveness upper bound for constant-depth transformers with constant-bit precision, which can only solve problems in $\\mathsf{AC}^0$, a proper subset of $ \\mathsf{TC}^0$. However, with $T$ steps of CoT, constant-depth transformers using constant-bit precision and $O(\\log n)$ embedding size can solve any problem solvable by boolean circuits of size $T$. Empirically, enabling CoT dramatically improves the accuracy for tasks that are hard for parallel computation, including the composition of permutation groups, iterated squaring, and circuit value problems, especially for low-depth transformers.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.12875, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "46947755",
        "name": "Zhiyuan Li",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2284940181",
        "name": "Hong Liu",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2284824080",
        "name": "Denny Zhou",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2284835869",
        "name": "Tengyu Ma",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "A theoretical understanding of the power of CoT for decoder-only transformers through the lens of expressiveness is provided and an even tighter expressiveness upper bound for constant-depth transformers with constant-bit precision is shown."
  },
  {
    "paperId": "75c19f3249f644f5cb2182282fc117c089fd3f65",
    "title": "The Expressive Power of Transformers with Chain of Thought",
    "year": 2024,
    "abstract": null,
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2310.07923",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2310.07923?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2310.07923, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "2257349351",
        "name": "William Merrill",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2257349398",
        "name": "Ashish Sabharwal",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "This paper aims to demonstrate how transformers’ reasoning can be improved by allowing them to use a “chain of thought” or “scratchpad”, i.e., generate and condition on a sequence of intermediate tokens before answering."
  },
  {
    "paperId": "2fe2f849b94cf08b559226bc9d78adcaef5ef186",
    "title": "AdaptFormer: Adapting Vision Transformers for Scalable Visual Recognition",
    "year": 2022,
    "abstract": "Pretraining Vision Transformers (ViTs) has achieved great success in visual recognition. A following scenario is to adapt a ViT to various image and video recognition tasks. The adaptation is challenging because of heavy computation and memory storage. Each model needs an independent and complete finetuning process to adapt to different tasks, which limits its transferability to different visual domains. To address this challenge, we propose an effective adaptation approach for Transformer, namely AdaptFormer, which can adapt the pre-trained ViTs into many different image and video tasks efficiently. It possesses several benefits more appealing than prior arts. Firstly, AdaptFormer introduces lightweight modules that only add less than 2% extra parameters to a ViT, while it is able to increase the ViT's transferability without updating its original pre-trained parameters, significantly outperforming the existing 100\\% fully fine-tuned models on action recognition benchmarks. Secondly, it can be plug-and-play in different Transformers and scalable to many visual tasks. Thirdly, extensive experiments on five image and video datasets show that AdaptFormer largely improves ViTs in the target domains. For example, when updating just 1.5% extra parameters, it achieves about 10% and 19% relative improvement compared to the fully fine-tuned models on Something-Something~v2 and HMDB51, respectively. Code is available at https://github.com/ShoufaChen/AdaptFormer.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2205.13535",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2205.13535, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "2107977968",
        "name": "Shoufa Chen",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "102482926",
        "name": "Chongjian Ge",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2143681543",
        "name": "Zhan Tong",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "46584859",
        "name": "Jiangliu Wang",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2255687",
        "name": "Yibing Song",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2167482071",
        "name": "Jue Wang",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2143481782",
        "name": "Ping Luo",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "AdaptFormer introduces lightweight modules that only add less than 2% extra parameters to a ViT, while it is able to increase the ViT's transferability without updating its original pre-trained parameters, significantly outperforming the existing 100\\% fully fine-tuned models on action recognition benchmarks."
  },
  {
    "paperId": "10bd38673951f5d7729568284093cbd80482ab16",
    "title": "Vision Transformers Need Registers",
    "year": 2023,
    "abstract": "Transformers have recently emerged as a powerful tool for learning visual representations. In this paper, we identify and characterize artifacts in feature maps of both supervised and self-supervised ViT networks. The artifacts correspond to high-norm tokens appearing during inference primarily in low-informative background areas of images, that are repurposed for internal computations. We propose a simple yet effective solution based on providing additional tokens to the input sequence of the Vision Transformer to fill that role. We show that this solution fixes that problem entirely for both supervised and self-supervised models, sets a new state of the art for self-supervised visual models on dense visual prediction tasks, enables object discovery methods with larger models, and most importantly leads to smoother feature maps and attention maps for downstream visual processing.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2309.16588",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.16588, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "2214523349",
        "name": "Timothée Darcet",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2248163611",
        "name": "Maxime Oquab",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2599292",
        "name": "J. Mairal",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2329288",
        "name": "Piotr Bojanowski",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "This paper identifies and characterize artifacts in feature maps of both supervised and self-supervised ViT networks, and proposes a simple yet effective solution based on providing additional tokens to the input sequence of the Vision Transformer to fill that role."
  },
  {
    "paperId": "61e721334296ebfbbf6443b5ed9eb8c83b708c95",
    "title": "Scaling Vision Transformers to 22 Billion Parameters",
    "year": 2023,
    "abstract": "The scaling of Transformers has driven breakthrough capabilities for language models. At present, the largest large language models (LLMs) contain upwards of 100B parameters. Vision Transformers (ViT) have introduced the same architecture to image and video modelling, but these have not yet been successfully scaled to nearly the same degree; the largest dense ViT contains 4B parameters (Chen et al., 2022). We present a recipe for highly efficient and stable training of a 22B-parameter ViT (ViT-22B) and perform a wide variety of experiments on the resulting model. When evaluated on downstream tasks (often with a lightweight linear model on frozen features), ViT-22B demonstrates increasing performance with scale. We further observe other interesting benefits of scale, including an improved tradeoff between fairness and performance, state-of-the-art alignment to human visual perception in terms of shape/texture bias, and improved robustness. ViT-22B demonstrates the potential for\"LLM-like\"scaling in vision, and provides key steps towards getting there.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2302.05442",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2302.05442, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "3226635",
        "name": "Mostafa Dehghani",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2941141",
        "name": "J. Djolonga",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "40608942",
        "name": "Basil Mustafa",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "31148950",
        "name": "Piotr Padlewski",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "151488492",
        "name": "J. Heek",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2058362",
        "name": "J. Gilmer",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2079614268",
        "name": "A. Steiner",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2062862676",
        "name": "Mathilde Caron",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1949747",
        "name": "Robert Geirhos",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2922782",
        "name": "Ibrahim M. Alabdulmohsin",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2068720",
        "name": "Rodolphe Jenatton",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "39611591",
        "name": "Lucas Beyer",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "143902495",
        "name": "Michael Tschannen",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "31638576",
        "name": "Anurag Arnab",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "144129720",
        "name": "Xiao Wang",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "145814174",
        "name": "C. Riquelme",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "46352821",
        "name": "M. Minderer",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1794202",
        "name": "J. Puigcerver",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "3399348",
        "name": "Utku Evci",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2157851754",
        "name": "Manoj Kumar",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "3440930",
        "name": "Sjoerd van Steenkiste",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "7843061",
        "name": "Gamaleldin F. Elsayed",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "32694028",
        "name": "Aravindh Mahendran",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1807197",
        "name": "F. Yu",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "35679876",
        "name": "Avital Oliver",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2174667321",
        "name": "Fantine Huot",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1994065972",
        "name": "Jasmijn Bastings",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "153247100",
        "name": "Mark Collier",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2194424",
        "name": "A. Gritsenko",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "3468723",
        "name": "Vighnesh Birodkar",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2901520",
        "name": "C. Vasconcelos",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "97947517",
        "name": "Yi Tay",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1722052",
        "name": "Thomas Mensink",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "144629422",
        "name": "Alexander Kolesnikov",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2170163036",
        "name": "Filip Paveti'c",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "47497262",
        "name": "Dustin Tran",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "41016725",
        "name": "Thomas Kipf",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2170162986",
        "name": "Mario Luvci'c",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2743563",
        "name": "Xiaohua Zhai",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2064752644",
        "name": "Daniel Keysers",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2066076307",
        "name": "Jeremiah Harmsen",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2815290",
        "name": "N. Houlsby",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "A recipe for highly efficient and stable training of a 22B-parameter ViT (ViT-22B) and a wide variety of experiments on the resulting model, which demonstrates the potential for \"LLM-like\"scaling in vision, and provides key steps towards getting there."
  },
  {
    "paperId": "561377c8e51bb864a1a689d830fab2f0881ae78e",
    "title": "Remote Sensing Image Change Detection With Transformers",
    "year": 2021,
    "abstract": "Modern change detection (CD) has achieved remarkable success by the powerful discriminative ability of deep convolutions. However, high-resolution remote sensing CD remains challenging due to the complexity of objects in the scene. Objects with the same semantic concept may show distinct spectral characteristics at different times and spatial locations. Most recent CD pipelines using pure convolutions are still struggling to relate long-range concepts in space-time. Nonlocal self-attention approaches show promising performance via modeling dense relationships among pixels, yet are computationally inefficient. Here, we propose a bitemporal image transformer (BIT) to efficiently and effectively model contexts within the spatial-temporal domain. Our intuition is that the high-level concepts of the change of interest can be represented by a few visual words, that is, semantic tokens. To achieve this, we express the bitemporal image into a few tokens and use a transformer encoder to model contexts in the compact token-based space-time. The learned context-rich tokens are then fed back to the pixel-space for refining the original features via a transformer decoder. We incorporate BIT in a deep feature differencing-based CD framework. Extensive experiments on three CD datasets demonstrate the effectiveness and efficiency of the proposed method. Notably, our BIT-based model significantly outperforms the purely convolutional baseline using only three times lower computational costs and model parameters. Based on a naive backbone (ResNet18) without sophisticated structures (e.g., feature pyramid network (FPN) and UNet), our model surpasses several state-of-the-art CD methods, including better than four recent attention-based methods in terms of efficiency and accuracy. Our code is available at https://github.com/justchenhao/BIT_CD.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2103.00208",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2103.00208, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": null,
        "name": "Hao Chen",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2072539630",
        "name": "Zipeng Qi",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1741174",
        "name": "Zhenwei Shi",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "This work proposes a bitemporal image transformer (BIT) to efficiently and effectively model contexts within the spatial-temporal domain and significantly outperforms the purely convolutional baseline using only three times lower computational costs and model parameters."
  },
  {
    "paperId": "acf87283fa8ae426f1a4987b345b401bf2913f61",
    "title": "Do Transformers Really Perform Badly for Graph Representation?",
    "year": 2021,
    "abstract": null,
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": null
    },
    "authors": [
      {
        "authorId": "2051552141",
        "name": "Chengxuan Ying",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "123970124",
        "name": "Tianle Cai",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2108801920",
        "name": "Shengjie Luo",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "150311931",
        "name": "Shuxin Zheng",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "35286545",
        "name": "Guolin Ke",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2266036459",
        "name": "Di He",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2266126249",
        "name": "Yanming Shen",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2266182896",
        "name": "Tie-Yan Liu",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "This paper presents Graphormer, which is built upon the standard Transformer architecture, and could attain excellent results on a broad range of graph representation learning tasks, especially on the recent OGB Large-Scale Challenge."
  },
  {
    "paperId": "b364cdb02d18b9d9a3c097f5ea446f7e9ab10325",
    "title": "Going deeper with Image Transformers",
    "year": 2021,
    "abstract": "Transformers have been recently adapted for large scale image classification, achieving high scores shaking up the long supremacy of convolutional neural networks. However the optimization of vision transformers has been little studied so far. In this work, we build and optimize deeper transformer networks for image classification. In particular, we investigate the interplay of architecture and optimization of such dedicated transformers. We make two architecture changes that significantly improve the accuracy of deep transformers. This leads us to produce models whose performance does not saturate early with more depth, for in-stance we obtain 86.5% top-1 accuracy on Imagenet when training with no external data, we thus attain the current sate of the art with less floating-point operations and parameters. Our best model establishes the new state of the art on Imagenet with Reassessed labels and Imagenet-V2 / match frequency, in the setting with no additional training data. We share our code and models1.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2103.17239",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2103.17239, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "2113243762",
        "name": "Hugo Touvron",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "51021910",
        "name": "M. Cord",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "3469062",
        "name": "Alexandre Sablayrolles",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2282478",
        "name": "Gabriel Synnaeve",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2065248680",
        "name": "Herv'e J'egou",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "This work builds and optimize deeper transformer networks for image classification and investigates the interplay of architecture and optimization of such dedicated transformers, making two architecture changes that significantly improve the accuracy of deep transformers."
  },
  {
    "paperId": "62b18cc55dcc7ffe52c28e1086aee893b7bc4334",
    "title": "Gated Linear Attention Transformers with Hardware-Efficient Training",
    "year": 2023,
    "abstract": "Transformers with linear attention allow for efficient parallel training but can simultaneously be formulated as an RNN with 2D (matrix-valued) hidden states, thus enjoying linear-time inference complexity. However, linear attention generally underperforms ordinary softmax attention. Moreover, current implementations of linear attention lack I/O-awareness and are thus slower than highly optimized implementations of softmax attention. This work describes a hardware-efficient algorithm for linear attention that trades off memory movement against parallelizability. The resulting implementation, dubbed FLASHLINEARATTENTION, is faster than FLASHATTENTION-2 (Dao, 2023) as a standalone layer even on short sequence lengths (e.g., 1K). We then generalize this algorithm to a more expressive variant of linear attention with data-dependent gates. When used as a replacement for the standard attention layer in Transformers, the resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer (Touvron et al., 2023) as well recent linear-time-inference baselines such as RetNet (Sun et al., 2023a) and Mamba (Gu&Dao, 2023) on moderate-scale language modeling experiments. GLA Transformer is especially effective at length generalization, enabling a model trained on 2K to generalize to sequences longer than 20K without significant perplexity degradations. For training speed, the GLA Transformer has higher throughput than a similarly-sized Mamba model.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.06635, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "50591392",
        "name": "Songlin Yang",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2257409822",
        "name": "Bailin Wang",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2273540596",
        "name": "Yikang Shen",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1819152",
        "name": "Rameswar Panda",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2261394948",
        "name": "Yoon Kim",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "The resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer as well recent linear-time-inference baselines such as RetNet and Mamba on moderate-scale language modeling experiments."
  },
  {
    "paperId": "4a7530bbaee7563ee244f3ffed6b706bd96f08a8",
    "title": "Trained Transformers Learn Linear Models In-Context",
    "year": 2023,
    "abstract": "Attention-based neural networks such as transformers have demonstrated a remarkable ability to exhibit in-context learning (ICL): Given a short prompt sequence of tokens from an unseen task, they can formulate relevant per-token and next-token predictions without any parameter updates. By embedding a sequence of labeled training data and unlabeled test data as a prompt, this allows for transformers to behave like supervised learning algorithms. Indeed, recent work has shown that when training transformer architectures over random instances of linear regression problems, these models' predictions mimic those of ordinary least squares. Towards understanding the mechanisms underlying this phenomenon, we investigate the dynamics of ICL in transformers with a single linear self-attention layer trained by gradient flow on linear regression tasks. We show that despite non-convexity, gradient flow with a suitable random initialization finds a global minimum of the objective function. At this global minimum, when given a test prompt of labeled examples from a new prediction task, the transformer achieves prediction error competitive with the best linear predictor over the test prompt distribution. We additionally characterize the robustness of the trained transformer to a variety of distribution shifts and show that although a number of shifts are tolerated, shifts in the covariate distribution of the prompts are not. Motivated by this, we consider a generalized ICL setting where the covariate distributions can vary across prompts. We show that although gradient flow succeeds at finding a global minimum in this setting, the trained transformer is still brittle under mild covariate shifts. We complement this finding with experiments on large, nonlinear transformer architectures which we show are more robust under covariate shifts.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2306.09927",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.09927, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": null,
        "name": "Ruiqi Zhang",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "120443772",
        "name": "Spencer Frei",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1745169",
        "name": "P. Bartlett",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "The dynamics of ICL in transformers with a single linear self-attention layer trained by gradient flow on linear regression tasks are investigated and it is shown that despite non-convexity, gradient flow with a suitable random initialization finds a global minimum of the objective function."
  },
  {
    "paperId": "2a805d0e1b067444a554c5169d189fa1f649f411",
    "title": "Scaling Vision Transformers",
    "year": 2021,
    "abstract": "Attention-based neural networks such as the Vision Transformer (ViT) have recently attained state-of-the-art results on many computer vision benchmarks. Scale is a primary ingredient in attaining excellent results, therefore, understanding a model's scaling properties is a key to designing future generations effectively. While the laws for scaling Transformer language models have been studied, it is unknown how Vision Transformers scale. To address this, we scale ViT models and data, both up and down, and characterize the relationships between error rate, data, and compute. Along the way, we refine the architecture and training of ViT, reducing memory consumption and increasing accuracy of the resulting models. As a result, we successfully train a ViT model with two billion parameters, which attains a new state-of-the-art on ImageNet of 90.45% top-1 accuracy. The model also performs well for few-shot transfer, for example, reaching 84.86% top-1 accuracy on ImageNet with only 10 examples per class.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2106.04560",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2106.04560, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "2743563",
        "name": "Xiaohua Zhai",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "144629422",
        "name": "Alexander Kolesnikov",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2815290",
        "name": "N. Houlsby",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "39611591",
        "name": "Lucas Beyer",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "A ViT model with two billion parameters is successfully trained, which attains a new state-of-the-art on ImageNet of 90.45% top-1 accuracy and performs well for few-shot transfer."
  },
  {
    "paperId": "02ad9f3fefe33cb9ca546591bec65dbdf7766c80",
    "title": "Ring Attention with Blockwise Transformers for Near-Infinite Context",
    "year": 2023,
    "abstract": "Transformers have emerged as the architecture of choice for many state-of-the-art AI models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands imposed by Transformers limit their ability to handle long sequences, thereby posing challenges in utilizing videos, actions, and other long-form sequences and modalities in complex environments. We present a novel approach, Ring Attention with Blockwise Transformers (Ring Attention), which leverages blockwise computation of self-attention and feedforward to distribute long sequences across multiple devices while fully overlapping the communication of key-value blocks with the computation of blockwise attention. Our approach enables training and inference of sequences that are up to device count times longer than those achievable by prior memory-efficient Transformers, without resorting to approximations or incurring additional communication and computation overheads. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of our approach in allowing millions of tokens context size and improving performance.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2310.01889",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.01889, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "2256317240",
        "name": "Hao Liu",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2253469012",
        "name": "Matei Zaharia",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2253464956",
        "name": "Pieter Abbeel",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "This work presents a novel approach, Ring Attention with Blockwise Transformers (Ring Attention), which leverages blockwise computation of self-attention and feedforward to distribute long sequences across multiple devices while fully overlapping the communication of key-value blocks with the computation of blockwise attention."
  },
  {
    "paperId": "f5e9337477d7a9eb6267d0310549fdefafbb7fe2",
    "title": "Transformers learn to implement preconditioned gradient descent for in-context learning",
    "year": 2023,
    "abstract": "Several recent works demonstrate that transformers can implement algorithms like gradient descent. By a careful construction of weights, these works show that multiple layers of transformers are expressive enough to simulate iterations of gradient descent. Going beyond the question of expressivity, we ask: Can transformers learn to implement such algorithms by training over random problem instances? To our knowledge, we make the first theoretical progress on this question via an analysis of the loss landscape for linear transformers trained over random instances of linear regression. For a single attention layer, we prove the global minimum of the training objective implements a single iteration of preconditioned gradient descent. Notably, the preconditioning matrix not only adapts to the input distribution but also to the variance induced by data inadequacy. For a transformer with $L$ attention layers, we prove certain critical points of the training objective implement $L$ iterations of preconditioned gradient descent. Our results call for future theoretical studies on learning algorithms by training transformers.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2306.00297",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.00297, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "9036928",
        "name": "Kwangjun Ahn",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2149478480",
        "name": "Xiang Cheng",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1764651",
        "name": "Hadi Daneshmand",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "3072326",
        "name": "S. Sra",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "This work makes the first theoretical progress on this question via an analysis of the loss landscape for linear transformers trained over random instances of linear regression, and proves the global minimum of the training objective implements a single iteration of preconditioned gradient descent."
  },
  {
    "paperId": "707bd332d2c21dc5eb1f02a52d4a0506199aae76",
    "title": "CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers",
    "year": 2022,
    "abstract": "Large-scale pretrained transformers have created milestones in text (GPT-3) and text-to-image (DALL-E and CogView) generation. Its application to video generation is still facing many challenges: The potential huge computation cost makes the training from scratch unaffordable; The scarcity and weak relevance of text-video datasets hinder the model understanding complex movement semantics. In this work, we present 9B-parameter transformer CogVideo, trained by inheriting a pretrained text-to-image model, CogView2. We also propose multi-frame-rate hierarchical training strategy to better align text and video clips. As (probably) the first open-source large-scale pretrained text-to-video model, CogVideo outperforms all publicly available models at a large margin in machine and human evaluations.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2205.15868",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2205.15868, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "2105844599",
        "name": "Wenyi Hong",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2055623340",
        "name": "Ming Ding",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2163967642",
        "name": "Wendi Zheng",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "46522721",
        "name": "Xinghan Liu",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2148911990",
        "name": "Jie Tang",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "This work presents 9B-parameter transformer CogVideo, trained by inheriting a pretrained text-to-image model, CogView2, and proposes multi-frame-rate hierarchical training strategy to better align text and video clips."
  },
  {
    "paperId": "70c3d5ab03a54281be91709b19e3f50a2e4be0e3",
    "title": "Transformers as Statisticians: Provable In-Context Learning with In-Context Algorithm Selection",
    "year": 2023,
    "abstract": "Neural sequence models based on the transformer architecture have demonstrated remarkable \\emph{in-context learning} (ICL) abilities, where they can perform new tasks when prompted with training and test examples, without any parameter update to the model. This work first provides a comprehensive statistical theory for transformers to perform ICL. Concretely, we show that transformers can implement a broad class of standard machine learning algorithms in context, such as least squares, ridge regression, Lasso, learning generalized linear models, and gradient descent on two-layer neural networks, with near-optimal predictive power on various in-context data distributions. Using an efficient implementation of in-context gradient descent as the underlying mechanism, our transformer constructions admit mild size bounds, and can be learned with polynomially many pretraining sequences. Building on these ``base'' ICL algorithms, intriguingly, we show that transformers can implement more complex ICL procedures involving \\emph{in-context algorithm selection}, akin to what a statistician can do in real life -- A \\emph{single} transformer can adaptively select different base ICL algorithms -- or even perform qualitatively different tasks -- on different input sequences, without any explicit prompting of the right algorithm or task. We both establish this in theory by explicit constructions, and also observe this phenomenon experimentally. In theory, we construct two general mechanisms for algorithm selection with concrete examples: pre-ICL testing, and post-ICL validation. As an example, we use the post-ICL validation mechanism to construct a transformer that can perform nearly Bayes-optimal ICL on a challenging task -- noisy linear models with mixed noise levels. Experimentally, we demonstrate the strong in-context algorithm selection capabilities of standard transformer architectures.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2306.04637",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.04637, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "8681368",
        "name": "Yu Bai",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "103244045",
        "name": "Fan Chen",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "46507194",
        "name": "Haiquan Wang",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2054594326",
        "name": "Caiming Xiong",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2068869988",
        "name": "Song Mei",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "This work provides a comprehensive statistical theory for transformers to perform ICL, and shows that transformers can implement a broad class of standard machine learning algorithms in context, such as least squares, ridge regression, Lasso, learning generalized linear models, and gradient descent on two-layer neural networks, with near-optimal predictive power on various in-context data distributions."
  },
  {
    "paperId": "189fde3f4dfa105bb51472a8945618f395919560",
    "title": "Repeat After Me: Transformers are Better than State Space Models at Copying",
    "year": 2024,
    "abstract": "Transformers are the dominant architecture for sequence modeling, but there is growing interest in models that use a fixed-size latent state that does not depend on the sequence length, which we refer to as\"generalized state space models\"(GSSMs). In this paper we show that while GSSMs are promising in terms of inference-time efficiency, they are limited compared to transformer models on tasks that require copying from the input context. We start with a theoretical analysis of the simple task of string copying and prove that a two layer transformer can copy strings of exponential length while GSSMs are fundamentally limited by their fixed-size latent state. Empirically, we find that transformers outperform GSSMs in terms of efficiency and generalization on synthetic tasks that require copying the context. Finally, we evaluate pretrained large language models and find that transformer models dramatically outperform state space models at copying and retrieving information from context. Taken together, these results suggest a fundamental gap between transformers and GSSMs on tasks of practical interest.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.01032, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "47009988",
        "name": "Samy Jelassi",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "35402876",
        "name": "David Brandfonbrener",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "144695232",
        "name": "S. Kakade",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "19201820",
        "name": "Eran Malach",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "It is proved that a two layer transformer can copy strings of exponential length while GSSMs are fundamentally limited by their fixed-size latent state, and a fundamental gap between transformers and GSSMs on tasks of practical interest is suggested."
  },
  {
    "paperId": "6a9d69fb35414b8461573df333dba800f254519f",
    "title": "Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting",
    "year": 2019,
    "abstract": null,
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "https://doi.org/10.1016/j.ijforecast.2021.03.012",
      "status": "HYBRID",
      "license": "CCBY",
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1912.09363, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "143845299",
        "name": "Bryan Lim",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2676352",
        "name": "Sercan Ö. Arik",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1983856",
        "name": "Nicolas Loeff",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1945962",
        "name": "Tomas Pfister",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "The Temporal Fusion Transformer is introduced -- a novel attention-based architecture which combines high-performance multi-horizon forecasting with interpretable insights into temporal dynamics and three practical interpretability use-cases of TFT are showcased."
  },
  {
    "paperId": "e38dc0554dd89745bb17039a4d4ee9d714cf77f1",
    "title": "SpectralFormer: Rethinking Hyperspectral Image Classification With Transformers",
    "year": 2021,
    "abstract": "Hyperspectral (HS) images are characterized by approximately contiguous spectral information, enabling the fine identification of materials by capturing subtle spectral discrepancies. Due to their excellent locally contextual modeling ability, convolutional neural networks (CNNs) have been proven to be a powerful feature extractor in HS image classification. However, CNNs fail to mine and represent the sequence attributes of spectral signatures well due to the limitations of their inherent network backbone. To solve this issue, we rethink HS image classification from a sequential perspective with transformers and propose a novel backbone network called SpectralFormer. Beyond bandwise representations in classic transformers, SpectralFormer is capable of learning spectrally local sequence information from neighboring bands of HS images, yielding groupwise spectral embeddings. More significantly, to reduce the possibility of losing valuable information in the layerwise propagation process, we devise a cross-layer skip connection to convey memory-like components from shallow to deep layers by adaptively learning to fuse “soft” residuals across layers. It is worth noting that the proposed SpectralFormer is a highly flexible backbone network, which can be applicable to both pixelwise and patchwise inputs. We evaluate the classification performance of the proposed SpectralFormer on three HS datasets by conducting extensive experiments, showing the superiority over classic transformers and achieving a significant improvement in comparison with state-of-the-art backbone networks. The codes of this work will be available at https://github.com/danfenghong/IEEE_TGRS_SpectralFormer for the sake of reproducibility.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2107.02988",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2107.02988, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "2412834",
        "name": "D. Hong",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2090447478",
        "name": "Zhu Han",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "151502810",
        "name": "Jing Yao",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2276599246",
        "name": "Lianru Gao",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "46824214",
        "name": "Bing Zhang",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "143767945",
        "name": "A. Plaza",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1744943",
        "name": "J. Chanussot",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "This work rethink HS image classification from a sequential perspective with transformers and proposes a novel backbone network called SpectralFormer, which is capable of learning spectrally local sequence information from neighboring bands of HS images, yielding groupwise spectral embeddings."
  },
  {
    "paperId": "c6c734e16f66fbfcefac7625cc64599e83292c1e",
    "title": "MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers",
    "year": 2020,
    "abstract": "Pre-trained language models (e.g., BERT (Devlin et al., 2018) and its variants) have achieved remarkable success in varieties of NLP tasks. However, these models usually consist of hundreds of millions of parameters which brings challenges for fine-tuning and online serving in real-life applications due to latency and capacity constraints. In this work, we present a simple and effective approach to compress large Transformer (Vaswani et al., 2017) based pre-trained models, termed as deep self-attention distillation. The small model (student) is trained by deeply mimicking the self-attention module, which plays a vital role in Transformer networks, of the large model (teacher). Specifically, we propose distilling the self-attention module of the last Transformer layer of the teacher, which is effective and flexible for the student. Furthermore, we introduce the scaled dot-product between values in the self-attention module as the new deep self-attention knowledge, in addition to the attention distributions (i.e., the scaled dot-product of queries and keys) that have been used in existing works. Moreover, we show that introducing a teacher assistant (Mirzadeh et al., 2019) also helps the distillation of large pre-trained Transformer models. Experimental results demonstrate that our monolingual model outperforms state-of-the-art baselines in different parameter size of student models. In particular, it retains more than 99% accuracy on SQuAD 2.0 and several GLUE benchmark tasks using 50% of the Transformer parameters and computations of the teacher model. We also obtain competitive results in applying deep self-attention distillation to multilingual pre-trained models.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2002.10957, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "51456429",
        "name": "Wenhui Wang",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "49807919",
        "name": "Furu Wei",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "145307652",
        "name": "Li Dong",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "10699417",
        "name": "Hangbo Bao",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "144610884",
        "name": "Nan Yang",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "92660691",
        "name": "Ming Zhou",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "This work presents a simple and effective approach to compress large Transformer (Vaswani et al., 2017) based pre-trained models, termed as deep self-attention distillation, and demonstrates that the monolingual model outperforms state-of-the-art baselines in different parameter size of student models."
  },
  {
    "paperId": "39b492db00faead70bc3f4fb4b0364d94398ffdb",
    "title": "Do Vision Transformers See Like Convolutional Neural Networks?",
    "year": 2021,
    "abstract": "Convolutional neural networks (CNNs) have so far been the de-facto model for visual data. Recent work has shown that (Vision) Transformer models (ViT) can achieve comparable or even superior performance on image classification tasks. This raises a central question: how are Vision Transformers solving these tasks? Are they acting like convolutional networks, or learning entirely different visual representations? Analyzing the internal representation structure of ViTs and CNNs on image classification benchmarks, we find striking differences between the two architectures, such as ViT having more uniform representations across all layers. We explore how these differences arise, finding crucial roles played by self-attention, which enables early aggregation of global information, and ViT residual connections, which strongly propagate features from lower to higher layers. We study the ramifications for spatial localization, demonstrating ViTs successfully preserve input spatial information, with noticeable effects from different classification methods. Finally, we study the effect of (pretraining) dataset scale on intermediate features and transfer learning, and conclude with a discussion on connections to new architectures such as the MLP-Mixer.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2108.08810, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "40297238",
        "name": "M. Raghu",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2465270",
        "name": "Thomas Unterthiner",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "40464924",
        "name": "Simon Kornblith",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2145179541",
        "name": "Chiyuan Zhang",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2841331",
        "name": "Alexey Dosovitskiy",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "Analyzing the internal representation structure of ViTs and CNNs on image classification benchmarks, there are striking differences between the two architectures, such as ViT having more uniform representations across all layers and ViT residual connections, which strongly propagate features from lower to higher layers."
  },
  {
    "paperId": "eb20e67ee20cd4122f3812b695b79a3220f2fe4a",
    "title": "TransFuse: Fusing Transformers and CNNs for Medical Image Segmentation",
    "year": 2021,
    "abstract": "Medical image segmentation - the prerequisite of numerous clinical needs - has been significantly prospered by recent advances in convolutional neural networks (CNNs). However, it exhibits general limitations on modeling explicit long-range relation, and existing cures, resorting to building deep encoders along with aggressive downsampling operations, leads to redundant deepened networks and loss of localized details. Hence, the segmentation task awaits a better solution to improve the efficiency of modeling global contexts while maintaining a strong grasp of low-level details. In this paper, we propose a novel parallel-in-branch architecture, TransFuse, to address this challenge. TransFuse combines Transformers and CNNs in a parallel style, where both global dependency and low-level spatial details can be efficiently captured in a much shallower manner. Besides, a novel fusion technique - BiFusion module is created to efficiently fuse the multi-level features from both branches. Extensive experiments demonstrate that TransFuse achieves the newest state-of-the-art results on both 2D and 3D medical image sets including polyp, skin lesion, hip, and prostate segmentation, with significant parameter decrease and inference speed improvement.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2102.08005, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "49890233",
        "name": "Yundong Zhang",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "30949464",
        "name": "Huiye Liu",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "152280946",
        "name": "Qiang Hu",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "A novel parallel-in-branch architecture, TransFuse, that combines Transformers and CNNs in a parallel style, where both global dependency and low-level spatial details can be efficiently captured in a much shallower manner, with significant parameter decrease and inference speed improvement."
  },
  {
    "paperId": "7d97c17a75beb89f938eaac1d3ca60ac2245fb2e",
    "title": "Faith and Fate: Limits of Transformers on Compositionality",
    "year": 2023,
    "abstract": "Transformer large language models (LLMs) have sparked admiration for their exceptional performance on tasks that demand intricate multi-step reasoning. Yet, these models simultaneously show failures on surprisingly trivial problems. This begs the question: Are these errors incidental, or do they signal more substantial limitations? In an attempt to demystify transformer LLMs, we investigate the limits of these models across three representative compositional tasks -- multi-digit multiplication, logic grid puzzles, and a classic dynamic programming problem. These tasks require breaking problems down into sub-steps and synthesizing these steps into a precise answer. We formulate compositional tasks as computation graphs to systematically quantify the level of complexity, and break down reasoning steps into intermediate sub-procedures. Our empirical findings suggest that transformer LLMs solve compositional tasks by reducing multi-step compositional reasoning into linearized subgraph matching, without necessarily developing systematic problem-solving skills. To round off our empirical study, we provide theoretical arguments on abstract multi-step reasoning problems that highlight how autoregressive generations' performance can rapidly decay with\\,increased\\,task\\,complexity.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.18654, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "46217681",
        "name": "Nouha Dziri",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "50085131",
        "name": "Ximing Lu",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1947172233",
        "name": "Melanie Sclar",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1737850",
        "name": "Xiang Lorraine Li",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2218495662",
        "name": "Liwei Jian",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "51583409",
        "name": "Bill Yuchen Lin",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "119659229",
        "name": "Peter West",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1857797",
        "name": "Chandra Bhagavatula",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "39227408",
        "name": "Ronan Le Bras",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2012510",
        "name": "Jena D. Hwang",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "3313909",
        "name": "Soumya Sanyal",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2129663",
        "name": "S. Welleck",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "145201124",
        "name": "Xiang Ren",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "37907837",
        "name": "Allyson Ettinger",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1753355",
        "name": "Zaïd Harchaoui",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1699545",
        "name": "Yejin Choi",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "The empirical findings suggest that transformer LLMs solve compositional tasks by reducing multi-step compositional reasoning into linearized subgraph matching, without necessarily developing systematic problem-solving skills."
  },
  {
    "paperId": "37246e26163cdd0f5ddd1ea47a5a5019dead8abb",
    "title": "Vision Transformers for Single Image Dehazing",
    "year": 2022,
    "abstract": "Image dehazing is a representative low-level vision task that estimates latent haze-free images from hazy images. In recent years, convolutional neural network-based methods have dominated image dehazing. However, vision Transformers, which has recently made a breakthrough in high-level vision tasks, has not brought new dimensions to image dehazing. We start with the popular Swin Transformer and find that several of its key designs are unsuitable for image dehazing. To this end, we propose DehazeFormer, which consists of various improvements, such as the modified normalization layer, activation function, and spatial information aggregation scheme. We train multiple variants of DehazeFormer on various datasets to demonstrate its effectiveness. Specifically, on the most frequently used SOTS indoor set, our small model outperforms FFA-Net with only 25% #Param and 5% computational cost. To the best of our knowledge, our large model is the first method with the PSNR over 40 dB on the SOTS indoor set, dramatically outperforming the previous state-of-the-art methods. We also collect a large-scale realistic remote sensing dehazing dataset for evaluating the method’s capability to remove highly non-homogeneous haze. We share our code and dataset at https://github.com/IDKiro/DehazeFormer.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2204.03883",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2204.03883, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "152241734",
        "name": "Yuda Song",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2115934613",
        "name": "Zhuqing He",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2112125907",
        "name": "Hui Qian",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2114714362",
        "name": "Xin Du",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "This work proposes DehazeFormer, which consists of various improvements, such as the modified normalization layer, activation function, and spatial information aggregation scheme, which is dramatically outperforming the previous state-of-the-art methods on the SOTS indoor set."
  },
  {
    "paperId": "327a546280368245b551183279b41023393334d4",
    "title": "Reconstructing Hands in 3D with Transformers",
    "year": 2023,
    "abstract": "We present an approach that can reconstruct hands in 3D from monocular input. Our approach for Hand Mesh Recovery, HaMeR, follows a fully transformer-based architecture and can analyze hands with significantly increased accuracy and robustness compared to previous work. The key to HaMeR's success lies in scaling up both the data used for training and the capacity of the deep network for hand reconstruction. For training data, we combine multiple datasets that contain 2D or 3D hand annotations. For the deep model, we use a large scale Vision Transformer architecture. Our final model consistently outperforms the previous baselines on popular 3D hand pose benchmarks. To further evaluate the effect of our design in non-controlled settings, we annotate existing in-the-wild datasets with 2D hand keypoint annotations. On this newly collected dataset of annotations, HInt, we demonstrate significant improvements over existing baselines. We will make our code, data and models publicly available upon publication. We make our code, data and models available on the project website: https://geopavlakos.github.io/hamer/. “It is because of his being armed with hands that man is the most intelligent animal.” Anaxagoras",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2312.05251",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.05251, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "2829330",
        "name": "G. Pavlakos",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2058873326",
        "name": "Dandan Shan",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "30407997",
        "name": "Ilija Radosavovic",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "20615377",
        "name": "Angjoo Kanazawa",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1786435",
        "name": "David F. Fouhey",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2257249601",
        "name": "Jitendra Malik",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "This work follows a fully transformer-based architecture and can analyze hands with significantly increased accuracy and robustness compared to previous work, and demonstrates significant improvements over existing baselines on popular 3D hand pose benchmarks."
  },
  {
    "paperId": "6709d5583f658f589ae6a2184805933aceb18849",
    "title": "Twins: Revisiting the Design of Spatial Attention in Vision Transformers",
    "year": 2021,
    "abstract": "Very recently, a variety of vision transformer architectures for dense prediction tasks have been proposed and they show that the design of spatial attention is critical to their success in these tasks. In this work, we revisit the design of the spatial attention and demonstrate that a carefully-devised yet simple spatial attention mechanism performs favourably against the state-of-the-art schemes. As a result, we propose two vision transformer architectures, namely, Twins-PCPVT and Twins-SVT. Our proposed architectures are highly-efficient and easy to implement, only involving matrix multiplications that are highly optimized in modern deep learning frameworks. More importantly, the proposed architectures achieve excellent performance on a wide range of visual tasks, including image level classification as well as dense detection and segmentation. The simplicity and strong performance suggest that our proposed architectures may serve as stronger backbones for many vision tasks. Our code is released at https://github.com/Meituan-AutoML/Twins .",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2104.13840, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "27628828",
        "name": "Xiangxiang Chu",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2069520672",
        "name": "Zhi Tian",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2108035065",
        "name": "Yuqing Wang",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": null,
        "name": "Bo Zhang",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1558541225",
        "name": "Haibing Ren",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "49141839",
        "name": "Xiaolin Wei",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2065044626",
        "name": "Huaxia Xia",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "12459603",
        "name": "Chunhua Shen",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "This work revisits the design of the spatial attention and demonstrates that a carefully-devised yet simple spatial attention mechanism performs favourably against the state-of-the-art schemes."
  },
  {
    "paperId": "4be7d1524edb0137599a5cc95f72844b85a52fe1",
    "title": "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale",
    "year": 2022,
    "abstract": "Large language models have been widely adopted but require significant GPU memory for inference. We develop a procedure for Int8 matrix multiplication for feed-forward and attention projection layers in transformers, which cut the memory needed for inference by half while retaining full precision performance. With our method, a 175B parameter 16/32-bit checkpoint can be loaded, converted to Int8, and used immediately without performance degradation. This is made possible by understanding and working around properties of highly systematic emergent features in transformer language models that dominate attention and transformer predictive performance. To cope with these features, we develop a two-part quantization procedure, LLM.int8(). We first use vector-wise quantization with separate normalization constants for each inner product in the matrix multiplication, to quantize most of the features. However, for the emergent outliers, we also include a new mixed-precision decomposition scheme, which isolates the outlier feature dimensions into a 16-bit matrix multiplication while still more than 99.9% of values are multiplied in 8-bit. Using LLM.int8(), we show empirically it is possible to perform inference in LLMs with up to 175B parameters without any performance degradation. This result makes such models much more accessible, for example making it possible to use OPT-175B/BLOOM on a single server with consumer GPUs. We open-source our software.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2208.07339",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2208.07339, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "3239480",
        "name": "Tim Dettmers",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "35084211",
        "name": "M. Lewis",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2037496520",
        "name": "Younes Belkada",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1982950",
        "name": "Luke Zettlemoyer",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "A procedure for Int8 matrix multiplication for feed-forward and attention projection layers in transformers, which cut the memory needed for inference by half while retaining full precision performance, and makes such models much more accessible."
  },
  {
    "paperId": "7e5709d81558d3ef4265de29ea75931afeb1f2dd",
    "title": "Efficient Transformers: A Survey",
    "year": 2020,
    "abstract": "Transformer model architectures have garnered immense interest lately due to their effectiveness across a range of domains like language, vision, and reinforcement learning. In the field of natural language processing for example, Transformers have become an indispensable staple in the modern deep learning stack. Recently, a dizzying number of “X-former” models have been proposed—Reformer, Linformer, Performer, Longformer, to name a few—which improve upon the original Transformer architecture, many of which make improvements around computational and memory efficiency. With the aim of helping the avid researcher navigate this flurry, this article characterizes a large and thoughtful selection of recent efficiency-flavored “X-former” models, providing an organized and comprehensive overview of existing work and models across multiple domains.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/3530811",
      "status": "BRONZE",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2009.06732, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "144447820",
        "name": "Yi Tay",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "3226635",
        "name": "Mostafa Dehghani",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "11774695",
        "name": "Dara Bahri",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1680617",
        "name": "Donald Metzler",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "This article characterizes a large and thoughtful selection of recent efficiency-flavored “X-former” models, providing an organized and comprehensive overview of existing work and models across multiple domains."
  },
  {
    "paperId": "9ffc8d59270b01def8bde81a8ec1d759a2029dbb",
    "title": "Convolutions are competitive with transformers for protein sequence pretraining",
    "year": 2024,
    "abstract": "Pretrained protein sequence language models have been shown to improve the performance of many prediction tasks, and are now routinely integrated into bioinformatics tools. However, these models largely rely on the Transformer architecture, which scales quadratically with sequence length in both run-time and memory. Therefore, state-of-the-art models have limitations on sequence length. To address this limitation, we investigated if convolutional neural network (CNN) architectures, which scale linearly with sequence length, could be as effective as transformers in protein language models. With masked language model pretraining, CNNs are competitive to and occasionally superior to Transformers across downstream applications while maintaining strong performance on sequences longer than those allowed in the current state-of-the-art Transformer models. Our work suggests that computational efficiency can be improved without sacrificing performance simply by using a CNN architecture instead of a Transformer, and emphasizes the importance of disentangling pretraining task and model architecture.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "https://www.biorxiv.org/content/biorxiv/early/2022/05/25/2022.05.19.492714.full.pdf",
      "status": "GREEN",
      "license": "public-domain",
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1101/2022.05.19.492714?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1101/2022.05.19.492714, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "47271615",
        "name": "Kevin Kaichuang Yang",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "48272336",
        "name": "Alex X. Lu",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2723245",
        "name": "Nicoló Fusi",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "This work suggests that computational efficiency can be improved without sacrificing performance simply by using a CNN architecture instead of a Transformer, and emphasizes the importance of disentangling pretraining task and model architecture."
  },
  {
    "paperId": "de32da8f5c6a50a6c311e9357ba16aa7d05a1bc9",
    "title": "What Can Transformers Learn In-Context? A Case Study of Simple Function Classes",
    "year": 2022,
    "abstract": "In-context learning refers to the ability of a model to condition on a prompt sequence consisting of in-context examples (input-output pairs corresponding to some task) along with a new query input, and generate the corresponding output. Crucially, in-context learning happens only at inference time without any parameter updates to the model. While large language models such as GPT-3 exhibit some ability to perform in-context learning, it is unclear what the relationship is between tasks on which this succeeds and what is present in the training data. To make progress towards understanding in-context learning, we consider the well-defined problem of training a model to in-context learn a function class (e.g., linear functions): that is, given data derived from some functions in the class, can we train a model to in-context learn\"most\"functions from this class? We show empirically that standard Transformers can be trained from scratch to perform in-context learning of linear functions -- that is, the trained model is able to learn unseen linear functions from in-context examples with performance comparable to the optimal least squares estimator. In fact, in-context learning is possible even under two forms of distribution shift: (i) between the training data of the model and inference-time prompts, and (ii) between the in-context examples and the query input during inference. We also show that we can train Transformers to in-context learn more complex function classes -- namely sparse linear functions, two-layer neural networks, and decision trees -- with performance that matches or exceeds task-specific learning algorithms. Our code and models are available at https://github.com/dtsip/in-context-learning .",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2208.01066",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2208.01066, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "3052074",
        "name": "Shivam Garg",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2754804",
        "name": "Dimitris Tsipras",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "145419642",
        "name": "Percy Liang",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1806083",
        "name": "G. Valiant",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "It is shown empirically that standard Transformers can be trained from scratch to perform in-context learning of linear functions -- that is, the trained model is able to learn unseen linear functions from in- context examples with performance comparable to the optimal least squares estimator."
  },
  {
    "paperId": "525d93a382f6e7873b5d8a2e0713eb3dff7fb250",
    "title": "Transformers learn in-context by gradient descent",
    "year": 2022,
    "abstract": "At present, the mechanisms of in-context learning in Transformers are not well understood and remain mostly an intuition. In this paper, we suggest that training Transformers on auto-regressive objectives is closely related to gradient-based meta-learning formulations. We start by providing a simple weight construction that shows the equivalence of data transformations induced by 1) a single linear self-attention layer and by 2) gradient-descent (GD) on a regression loss. Motivated by that construction, we show empirically that when training self-attention-only Transformers on simple regression tasks either the models learned by GD and Transformers show great similarity or, remarkably, the weights found by optimization match the construction. Thus we show how trained Transformers become mesa-optimizers i.e. learn models by gradient descent in their forward pass. This allows us, at least in the domain of regression problems, to mechanistically understand the inner workings of in-context learning in optimized Transformers. Building on this insight, we furthermore identify how Transformers surpass the performance of plain gradient descent by learning an iterative curvature correction and learn linear models on deep data representations to solve non-linear regression tasks. Finally, we discuss intriguing parallels to a mechanism identified to be crucial for in-context learning termed induction-head (Olsson et al., 2022) and show how it could be understood as a specific case of in-context learning by gradient descent learning within Transformers. Code to reproduce the experiments can be found at https://github.com/google-research/self-organising-systems/tree/master/transformers_learn_icl_by_gd .",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2212.07677",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2212.07677, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "145167136",
        "name": "J. Oswald",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "51440033",
        "name": "Eyvind Niklasson",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "72142084",
        "name": "E. Randazzo",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "3105061",
        "name": "J. Sacramento",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2050989525",
        "name": "A. Mordvintsev",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "3422677",
        "name": "A. Zhmoginov",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "3316311",
        "name": "Max Vladymyrov",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "It is suggested that training Transformers on auto-regressive objectives is closely related to gradient-based meta-learning formulations and how Transformers surpass the performance of plain gradient descent by learning an iterative curvature correction and learn linear models on deep data representations to solve non-linear regression tasks."
  },
  {
    "paperId": "68cda2cfefe8c21dc64fee55deab87672a517d39",
    "title": "Scaling Vision Transformers to Gigapixel Images via Hierarchical Self-Supervised Learning",
    "year": 2022,
    "abstract": "Vision Transformers (ViTs) and their multi-scale and hierarchical variations have been successful at capturing image representations but their use has been generally studied for low-resolution images (e.g. 256 × 256, 384 × 384). For gigapixel whole-slide imaging (WSI) in computational pathology, WSIs can be as large as 150000 × 150000 pixels at 20 × magnification and exhibit a hierarchical structure of visual tokens across varying resolutions: from 16 × 16 images capturing individual cells, to 4096 × 4096 images characterizing interactions within the tissue microenvironment. We introduce a new ViT architecture called the Hierarchical Image Pyramid Transformer (HIPT), which leverages the natural hierarchical structure inherent in WSIs using two levels of self-supervised learning to learn high-resolution image representations. HIPT is pretrained across 33 cancer types using 10,678 gigapixel WSIs, 408,218 4096 × 4096 images, and 104M 256 × 256 images. We benchmark HIPT representations on 9 slide-level tasks, and demonstrate that: 1) HIPT with hierarchical pretraining outperforms current state-of-the-art methods for cancer subtyping and survival prediction, 2) self-supervised ViTs are able to model important inductive biases about the hierarchical structure of phenotypes in the tumor microenvironment.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2206.02647",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2206.02647, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "2108279369",
        "name": "Richard J. Chen",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2145775609",
        "name": "Chengkuan Chen",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2154570723",
        "name": "Yicong Li",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2242468870",
        "name": "Tiffany Y. Chen",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "3430870",
        "name": "A. Trister",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "145253891",
        "name": "R. G. Krishnan",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "37122655",
        "name": "Faisal Mahmood",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "HIPT with hierarchical pretraining outperforms current state-of-the-art methods for cancer subtyping and survival prediction, and self-supervised ViTs are able to model important inductive biases about the hierarchical structure of phenotypes in the tumor microenvironment."
  },
  {
    "paperId": "8064d3873c646dc9ff949d72c54c634a906fc092",
    "title": "Non-stationary Transformers: Exploring the Stationarity in Time Series Forecasting",
    "year": 2022,
    "abstract": "Transformers have shown great power in time series forecasting due to their global-range modeling ability. However, their performance can degenerate terribly on non-stationary real-world data in which the joint distribution changes over time. Previous studies primarily adopt stationarization to attenuate the non-stationarity of original series for better predictability. But the stationarized series deprived of inherent non-stationarity can be less instructive for real-world bursty events forecasting. This problem, termed over-stationarization in this paper, leads Transformers to generate indistinguishable temporal attentions for different series and impedes the predictive capability of deep models. To tackle the dilemma between series predictability and model capability, we propose Non-stationary Transformers as a generic framework with two interdependent modules: Series Stationarization and De-stationary Attention. Concretely, Series Stationarization unifies the statistics of each input and converts the output with restored statistics for better predictability. To address the over-stationarization problem, De-stationary Attention is devised to recover the intrinsic non-stationary information into temporal dependencies by approximating distinguishable attentions learned from raw series. Our Non-stationary Transformers framework consistently boosts mainstream Transformers by a large margin, which reduces MSE by 49.43% on Transformer, 47.34% on Informer, and 46.89% on Reformer, making them the state-of-the-art in time series forecasting. Code is available at this repository: https://github.com/thuml/Nonstationary_Transformers.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2205.14415, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "2144386711",
        "name": "Yong Liu",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2051867856",
        "name": "Haixu Wu",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2144499343",
        "name": "Jianmin Wang",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2054275000",
        "name": "Mingsheng Long",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "The proposed Non-stationary Transformers framework consistently boosts mainstream Transformers by a large margin, which reduces MSE by 49.43% on Transformer, 47.34% on Informer, and 46.89% on Reformer, making them the state-of-the-art in time series forecasting."
  },
  {
    "paperId": "430bab3890e1e52c4c1f74900b0e408e47a1cb8f",
    "title": "How Do Vision Transformers Work?",
    "year": 2022,
    "abstract": "The success of multi-head self-attentions (MSAs) for computer vision is now indisputable. However, little is known about how MSAs work. We present fundamental explanations to help better understand the nature of MSAs. In particular, we demonstrate the following properties of MSAs and Vision Transformers (ViTs): (1) MSAs improve not only accuracy but also generalization by flattening the loss landscapes. Such improvement is primarily attributable to their data specificity, not long-range dependency. On the other hand, ViTs suffer from non-convex losses. Large datasets and loss landscape smoothing methods alleviate this problem; (2) MSAs and Convs exhibit opposite behaviors. For example, MSAs are low-pass filters, but Convs are high-pass filters. Therefore, MSAs and Convs are complementary; (3) Multi-stage neural networks behave like a series connection of small individual models. In addition, MSAs at the end of a stage play a key role in prediction. Based on these insights, we propose AlterNet, a model in which Conv blocks at the end of a stage are replaced with MSA blocks. AlterNet outperforms CNNs not only in large data regimes but also in small data regimes. The code is available at https://github.com/xxxnell/how-do-vits-work.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2202.06709, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "150248298",
        "name": "Namuk Park",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2917665",
        "name": "Songkuk Kim",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "AlterNet is proposed, a model in which Conv blocks at the end of a stage are replaced with MSA blocks, which outperforms CNNs not only in large data regimes but also in small data regimes."
  },
  {
    "paperId": "dd1139cfc609c2f3263d02e97176d5275caebc0a",
    "title": "EfficientFormer: Vision Transformers at MobileNet Speed",
    "year": 2022,
    "abstract": "Vision Transformers (ViT) have shown rapid progress in computer vision tasks, achieving promising results on various benchmarks. However, due to the massive number of parameters and model design, \\textit{e.g.}, attention mechanism, ViT-based models are generally times slower than lightweight convolutional networks. Therefore, the deployment of ViT for real-time applications is particularly challenging, especially on resource-constrained hardware such as mobile devices. Recent efforts try to reduce the computation complexity of ViT through network architecture search or hybrid design with MobileNet block, yet the inference speed is still unsatisfactory. This leads to an important question: can transformers run as fast as MobileNet while obtaining high performance? To answer this, we first revisit the network architecture and operators used in ViT-based models and identify inefficient designs. Then we introduce a dimension-consistent pure transformer (without MobileNet blocks) as a design paradigm. Finally, we perform latency-driven slimming to get a series of final models dubbed EfficientFormer. Extensive experiments show the superiority of EfficientFormer in performance and speed on mobile devices. Our fastest model, EfficientFormer-L1, achieves $79.2\\%$ top-1 accuracy on ImageNet-1K with only $1.6$ ms inference latency on iPhone 12 (compiled with CoreML), which runs as fast as MobileNetV2$\\times 1.4$ ($1.6$ ms, $74.7\\%$ top-1), and our largest model, EfficientFormer-L7, obtains $83.3\\%$ accuracy with only $7.0$ ms latency. Our work proves that properly designed transformers can reach extremely low latency on mobile devices while maintaining high performance.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2206.01191",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2206.01191, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "1527091497",
        "name": "Yanyu Li",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "9347641",
        "name": "Geng Yuan",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2167854921",
        "name": "Yang Wen",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2167582358",
        "name": "Eric Hu",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "143998839",
        "name": "Georgios Evangelidis",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "145582202",
        "name": "S. Tulyakov",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2136922252",
        "name": "Yanzhi Wang",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2111473627",
        "name": "Jian Ren",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "This work proves that properly designed transformers can reach extremely low latency on mobile devices while maintaining high performance."
  },
  {
    "paperId": "066c143b427571fb5568f2c581ea9066478d2e55",
    "title": "Separable Self-attention for Mobile Vision Transformers",
    "year": 2022,
    "abstract": "Mobile vision transformers (MobileViT) can achieve state-of-the-art performance across several mobile vision tasks, including classification and detection. Though these models have fewer parameters, they have high latency as compared to convolutional neural network-based models. The main efficiency bottleneck in MobileViT is the multi-headed self-attention (MHA) in transformers, which requires $O(k^2)$ time complexity with respect to the number of tokens (or patches) $k$. Moreover, MHA requires costly operations (e.g., batch-wise matrix multiplication) for computing self-attention, impacting latency on resource-constrained devices. This paper introduces a separable self-attention method with linear complexity, i.e. $O(k)$. A simple yet effective characteristic of the proposed method is that it uses element-wise operations for computing self-attention, making it a good choice for resource-constrained devices. The improved model, MobileViTv2, is state-of-the-art on several mobile vision tasks, including ImageNet object classification and MS-COCO object detection. With about three million parameters, MobileViTv2 achieves a top-1 accuracy of 75.6% on the ImageNet dataset, outperforming MobileViT by about 1% while running $3.2\\times$ faster on a mobile device. Our source code is available at: \\url{https://github.com/apple/ml-cvnets}",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2206.02680",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2206.02680, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "144839857",
        "name": "Sachin Mehta",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "143887493",
        "name": "Mohammad Rastegari",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "A separable self-attention method with linear complexity, making it a good choice for resource-constrained devices, and the improved model, MobileViTv2, is state-of-the-art on several mobile vision tasks, including ImageNet object classification and MS-COCO object detection."
  },
  {
    "paperId": "58a3fedc03ab9f5908c077c115eff4c8d2d87660",
    "title": "ActionFormer: Localizing Moments of Actions with Transformers",
    "year": 2022,
    "abstract": "Self-attention based Transformer models have demonstrated impressive results for image classification and object detection, and more recently for video understanding. Inspired by this success, we investigate the application of Transformer networks for temporal action localization in videos. To this end, we present ActionFormer -- a simple yet powerful model to identify actions in time and recognize their categories in a single shot, without using action proposals or relying on pre-defined anchor windows. ActionFormer combines a multiscale feature representation with local self-attention, and uses a light-weighted decoder to classify every moment in time and estimate the corresponding action boundaries. We show that this orchestrated design results in major improvements upon prior works. Without bells and whistles, ActionFormer achieves 71.0% mAP at tIoU=0.5 on THUMOS14, outperforming the best prior model by 14.1 absolute percentage points. Further, ActionFormer demonstrates strong results on ActivityNet 1.3 (36.6% average mAP) and EPIC-Kitchens 100 (+13.5% average mAP over prior works). Our code is available at http://github.com/happyharrycn/actionformer_release.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2202.07925, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "2200059252",
        "name": "Chen-Lin Zhang",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2115904194",
        "name": "Jianxin Wu",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "47002659",
        "name": "Yin Li",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "This work presents ActionFormer -- a simple yet powerful model to identify actions in time and recognize their categories in a single shot, without using action proposals or relying on pre-defined anchor windows."
  },
  {
    "paperId": "c5e6f0c52c1f91086879f46120efa79e96158eba",
    "title": "Cross-view Transformers for real-time Map-view Semantic Segmentation",
    "year": 2022,
    "abstract": "We present cross-view transformers, an efficient attention-based model for map-view semantic segmentation from multiple cameras. Our architecture implicitly learns a mapping from individual camera views into a canonical map-view representation using a camera-aware cross-view attention mechanism. Each camera uses positional embeddings that depend on its intrinsic and extrinsic calibration. These embeddings allow a transformer to learn the mapping across different views without ever explicitly modeling it geometrically. The architecture consists of a convolutional image encoder for each view and cross-view transformer layers to infer a map-view semantic segmentation. Our model is simple, easily parallelizable, and runs in realtime. The presented architecture performs at state-of-the-art on the nuScenes dataset, with 4x faster inference speeds. Code is available at https://github.com/bradyz/cross_view_transformers.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2205.02833",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2205.02833, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "144576462",
        "name": "Brady Zhou",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2305682524",
        "name": "Philipp Krahenbuhl",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "The architecture implicitly learns a mapping from individual camera views into a canonical map-view representation using a camera-aware cross-view attention mechanism, performing at state-of-the-art on the nuScenes dataset, with 4x faster inference speeds."
  },
  {
    "paperId": "dd2819016c6bf244c39b3e6707b60389bbdbcd21",
    "title": "Point-BERT: Pre-training 3D Point Cloud Transformers with Masked Point Modeling",
    "year": 2021,
    "abstract": "We present Point-BERT, a new paradigm for learning Transformers to generalize the concept of BERT [8] to 3D point cloud. Inspired by BERT, we devise a Masked Point Modeling (MPM) task to pre-train point cloud Transformers. Specifically, we first divide a point cloud into several local point patches, and a point cloud Tokenizer with a discrete Variational AutoEncoder (dVAE) is designed to generate discrete point tokens containing meaningful local information. Then, we randomly mask out some patches of input point clouds and feed them into the backbone Transformers. The pre-training objective is to recover the original point tokens at the masked locations under the supervision of point tokens obtained by the Tokenizer. Extensive experiments demonstrate that the proposed BERT-style pre-training strategy significantly improves the performance of standard point cloud Transformers. Equipped with our pre-training strategy, we show that a pure Transformer architecture attains 93.8% accuracy on ModelNet40 and 83.1% accuracy on the hardest setting of ScanObjectNN, surpassing carefully designed point cloud models with much fewer hand-made designs. We also demonstrate that the representations learned by Point-BERT transfer well to new tasks and domains, where our models largely advance the state-of-the-art of few-shot point cloud classification task. The code and pre-trained models are available at https://github.com/lulutang0608/Point-BERT.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2111.14819",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2111.14819, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "2116329737",
        "name": "Xumin Yu",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "145016965",
        "name": "Lulu Tang",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2052552620",
        "name": "Yongming Rao",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "34097174",
        "name": "Tiejun Huang",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "48128428",
        "name": "Jie Zhou",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1697700",
        "name": "Jiwen Lu",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "Point-BERT, a new paradigm for learning Transformers to generalize the concept of BERT to 3D point cloud, is presented and it is shown that a pure Transformer architecture attains 93.8% accuracy on ModelNet40 and 83.1% accuracy in the hardest setting of ScanObjectNN, surpassing carefully designed point cloud models with much fewer hand-made designs."
  },
  {
    "paperId": "dbdcabd0444ad50b68ee09e30f39b66e9068f5d2",
    "title": "DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification",
    "year": 2021,
    "abstract": "Attention is sparse in vision transformers. We observe the final prediction in vision transformers is only based on a subset of most informative tokens, which is sufficient for accurate image recognition. Based on this observation, we propose a dynamic token sparsification framework to prune redundant tokens progressively and dynamically based on the input. Specifically, we devise a lightweight prediction module to estimate the importance score of each token given the current features. The module is added to different layers to prune redundant tokens hierarchically. To optimize the prediction module in an end-to-end manner, we propose an attention masking strategy to differentiably prune a token by blocking its interactions with other tokens. Benefiting from the nature of self-attention, the unstructured sparse tokens are still hardware friendly, which makes our framework easy to achieve actual speed-up. By hierarchically pruning 66% of the input tokens, our method greatly reduces 31%~37% FLOPs and improves the throughput by over 40% while the drop of accuracy is within 0.5% for various vision transformers. Equipped with the dynamic token sparsification framework, DynamicViT models can achieve very competitive complexity/accuracy trade-offs compared to state-of-the-art CNNs and vision transformers on ImageNet. Code is available at https://github.com/raoyongming/DynamicViT",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2106.02034, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "39358728",
        "name": "Yongming Rao",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2118223312",
        "name": "Wenliang Zhao",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "67215934",
        "name": "Benlin Liu",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1697700",
        "name": "Jiwen Lu",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "49178343",
        "name": "Jie Zhou",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1793529",
        "name": "Cho-Jui Hsieh",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "A dynamic token sparsification framework to prune redundant tokens progressively and dynamically based on the input and an attention masking strategy to differentiably prune a token by blocking its interactions with other tokens is proposed."
  },
  {
    "paperId": "d8d2e574965fe733eb1416e03df2b5c2914fc530",
    "title": "A Survey of Transformers",
    "year": 2021,
    "abstract": null,
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "https://doi.org/10.1016/j.aiopen.2022.10.001",
      "status": "GOLD",
      "license": "CCBY",
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2106.04554, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "2115348804",
        "name": "Tianyang Lin",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2115828967",
        "name": "Yuxin Wang",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2144226697",
        "name": "Xiangyang Liu",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1767521",
        "name": "Xipeng Qiu",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "This survey provides a comprehensive review of various Transformer variants and proposes a new taxonomy of X-formers from three perspectives: architectural modification, pre-training, and applications."
  },
  {
    "paperId": "6b66447a6039acd79c6810ff07bb378c3e79b8c6",
    "title": "Comparing Vision Transformers and Convolutional Neural Networks for Image Classification: A Literature Review",
    "year": 2023,
    "abstract": "Transformers are models that implement a mechanism of self-attention, individually weighting the importance of each part of the input data. Their use in image classification tasks is still somewhat limited since researchers have so far chosen Convolutional Neural Networks for image classification and transformers were more targeted to Natural Language Processing (NLP) tasks. Therefore, this paper presents a literature review that shows the differences between Vision Transformers (ViT) and Convolutional Neural Networks. The state of the art that used the two architectures for image classification was reviewed and an attempt was made to understand what factors may influence the performance of the two deep learning architectures based on the datasets used, image size, number of target classes (for the classification problems), hardware, and evaluated architectures and top results. The objective of this work is to identify which of the architectures is the best for image classification and under what conditions. This paper also describes the importance of the Multi-Head Attention mechanism for improving the performance of ViT in image classification.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "https://www.mdpi.com/2076-3417/13/9/5521/pdf?version=1683174100",
      "status": "GOLD",
      "license": "CCBY",
      "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/app13095521?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/app13095521, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "2073403590",
        "name": "J. Maurício",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "145908670",
        "name": "Inês Domingues",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1716043",
        "name": "Jorge Bernardino",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "An attempt was made to understand what factors may influence the performance of the two deep learning architectures based on the datasets used, image size, number of target classes, hardware, and evaluated architectures and top results."
  },
  {
    "paperId": "6f6e2e0311589a9af045f6acd00b7dee6d19fce4",
    "title": "The Impact of Positional Encoding on Length Generalization in Transformers",
    "year": 2023,
    "abstract": "Length generalization, the ability to generalize from small training context sizes to larger ones, is a critical challenge in the development of Transformer-based language models. Positional encoding (PE) has been identified as a major factor influencing length generalization, but the exact impact of different PE schemes on extrapolation in downstream tasks remains unclear. In this paper, we conduct a systematic empirical study comparing the length generalization performance of decoder-only Transformers with five different position encoding approaches including Absolute Position Embedding (APE), T5's Relative PE, ALiBi, and Rotary, in addition to Transformers without positional encoding (NoPE). Our evaluation encompasses a battery of reasoning and mathematical tasks. Our findings reveal that the most commonly used positional encoding methods, such as ALiBi, Rotary, and APE, are not well suited for length generalization in downstream tasks. More importantly, NoPE outperforms other explicit positional encoding methods while requiring no additional computation. We theoretically demonstrate that NoPE can represent both absolute and relative PEs, but when trained with SGD, it mostly resembles T5's relative PE attention patterns. Finally, we find that scratchpad is not always helpful to solve length generalization and its format highly impacts the model's performance. Overall, our work suggests that explicit position embeddings are not essential for decoder-only Transformers to generalize well to longer sequences.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.19466",
      "status": "CLOSED",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.19466, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "1754452702",
        "name": "Amirhossein Kazemnejad",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "8350409",
        "name": "Inkit Padhi",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1704263",
        "name": "K. Ramamurthy",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1730372",
        "name": "Payel Das",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "145732771",
        "name": "Siva Reddy",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "This work suggests that explicit position embeddings are not essential for decoder-only Transformers to generalize well to longer sequences, and NoPE outperforms other explicit positional encoding methods while requiring no additional computation."
  },
  {
    "paperId": "42c46c38a5f9336700f173e39d274d333a4557eb",
    "title": "A survey of the vision transformers and their CNN-transformer based variants",
    "year": 2023,
    "abstract": "Vision transformers have become popular as a possible substitute to convolutional neural networks (CNNs) for a variety of computer vision applications. These transformers, with their ability to focus on global relationships in images, offer large learning capacity. However, they may suffer from limited generalization as they do not tend to model local correlation in images. Recently, in vision transformers hybridization of both the convolution operation and self-attention mechanism has emerged, to exploit both the local and global image representations. These hybrid vision transformers, also referred to as CNN-Transformer architectures, have demonstrated remarkable results in vision applications. Given the rapidly growing number of hybrid vision transformers, it has become necessary to provide a taxonomy and explanation of these hybrid architectures. This survey presents a taxonomy of the recent vision transformer architectures and more specifically that of the hybrid vision transformers. Additionally, the key features of these architectures such as the attention mechanisms, positional embeddings, multi-scale processing, and convolution are also discussed. In contrast to the previous survey papers that are primarily focused on individual vision transformer architectures or CNNs, this survey uniquely emphasizes the emerging trend of hybrid vision transformers. By showcasing the potential of hybrid vision transformers to deliver exceptional performance across a range of computer vision tasks, this survey sheds light on the future directions of this rapidly evolving architecture.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.09880",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.09880, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "2241395568",
        "name": "Asifullah Khan",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2029680920",
        "name": "Zunaira Rauf",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "23690960",
        "name": "A. Sohail",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2149888758",
        "name": "Abdul Rehman Khan",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2191906442",
        "name": "Hifsa Asif",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2169419730",
        "name": "Aqsa Asif",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2217505318",
        "name": "Umair Farooq",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "This survey presents a taxonomy of the recent vision transformer architectures and more specifically that of the hybrid vision transformers, and sheds light on the future directions of this rapidly evolving architecture."
  },
  {
    "paperId": "8b0357f1bceb9cf7a5629b0ba3acb5660edf90b2",
    "title": "Advances in Medical Image Analysis with Vision Transformers: A Comprehensive Review",
    "year": 2023,
    "abstract": "The remarkable performance of the Transformer architecture in natural language processing has recently also triggered broad interest in Computer Vision. Among other merits, Transformers are witnessed as capable of learning long-range dependencies and spatial correlations, which is a clear advantage over convolutional neural networks (CNNs), which have been the de facto standard in Computer Vision problems so far. Thus, Transformers have become an integral part of modern medical image analysis. In this review, we provide an encyclopedic review of the applications of Transformers in medical imaging. Specifically, we present a systematic and thorough review of relevant recent Transformer literature for different medical image analysis tasks, including classification, segmentation, detection, registration, synthesis, and clinical report generation. For each of these applications, we investigate the novelty, strengths and weaknesses of the different proposed strategies and develop taxonomies highlighting key properties and contributions. Further, if applicable, we outline current benchmarks on different datasets. Finally, we summarize key challenges and discuss different future research directions. In addition, we have provided cited papers with their corresponding implementations in https://github.com/mindflow-institue/Awesome-Transformer.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2301.03505",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2301.03505, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "1763181",
        "name": "Reza Azad",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2131612425",
        "name": "A. Kazerouni",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1491490451",
        "name": "Moein Heidari",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1411236504",
        "name": "Ehsan Khodapanah Aghdam",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2167168779",
        "name": "Amir Molaei",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2192610282",
        "name": "Yiwei Jia",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "145895943",
        "name": "Abin Jose",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2190045909",
        "name": "Rijo Roy",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1737693",
        "name": "D. Merhof",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "This review presents a systematic and thorough review of relevant recent Transformer literature for different medical image analysis tasks, including classification, segmentation, detection, registration, synthesis, and clinical report generation."
  },
  {
    "paperId": "80da612e1831b8c11539180871843cff6dfaac90",
    "title": "PoinTr: Diverse Point Cloud Completion with Geometry-Aware Transformers",
    "year": 2021,
    "abstract": "Point clouds captured in real-world applications are of-ten incomplete due to the limited sensor resolution, single viewpoint, and occlusion. Therefore, recovering the complete point clouds from partial ones becomes an indispensable task in many practical applications. In this paper, we present a new method that reformulates point cloud completion as a set-to-set translation problem and design a new model, called PoinTr that adopts a transformer encoder-decoder architecture for point cloud completion. By rep-resenting the point cloud as a set of unordered groups of points with position embeddings, we convert the point cloud to a sequence of point proxies and employ the transformers for point cloud generation. To facilitate transformers to better leverage the inductive bias about 3D geometric structures of point clouds, we further devise a geometry-aware block that models the local geometric relationships explicitly. The migration of transformers enables our model to better learn structural knowledge and preserve detailed information for point cloud completion. Furthermore, we propose two more challenging benchmarks with more diverse incomplete point clouds that can better reflect the real-world scenarios to promote future research. Experimental results show that our method outperforms state-of-the-art methods by a large margin on both the new bench-marks and the existing ones. Code is available at https://github.com/yuxumin/PoinTr.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2108.08839",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2108.08839, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "2116330410",
        "name": "Xumin Yu",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "39358728",
        "name": "Yongming Rao",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2142663191",
        "name": "Ziyi Wang",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2124814824",
        "name": "Zuyan Liu",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1697700",
        "name": "Jiwen Lu",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2108485135",
        "name": "Jie Zhou",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "A new method is presented that reformulates point cloud completion as a set-to-set translation problem and design a new model, called PoinTr, that adopts a transformer encoder-decoder architecture for point clouds completion that outperforms state-of-the-art methods by a large margin."
  },
  {
    "paperId": "a03738b89bd2e6554fde2725f01f69552f2389ca",
    "title": "Feature Shrinkage Pyramid for Camouflaged Object Detection with Transformers",
    "year": 2023,
    "abstract": "Vision transformers have recently shown strong global context modeling capabilities in camouflaged object detection. However, they suffer from two major limitations: less effective locality modeling and insufficient feature aggregation in decoders, which are not conducive to camou-flaged object detection that explores subtle cues from indistinguishable backgrounds. To address these issues, in this paper, we propose a novel transformer-based Feature Shrinkage Pyramid Network (FSPNet), which aims to hierarchically decode locality-enhanced neighboring transformer features through progressive shrinking for camou-flaged object detection. Specifically, we propose a non-local token enhancement module (NL-TEM) that employs the non-local mechanism to interact neighboring tokens and explore graph-based high-order relations within tokens to enhance local representations of transformers. Moreover, we design a feature shrinkage decoder (FSD) with adjacent interaction modules (AIM), which progressively aggregates adjacent transformer features through a layer-by-layer shrinkage pyramid to accumulate imperceptible but effective cues as much as possible for object information decoding. Extensive quantitative and qualitative experiments demonstrate that the proposed model significantly outperforms the existing 24 competitors on three challenging COD benchmark datasets under six widely-used evaluation metrics. Our code is publicly available at https://github.com/ZhouHuang23/FSPNet.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2303.14816",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2303.14816, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "153858004",
        "name": "Zhou Huang",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "46447978",
        "name": "Hang Dai",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "3408056",
        "name": "Tian-Zhu Xiang",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2146295031",
        "name": "Shuo Wang",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "103302856",
        "name": "Huaixin Chen",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2187055155",
        "name": "Jie Qin",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2054473409",
        "name": "Huan Xiong",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "A novel transformer-based Feature Shrinkage Pyramid Network (FSPNet), which aims to hierarchically decode locality-enhanced neighboring transformer features through progressive shrinking for camou-flaged object detection and significantly outperforms the existing 24 competitors on three challenging COD benchmark datasets under six widely-used evaluation metrics."
  },
  {
    "paperId": "610b302950a19acef1c45456111dcd495f638c18",
    "title": "ConViT: improving vision transformers with soft convolutional inductive biases",
    "year": 2021,
    "abstract": "Convolutional architectures have proven to be extremely successful for vision tasks. Their hard inductive biases enable sample-efficient learning, but come at the cost of a potentially lower performance ceiling. Vision transformers rely on more flexible self-attention layers, and have recently outperformed CNNs for image classification. However, they require costly pre-training on large external datasets or distillation from pre-trained convolutional networks. In this paper, we ask the following question: is it possible to combine the strengths of these two architectures while avoiding their respective limitations? To this end, we introduce gated positional self-attention (GPSA), a form of positional self-attention which can be equipped with a ‘soft’ convolutional inductive bias. We initialize the GPSA layers to mimic the locality of convolutional layers, then give each attention head the freedom to escape locality by adjusting a gating parameter regulating the attention paid to position versus content information. The resulting convolutional-like ViT architecture, ConViT, outperforms the DeiT (Touvron et al 2020 arXiv:2012.12877) on ImageNet, while offering a much improved sample efficiency. We further investigate the role of locality in learning by first quantifying how it is encouraged in vanilla self-attention layers, then analyzing how it has escaped in GPSA layers. We conclude by presenting various ablations to better understand the success of the ConViT. Our code and models are released publicly at https://github.com/facebookresearch/convit.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2103.10697",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2103.10697, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "1400419176",
        "name": "Stéphane d'Ascoli",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2113243762",
        "name": "Hugo Touvron",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2028252288",
        "name": "Matthew L. Leavitt",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "4690624",
        "name": "Ari S. Morcos",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2188423",
        "name": "G. Biroli",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2246570",
        "name": "Levent Sagun",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "GPSA is introduced, a form of positional self-attention which can be equipped with a ‘soft’ convolutional inductive bias and outperforms the DeiT on ImageNet, while offering a much improved sample efficiency."
  },
  {
    "paperId": "f48ae425e2567be2d993efcaaf74c2274fc9d7c5",
    "title": "COMET: Commonsense Transformers for Automatic Knowledge Graph Construction",
    "year": 2019,
    "abstract": "We present the first comprehensive study on automatic knowledge base construction for two prevalent commonsense knowledge graphs: ATOMIC (Sap et al., 2019) and ConceptNet (Speer et al., 2017). Contrary to many conventional KBs that store knowledge with canonical templates, commonsense KBs only store loosely structured open-text descriptions of knowledge. We posit that an important step toward automatic commonsense completion is the development of generative models of commonsense knowledge, and propose COMmonsEnse Transformers (COMET) that learn to generate rich and diverse commonsense descriptions in natural language. Despite the challenges of commonsense modeling, our investigation reveals promising results when implicit knowledge from deep pre-trained language models is transferred to generate explicit knowledge in commonsense knowledge graphs. Empirical results demonstrate that COMET is able to generate novel knowledge that humans rate as high quality, with up to 77.5% (ATOMIC) and 91.7% (ConceptNet) precision at top 1, which approaches human performance for these resources. Our findings suggest that using generative commonsense models for automatic commonsense KB completion could soon be a plausible alternative to extractive methods.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "https://www.aclweb.org/anthology/P19-1470.pdf",
      "status": "HYBRID",
      "license": "CCBY",
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1906.05317, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "2691021",
        "name": "Antoine Bosselut",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2516777",
        "name": "Hannah Rashkin",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2729164",
        "name": "Maarten Sap",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "8805254",
        "name": "Chaitanya Malaviya",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1709797",
        "name": "Asli Celikyilmaz",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1699545",
        "name": "Yejin Choi",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "This investigation reveals promising results when implicit knowledge from deep pre-trained language models is transferred to generate explicit knowledge in commonsense knowledge graphs, and suggests that using generative commonsense models for automatic commonsense KB completion could soon be a plausible alternative to extractive methods."
  },
  {
    "paperId": "9137efc758f80dd22bb56f82cca5c94f78a5db3e",
    "title": "MViTv2: Improved Multiscale Vision Transformers for Classification and Detection",
    "year": 2021,
    "abstract": "In this paper, we study Multiscale Vision Transformers (MViTv2) as a unified architecture for image and video classification, as well as object detection. We present an improved version of MViT that incorporates decomposed relative positional embeddings and residual pooling connections. We instantiate this architecture in five sizes and evaluate it for ImageNet classification, COCO detection and Kinetics video recognition where it outperforms prior work. We further compare MViTv2s' pooling attention to window attention mechanisms where it outperforms the latter in accuracy/compute. Without bells-and-whistles, MViTv2 has state-of-the-art performance in 3 domains: 88.8% accuracy on ImageNet classification, 58.7 APbox on COCO object detection as well as 86.1% on Kinetics-400 video classification. Code and models are available at https://github.com/facebookresearch/mvit.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2112.01526",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2112.01526, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "2359205979",
        "name": "Yanghao Li",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "9194382",
        "name": "Chaoxia Wu",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "146884473",
        "name": "Haoqi Fan",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "11379939",
        "name": "K. Mangalam",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2142452652",
        "name": "Bo Xiong",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "153652147",
        "name": "J. Malik",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2322150",
        "name": "Christoph Feichtenhofer",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "An improved version of MViT that incorporates decomposed relative positional embeddings and residual pooling connections is presented that outperforms prior work on image and video classification, as well as object detection."
  },
  {
    "paperId": "16f2d2f2b8103ed0c4a4e6f339a21247e58c5e78",
    "title": "Bottleneck Transformers for Visual Recognition",
    "year": 2021,
    "abstract": "We present BoTNet, a conceptually simple yet powerful backbone architecture that incorporates self-attention for multiple computer vision tasks including image classification, object detection and instance segmentation. By just replacing the spatial convolutions with global self-attention in the final three bottleneck blocks of a ResNet and no other changes, our approach improves upon the baselines significantly on instance segmentation and object detection while also reducing the parameters, with minimal overhead in latency. Through the design of BoTNet, we also point out how ResNet bottleneck blocks with self-attention can be viewed as Transformer blocks. Without any bells and whistles, BoTNet achieves 44.4% Mask AP and 49.7% Box AP on the COCO Instance Segmentation benchmark using the Mask R-CNN framework; surpassing the previous best published single model and single scale results of ResNeSt [67] evaluated on the COCO validation set. Finally, we present a simple adaptation of the BoTNet design for image classification, resulting in models that achieve a strong performance of 84.7% top-1 accuracy on the ImageNet benchmark while being up to 1.64x faster in \"compute\"1 time than the popular EfficientNet models on TPU-v3 hardware. We hope our simple and effective approach will serve as a strong baseline for future research in self-attention models for vision.2",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2101.11605",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2101.11605, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "41207614",
        "name": "A. Srinivas",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "33493200",
        "name": "Tsung-Yi Lin",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "3877127",
        "name": "Niki Parmar",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1789737",
        "name": "Jonathon Shlens",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1689992",
        "name": "P. Abbeel",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1630664874",
        "name": "Ashish Vaswani",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "BoTNet is presented, a conceptually simple yet powerful backbone architecture that incorporates self-attention for multiple computer vision tasks including image classification, object detection and instance segmentation, and a simple adaptation of the BoTNet design for image classification is presented."
  },
  {
    "paperId": "076a8e778f2e9efb3c2fd45fed534ae9e6035f1b",
    "title": "Self-Supervised Pre-Training of Swin Transformers for 3D Medical Image Analysis",
    "year": 2021,
    "abstract": "Vision Transformers (ViT)s have shown great performance in self-supervised learning of global and local representations that can be transferred to downstream applications. Inspired by these results, we introduce a novel self-supervised learning framework with tailored proxy tasks for medical image analysis. Specifically, we propose: (i) a new 3D transformer-based model, dubbed Swin UNEt TRansformers (Swin UNETR), with a hierarchical encoder for self-supervised pretraining; (ii) tailored proxy tasks for learning the underlying pattern of human anatomy. We demonstrate successful pre-training of the proposed model on 5,050 publicly available computed tomography (CT) images from various body organs. The effectiveness of our approach is validated by fine-tuning the pre-trained models on the Beyond the Cranial Vault (BTCV) Segmentation Challenge with 13 abdominal organs and segmentation tasks from the Medical Segmentation Decathlon (MSD) dataset. Our model is currently the state-of-the-art on the public test leaderboards of both MSD11https://decathlon-10.grand-challenge.org/evaluation/challenge/leaderboard/ and BTCV 22https://www.synapse.org/#!Synapse:syn3193805/wiki/217785/ datasets. Code: https://monai.io/research/swin-unetr.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2111.14791",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2111.14791, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "46556781",
        "name": "Yucheng Tang",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "144041873",
        "name": "Dong Yang",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2108730532",
        "name": "Wenqi Li",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "144531567",
        "name": "H. Roth",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1699344",
        "name": "B. Landman",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "3262394",
        "name": "Daguang Xu",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "10751841",
        "name": "V. Nath",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "31374559",
        "name": "Ali Hatamizadeh",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "A new 3D transformer-based model, dubbed Swin UNEt TRansformers (Swin UNETR), with a hierarchical encoder for self-supervised pretraining for medical image analysis and tailored proxy tasks for learning the underlying pattern of human anatomy is proposed."
  },
  {
    "paperId": "761240b06248b9836ee564bdab61559c84b681ed",
    "title": "CMT: Convolutional Neural Networks Meet Vision Transformers",
    "year": 2021,
    "abstract": "Vision transformers have been successfully applied to image recognition tasks due to their ability to capture long-range dependencies within an image. However, there are still gaps in both performance and computational cost between transformers and existing convolutional neural networks (CNNs). In this paper, we aim to address this issue and develop a network that can outperform not only the canonical transformers, but also the high-performance convolutional models. We propose a new transformer based hybrid network by taking advantage of transformers to capture long-range dependencies, and of CNNs to extract local information. Furthermore, we scale it to obtain a family of models, called CMTs, obtaining much better trade-off for accuracy and efficiency than previous CNN-based and transformer-based models. In particular, our CMT-S achieves 83.5% top-1 accuracy on ImageNet, while being 14x and 2x smaller on FLOPs than the existing DeiT and EfficientNet, respectively. The proposed CMT-S also generalizes well on CIFAR10 (99.2%), CIFAR100 (91.7%), Flowers (98.7%), and other challenging vision datasets such as COCO (44.3% mAP), with considerably less computational cost.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2107.06263",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2107.06263, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "2148899357",
        "name": "Jianyuan Guo",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "3826388",
        "name": "Kai Han",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2109295256",
        "name": "Han Wu",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "9196284",
        "name": "Chang Xu",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "103603255",
        "name": "Yehui Tang",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1691522",
        "name": "Chunjing Xu",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2108702980",
        "name": "Yunhe Wang",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "A new transformer based hybrid network is proposed by taking advantage of transformers to capture long-range dependencies, and of CNNs to extract local information, obtaining much better trade-off for accuracy and efficiency than previous CNN-based and transformer-based models."
  },
  {
    "paperId": "7e9ff94476f41041c75e253e84f487db00e9c861",
    "title": "Long Range Arena: A Benchmark for Efficient Transformers",
    "year": 2020,
    "abstract": "Transformers do not scale very well to long sequence lengths largely because of quadratic self-attention complexity. In the recent months, a wide spectrum of efficient, fast Transformers have been proposed to tackle this problem, more often than not claiming superior or comparable model quality to vanilla Transformer models. To this date, there is no well-established consensus on how to evaluate this class of models. Moreover, inconsistent benchmarking on a wide spectrum of tasks and datasets makes it difficult to assess relative model quality amongst many models. This paper proposes a systematic and unified benchmark, LRA, specifically focused on evaluating model quality under long-context scenarios. Our benchmark is a suite of tasks consisting of sequences ranging from $1K$ to $16K$ tokens, encompassing a wide range of data types and modalities such as text, natural, synthetic images, and mathematical expressions requiring similarity, structural, and visual-spatial reasoning. We systematically evaluate ten well-established long-range Transformer models (Reformers, Linformers, Linear Transformers, Sinkhorn Transformers, Performers, Synthesizers, Sparse Transformers, and Longformers) on our newly proposed benchmark suite. LRA paves the way towards better understanding this class of efficient Transformer models, facilitates more research in this direction, and presents new challenging tasks to tackle. Our benchmark code will be released at this https URL.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2011.04006, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "144447820",
        "name": "Yi Tay",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "3226635",
        "name": "Mostafa Dehghani",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2786352",
        "name": "Samira Abnar",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2714199",
        "name": "Yikang Shen",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2119725651",
        "name": "Dara Bahri",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "38552691",
        "name": "Philip Pham",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "30586030",
        "name": "J. Rao",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2119062135",
        "name": "Liu Yang",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2884561",
        "name": "Sebastian Ruder",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1680617",
        "name": "Donald Metzler",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "A systematic and unified benchmark, LRA, specifically focused on evaluating model quality under long-context scenarios is proposed, paving the way towards better understanding this class of efficient Transformer models."
  },
  {
    "paperId": "7507603da9711c0e43e29f3094f33d9337e7dfbd",
    "title": "3D Human Pose Estimation with Spatial and Temporal Transformers",
    "year": 2021,
    "abstract": "Transformer architectures have become the model of choice in natural language processing and are now being introduced into computer vision tasks such as image classification, object detection, and semantic segmentation. However, in the field of human pose estimation, convolutional architectures still remain dominant. In this work, we present PoseFormer, a purely transformer-based approach for 3D human pose estimation in videos without convolutional architectures involved. Inspired by recent developments in vision transformers, we design a spatial-temporal transformer structure to comprehensively model the human joint relations within each frame as well as the temporal correlations across frames, then output an accurate 3D human pose of the center frame. We quantitatively and qualitatively evaluate our method on two popular and standard benchmark datasets: Human3.6M and MPI-INF-3DHP. Extensive experiments show that PoseFormer achieves state-of-the-art performance on both datasets. Code is available at https://github.com/zczcwh/PoseFormer",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2103.10455",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2103.10455, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "2113919899",
        "name": "Ce Zheng",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "9385903",
        "name": "Sijie Zhu",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1422036273",
        "name": "Mat'ias Mendieta",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1390892946",
        "name": "Taojiannan Yang",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2141809453",
        "name": "Chen Chen",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2788685",
        "name": "Zhengming Ding",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "PoseFormer is presented, a purely transformer-based approach for 3D human pose estimation in videos without convolutional architectures involved, designed to comprehensively model the human joint relations within each frame as well as the temporal correlations across frames."
  },
  {
    "paperId": "4b06c7e29280b1c6bc05c9df39023b48fef02c93",
    "title": "Escaping the Big Data Paradigm with Compact Transformers",
    "year": 2021,
    "abstract": "With the rise of Transformers as the standard for language processing, and their advancements in computer vision, there has been a corresponding growth in parameter size and amounts of training data. Many have come to believe that because of this, transformers are not suitable for small sets of data. This trend leads to concerns such as: limited availability of data in certain scientific domains and the exclusion of those with limited resource from research in the field. In this paper, we aim to present an approach for small-scale learning by introducing Compact Transformers. We show for the first time that with the right size, convolutional tokenization, transformers can avoid overfitting and outperform state-of-the-art CNNs on small datasets. Our models are flexible in terms of model size, and can have as little as 0.28M parameters while achieving competitive results. Our best model can reach 98% accuracy when training from scratch on CIFAR-10 with only 3.7M parameters, which is a significant improvement in data-efficiency over previous Transformer based models being over 10x smaller than other transformers and is 15% the size of ResNet50 while achieving similar performance. CCT also outperforms many modern CNN based approaches, and even some recent NAS-based approaches. Additionally, we obtain a new SOTA result on Flowers-102 with 99.76% top-1 accuracy, and improve upon the existing baseline on ImageNet (82.71% accuracy with 29% as many parameters as ViT), as well as NLP tasks. Our simple and compact design for transformers makes them more feasible to study for those with limited computing resources and/or dealing with small datasets, while extending existing research efforts in data efficient transformers. Our code and pre-trained models are publicly available at https://github.com/SHI-Labs/Compact-Transformers.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2104.05704, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "2855934",
        "name": "Ali Hassani",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "102471415",
        "name": "Steven Walton",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2087066452",
        "name": "Nikhil Shah",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "133551208",
        "name": "Abulikemu Abuduweili",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2125031571",
        "name": "Jiachen Li",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "48667025",
        "name": "Humphrey Shi",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "It is shown for the first time that with the right size, convolutional tokenization, transformers can avoid overfitting and outperform state-of-the-art CNNs on small datasets, and CCT outperforms many modern CNN based approaches, and even some recent NAS-based approaches."
  },
  {
    "paperId": "47ae807cd511b35e78a2cd4e198283dea6dafd41",
    "title": "Do Transformers Really Perform Bad for Graph Representation?",
    "year": 2021,
    "abstract": "The Transformer architecture has become a dominant choice in many domains, such as natural language processing and computer vision. Yet, it has not achieved competitive performance on popular leaderboards of graph-level prediction compared to mainstream GNN variants. Therefore, it remains a mystery how Transformers could perform well for graph representation learning. In this paper, we solve this mystery by presenting Graphormer, which is built upon the standard Transformer architecture, and could attain excellent results on a broad range of graph representation learning tasks, especially on the recent OGB Large-Scale Challenge. Our key insight to utilizing Transformer in the graph is the necessity of effectively encoding the structural information of a graph into the model. To this end, we propose several simple yet effective structural encoding methods to help Graphormer better model graph-structured data. Besides, we mathematically characterize the expressive power of Graphormer and exhibit that with our ways of encoding the structural information of graphs, many popular GNN variants could be covered as the special cases of Graphormer.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2106.05234, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "2051552141",
        "name": "Chengxuan Ying",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "123970124",
        "name": "Tianle Cai",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2108801920",
        "name": "Shengjie Luo",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "150311931",
        "name": "Shuxin Zheng",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "35286545",
        "name": "Guolin Ke",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1391126980",
        "name": "Di He",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2115437382",
        "name": "Yanming Shen",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2110264337",
        "name": "Tie-Yan Liu",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "This paper presents Graphormer, which is built upon the standard Transformer architecture, and could attain excellent results on a broad range of graph representation learning tasks, especially on the recent OGB Large-Scale Challenge."
  },
  {
    "paperId": "63812f583caac3ac32bbfb64f66ba69e57c1e90a",
    "title": "Conditional Positional Encodings for Vision Transformers",
    "year": 2021,
    "abstract": "We propose a conditional positional encoding (CPE) scheme for vision Transformers. Unlike previous fixed or learnable positional encodings, which are pre-defined and independent of input tokens, CPE is dynamically generated and conditioned on the local neighborhood of the input tokens. As a result, CPE can easily generalize to the input sequences that are longer than what the model has ever seen during training. Besides, CPE can keep the desired translation-invariance in the image classification task, resulting in improved performance. We implement CPE with a simple Position Encoding Generator (PEG) to get seamlessly incorporated into the current Transformer framework. Built on PEG, we present Conditional Position encoding Vision Transformer (CPVT). We demonstrate that CPVT has visually similar attention maps compared to those with learned positional encodings and delivers outperforming results. Our code is available at https://github.com/Meituan-AutoML/CPVT .",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2102.10882, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "27628828",
        "name": "Xiangxiang Chu",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2069520672",
        "name": "Zhi Tian",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "49846372",
        "name": "Bo Zhang",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "51316629",
        "name": "Xinlong Wang",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "12459603",
        "name": "Chunhua Shen",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "This work proposes a conditional positional encoding (CPE) scheme for vision Transformers and implements CPE with a simple Position Encoding Generator (PEG) to get seamlessly incorporated into the current Transformer framework."
  },
  {
    "paperId": "40f4d7fe800810288a80f84cdb357a8f4c28e880",
    "title": "Rethinking Spatial Dimensions of Vision Transformers",
    "year": 2021,
    "abstract": "Vision Transformer (ViT) extends the application range of transformers from language processing to computer vision tasks as being an alternative architecture against the existing convolutional neural networks (CNN). Since the transformer-based architecture has been innovative for computer vision modeling, the design convention towards an effective architecture has been less studied yet. From the successful design principles of CNN, we investigate the role of spatial dimension conversion and its effectiveness on transformer-based architecture. We particularly attend to the dimension reduction principle of CNNs; as the depth increases, a conventional CNN increases channel dimension and decreases spatial dimensions. We empirically show that such a spatial dimension reduction is beneficial to a transformer architecture as well, and propose a novel Pooling-based Vision Transformer (PiT) upon the original ViT model. We show that PiT achieves the improved model capability and generalization performance against ViT. Throughout the extensive experiments, we further show PiT outperforms the baseline on several tasks such as image classification, object detection, and robustness evaluation. Source codes and ImageNet models are available at https://github.com/naver-ai/pit.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2103.16302",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2103.16302, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "3086596",
        "name": "Byeongho Heo",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2151587",
        "name": "Sangdoo Yun",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2086576",
        "name": "Dongyoon Han",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2647582",
        "name": "Sanghyuk Chun",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "3338475",
        "name": "Junsuk Choe",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2390510",
        "name": "Seong Joon Oh",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "A novel Pooling-based Vision Transformer (PiT) is proposed, which achieves the improved model capability and generalization performance against ViT and outperforms the baseline on several tasks such as image classification, object detection and robustness evaluation."
  },
  {
    "paperId": "cf5e6e3c50a798d87033e0e108e88b3647738bbe",
    "title": "How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers",
    "year": 2021,
    "abstract": "Vision Transformers (ViT) have been shown to attain highly competitive performance for a wide range of vision applications, such as image classification, object detection and semantic image segmentation. In comparison to convolutional neural networks, the Vision Transformer's weaker inductive bias is generally found to cause an increased reliance on model regularization or data augmentation (\"AugReg\"for short) when training on smaller training datasets. We conduct a systematic empirical study in order to better understand the interplay between the amount of training data, AugReg, model size and compute budget. As one result of this study we find that the combination of increased compute and AugReg can yield models with the same performance as models trained on an order of magnitude more training data: we train ViT models of various sizes on the public ImageNet-21k dataset which either match or outperform their counterparts trained on the larger, but not publicly available JFT-300M dataset.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2106.10270, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "2079614268",
        "name": "A. Steiner",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "144629422",
        "name": "Alexander Kolesnikov",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2743563",
        "name": "Xiaohua Zhai",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2113839396",
        "name": "Ross Wightman",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "39328010",
        "name": "Jakob Uszkoreit",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "39611591",
        "name": "Lucas Beyer",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "This study trains ViT models of various sizes on the public ImageNet-21k dataset which either match or outperform their counterparts trained on the larger, but not publicly available JFT-300M dataset."
  },
  {
    "paperId": "7fff8018bf625447df837c2fda5c58a705fbc038",
    "title": "XCiT: Cross-Covariance Image Transformers",
    "year": 2021,
    "abstract": "Following their success in natural language processing, transformers have recently shown much promise for computer vision. The self-attention operation underlying transformers yields global interactions between all tokens ,i.e. words or image patches, and enables flexible modelling of image data beyond the local interactions of convolutions. This flexibility, however, comes with a quadratic complexity in time and memory, hindering application to long sequences and high-resolution images. We propose a\"transposed\"version of self-attention that operates across feature channels rather than tokens, where the interactions are based on the cross-covariance matrix between keys and queries. The resulting cross-covariance attention (XCA) has linear complexity in the number of tokens, and allows efficient processing of high-resolution images. Our cross-covariance image transformer (XCiT) is built upon XCA. It combines the accuracy of conventional transformers with the scalability of convolutional architectures. We validate the effectiveness and generality of XCiT by reporting excellent results on multiple vision benchmarks, including image classification and self-supervised feature learning on ImageNet-1k, object detection and instance segmentation on COCO, and semantic segmentation on ADE20k.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "",
      "status": null,
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2106.09681, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "1388811741",
        "name": "Alaaeldin El-Nouby",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2113243762",
        "name": "Hugo Touvron",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2062862676",
        "name": "Mathilde Caron",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2329288",
        "name": "Piotr Bojanowski",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "3271933",
        "name": "Matthijs Douze",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2319608",
        "name": "Armand Joulin",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "143991676",
        "name": "I. Laptev",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2759569",
        "name": "N. Neverova",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2282478",
        "name": "Gabriel Synnaeve",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "34602236",
        "name": "Jakob Verbeek",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "1681054",
        "name": "H. Jégou",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "This work proposes a \"transposed\"version of self-attention that operates across feature channels rather than tokens, where the interactions are based on the cross-covariance matrix between keys and queries, and builds upon XCA, which has linear complexity in the number of tokens and allows efficient processing of high-resolution images."
  },
  {
    "paperId": "ef89d5899eff8d5e62f85018b3f11889d920a1aa",
    "title": "TransVG: End-to-End Visual Grounding with Transformers",
    "year": 2021,
    "abstract": "In this paper, we present a neat yet effective transformer-based framework for visual grounding, namely TransVG, to address the task of grounding a language query to the corresponding region onto an image. The state-of-the-art methods, including two-stage or one-stage ones, rely on a complex module with manually-designed mechanisms to perform the query reasoning and multi-modal fusion. However, the involvement of certain mechanisms in fusion module design, such as query decomposition and image scene graph, makes the models easily overfit to datasets with specific scenarios, and limits the plenitudinous interaction between the visual-linguistic context. To avoid this caveat, we propose to establish the multi-modal correspondence by leveraging transformers, and empirically show that the complex fusion modules (e.g., modular attention network, dynamic graph, and multi-modal tree) can be replaced by a simple stack of transformer encoder layers with higher performance. Moreover, we re-formulate the visual grounding as a direct coordinates regression problem and avoid making predictions out of a set of candidates (i.e., region proposals or anchor boxes). Extensive experiments are conducted on five widely used datasets, and a series of state-of-the-art records are set by our TransVG. We build the benchmark of transformer-based visual grounding framework and make the code available at https://github.com/djiajunustc/TransVG.",
    "url": null,
    "venue": null,
    "publicationDate": null,
    "publicationTypes": [],
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2104.08541",
      "status": "GREEN",
      "license": null,
      "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2104.08541, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
    },
    "authors": [
      {
        "authorId": "2087218319",
        "name": "Jiajun Deng",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2149231840",
        "name": "Zhengyuan Yang",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "7934161",
        "name": "Tianlang Chen",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "38272296",
        "name": "Wen-gang Zhou",
        "affiliations": [],
        "url": null
      },
      {
        "authorId": "2108508109",
        "name": "Houqiang Li",
        "affiliations": [],
        "url": null
      }
    ],
    "citationCount": 0,
    "referenceCount": 0,
    "influentialCitationCount": 0,
    "fieldsOfStudy": [],
    "externalIds": {},
    "journal": null,
    "tldr": "A neat yet effective transformer-based framework for visual grounding, namely TransVG, to address the task of grounding a language query to the corresponding region onto an image by leveraging transformers and empirically showing that the complex fusion modules can be replaced by a simple stack of transformer encoder layers with higher performance."
  }
]